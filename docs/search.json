[
  {
    "objectID": "posts/voc_detection/index.html",
    "href": "posts/voc_detection/index.html",
    "title": "D-Fine on pascal Voc",
    "section": "",
    "text": "Object detection is the second type of problem we will be solving in this blog using lightning and pascal voc. This problem is very common because it allow to locate a variable number of objects in images.\nSince it is used in case of autonomous driving, real time object counting object detection architectures are grouped into two categories. The real time object detection where the boss is the YOLO version 11 at the time i write this article. The more recent networks that can fight with yolo in RTOD (Real time object detection) is the D-Fine architecture which is an improved DETR.\nWe implemented it in this article, we chose to train this architecture in lightning since the training loop can be re-used with other models and dataset. I donâ€™t know what is the best training paradigm but i want one that keeps as agnostic as possible to any framework, and for me lighting allow strong customization and proximity to pure torchscript while allowing easy multi device training and metrics logging etc..\n\nData\nWe will model pascal voc images in this project because its very easily accessible from torchvision repos.\n\nfrom torchvision.datasets import VOCDetection\nfrom torchvision.transforms import v2 \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import DataLoader\nimport torch \nfrom omegaconf import OmegaConf\nfrom torchvision.ops import box_convert\nfrom torchvision.utils import draw_bounding_boxes\nimport albumentations as A\nfrom transformers import DFineForObjectDetection, AutoImageProcessor\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision.ops import batched_nms\nimport torchmetrics\nimport lightning as L\nfrom torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\nfrom typing import Dict, List, Optional\n\n/home/arthings/big_partition/projet/myblog/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning:\n\nIProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\n/home/arthings/big_partition/projet/myblog/.venv/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning:\n\nA new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n\n\n\nd-fine expect bbox in coco format so we convert it in the get_item part.\n\ncategories = ['pottedplant', 'bottle', 'chair', 'diningtable', 'person', 'car', 'train', 'bus', 'bicycle', 'cat', 'aeroplane', 'tvmonitor', 'sofa', 'sheep', 'dog', 'bird', 'motorbike', 'horse', 'boat', 'cow', \"bg\"]\n\n\ndef unpack_box(box_dict:Dict):\n    \"\"\"\n    Unpack the box dictionary into a list of coordinates\n    \"\"\"\n    return torch.tensor(np.array([\n        box_dict[\"xmin\"],\n        box_dict[\"ymin\"],\n        box_dict[\"xmax\"],\n        box_dict[\"ymax\"]\n    ], dtype = float))\n\ndef annotation_to_torch(target:Dict):\n    rep = {}\n    detections = target[\"annotation\"][\"object\"]\n    \n    rep[\"labels\"] = np.array([categories.index(i[\"name\"]) for i in detections])\n    # xmin ymin xmax ymax\n    rep[\"boxes\"] = torch.stack([unpack_box(i[\"bndbox\"]) for i in detections])\n\n    return rep    \n\nimage_processor = AutoImageProcessor.from_pretrained(\"ustc-community/dfine_x_coco\")\n\nclass MyVoc(VOCDetection):\n    def __getitem__(self, index):\n        image, target = super().__getitem__(index)\n        # Apply your transformations here\n        target = annotation_to_torch(target)\n\n        transform = A.Compose([\n            A.PadIfNeeded(500,500),\n            A.HorizontalFlip(),\n            A.RandomCrop(400,400),\n            A.Resize(224, 224),\n            A.Normalize(normalization=\"min_max\"),\n            ToTensorV2()\n        ],\n        bbox_params=A.BboxParams(\n            format='pascal_voc', # Specify input format\n            label_fields=['class_labels'],\n            filter_invalid_bboxes=True)\n        )\n\n        transformed = transform(\n            image=np.array(image), \n            bboxes=target[\"boxes\"], \n            class_labels=target[\"labels\"]\n        )\n\n        transformed[\"labels\"] = transformed[\"class_labels\"]\n        transformed[\"boxes\"] = transformed[\"bboxes\"]\n        transformed.pop(\"bboxes\")\n\n        image = transformed.pop(\"image\")\n        transformed[\"boxes\"] = box_convert(\n            torch.from_numpy(transformed[\"boxes\"],\n            ),\n            \"xyxy\",\n            \"xywh\",).float()\n        transformed[\"labels\"] = torch.from_numpy(transformed[\"labels\"])\n        transformed[\"class_labels\"] = torch.from_numpy(transformed[\"class_labels\"])\n        return image.float(), transformed\n\n    def draw_item(self, index:Optional[int] = None, n=5):\n        if index is None:\n            index = np.random.randint(0, len(self))\n\n        image, labels = self[index]\n\n\n        with_boxes = draw_bounding_boxes(\n            image =  image,\n            boxes= box_convert(labels[\"boxes\"], \"xywh\", \"xyxy\"),\n            labels = [categories[i] for i in labels[\"labels\"]],\n            colors = \"red\"\n\n        )\n\n\n        plt.figure(figsize=(10, 10))\n        plt.imshow(with_boxes.permute(1, 2, 0).numpy())\n        plt.axis('off')\n        plt.show()\n        return \n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n\n\nNothing special here, we show what the data looks like.\n\nds = MyVoc(\n    root = \"./data\", \n    download = False,\n    image_set=\"train\"\n)\nval_ds = MyVoc(\n    root = \"./data\", \n    download = False,\n    image_set=\"val\",\n)\nds.draw_item()\n\n\n\n\n\n\n\n\n\n\nModeling\nModeling part is here, whith the lightning module.\n\n\nCode\nconfig = OmegaConf.create({\n    \"lr\": 1e-4,\n    \"batch_size\":2,\n    \"epochs\":3,\n    \"world_size\":1\n})\ndef apply_nms(preds:Dict, iou_thr:float = .5):\n    nms_indices = batched_nms(\n        preds[\"boxes\"],\n        scores = preds[\"scores\"],\n        idxs=preds[\"labels\"],\n        iou_threshold=iou_thr\n    )\n    preds_nms = {}\n    preds_nms[\"boxes\"] = preds[\"boxes\"][nms_indices,:]\n    preds_nms[\"scores\"] = preds[\"scores\"][nms_indices]\n    preds_nms[\"labels\"] = preds[\"labels\"][nms_indices]\n    \n    # high_scores_indices = preds_nms[\"scores\"] &gt; .3\n    # preds_nms[\"boxes\"] = preds_nms[\"boxes\"][high_scores_indices]\n    # preds_nms[\"scores\"] = preds_nms[\"scores\"][high_scores_indices]\n    # preds_nms[\"labels\"] = preds_nms[\"labels\"][high_scores_indices]\n\n    return preds_nms\n\nclass odModule(L.LightningModule):\n    def __init__(self, config, categories:List[str], nms_thr:float = .5):\n        super().__init__()\n        # if config.checkpoint is not None:\n        # print(f\"checkpoint from {config.checkpoint}\")\n        num_classes = len(categories)\n        self.categories = categories\n\n        self.nms_thr = nms_thr\n        self.config = config\n        # self.model = fasterrcnn_mobilenet_v3_large_fpn(pretrained = False, num_classes=num_classes)\n        self.model = DFineForObjectDetection.from_pretrained(\n            \"ustc-community/dfine_x_coco\",\n            id2label= {i:cat for i,cat in enumerate(categories)},\n            label2id={cat:i for i,cat in enumerate(categories)},\n            ignore_mismatched_sizes=True,\n        )\n\n        metrics = torchmetrics.MetricCollection(\n            [\n                torchmetrics.detection.mean_ap.MeanAveragePrecision(\n                    # extended_summary=True, \n                    iou_thresholds=np.linspace(0,1,20).tolist(),\n                    class_metrics=True, \n                    iou_type=\"bbox\",\n                ),\n            ]\n        )   \n        self.val_metrics = metrics.clone(prefix=\"Validation/\")\n        self.test_metrics = metrics.clone(prefix=\"Test/\")\n    \n        self.save_hyperparameters(ignore=[\"train_ds\"])\n\n    @staticmethod\n    def prepare_batch(batch):\n        images, targets = batch\n        return images, targets\n\n    def forward(self, x, y=None):\n        if y is not None:\n            return self.model(pixel_values = x, labels = y)\n        else:\n            preds = self.model(x)\n            n, c, h, w = x.shape\n            preds = image_processor.post_process_object_detection(\n                preds, \n                target_sizes=[(h,w) for _ in range(n)], \n                threshold=0.5)\n            return preds\n\n    def predict(self, x):\n        \"\"\"Forward the model then run NMS (for evaluation)\n\n        Args:\n            x (_type_): _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"        \n        preds:List = self(x)\n        # preds_nms = [apply_nms(i, self.nms_thr) for i in preds]\n        return preds\n\n\n    def training_step(self, batch, batch_idx):\n        img_b, target_b = self.prepare_batch(batch)\n        bs = len(img_b)\n\n        dfine_output = self(img_b, target_b)\n\n        self.log_dict(\n            dfine_output.loss_dict,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n            batch_size=bs,\n            prog_bar=True,\n        )\n\n        return {\"loss\": dfine_output.loss}\n\n    def validation_step(self, batch, batch_idx):\n        img_b, target_b = self.prepare_batch(batch)\n        output_nms = self.predict(img_b)\n\n        self.val_metrics(output_nms, target_b)\n        return\n\n    def on_validation_epoch_end(self):\n        \n        m = self.val_metrics.compute()\n        m_single= {i:j for i,j in m.items() if j.nelement() ==1}\n        \n        self.log_dict(m_single, on_epoch=True, sync_dist=False)\n\n        for i, class_id in enumerate(m[\"Validation/classes\"]):\n            self.log(f\"Validation/MAP {self.categories[class_id]}\", m[\"Validation/map_per_class\"][i])\n\n        self.val_metrics.reset()\n        return\n\n    def test_step(self, batch, batch_idx):\n        img_b, target_b = self.prepare_batch(batch)\n\n        output_nms = self.predict(img_b)\n\n        self.test_metrics(output_nms, target_b)\n        return\n\n    def on_test_epoch_end(self):\n        m = self.test_metrics.compute()\n        m_single= {i:j for i,j in m.items() if j.nelement() ==1}\n\n        self.log_dict(m_single, on_epoch=True, sync_dist=False)\n        \n        for i, class_id in enumerate(m[\"Test/classes\"]):\n            self.log(f\"Test/MAP {self.categories[class_id]}\", m[\"Test/map_per_class\"][i])\n\n        self.test_metrics.reset()\n\n        return \n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(\n            self.parameters(),\n            lr=self.config.lr,\n            weight_decay=1e-4 * self.config.batch_size / 16,\n        )\n\n        # scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(\n        #     optimizer, T_max=scheduler_nsteps, eta_min=self.config.lr / 10\n        # )\n\n        # sched_config1 = {\"scheduler\": scheduler1, \"interval\": \"epoch\"}\n\n\n        return [optimizer]#, [sched_config1]\n\nmodel = odModule(\n    config,\n    categories\n)\n\n\nSome weights of DFineForObjectDetection were not initialized from the model checkpoint at ustc-community/dfine_x_coco and are newly initialized because the shapes did not match:\n- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([21]) in the model instantiated\n- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([21, 256]) in the model instantiated\n- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([21]) in the model instantiated\n- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([21, 256]) in the model instantiated\n- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([21]) in the model instantiated\n- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([21, 256]) in the model instantiated\n- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([21]) in the model instantiated\n- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([21, 256]) in the model instantiated\n- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([21]) in the model instantiated\n- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([21, 256]) in the model instantiated\n- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([21]) in the model instantiated\n- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([21, 256]) in the model instantiated\n- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([22, 256]) in the model instantiated\n- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([21]) in the model instantiated\n- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([21, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\ndef collate_fn(batch):\n    # Separate the images and targets\n    images = []\n    targets = []\n    \n    for image, target in batch:\n        images.append(image)  # Assuming each item has an 'image' key\n        targets.append(target)  # Assuming each item has a 'target' key\n\n    # Stack images into a single tensor\n    images = torch.stack(images, dim=0)\n\n    # Convert targets to a list of dictionaries or tensors\n    # This depends on how your targets are structured\n    # For example, if targets are dictionaries with bounding boxes and labels\n    return images, targets\ntrain_loader = DataLoader(\n    ds,\n    batch_size = 2,\n    shuffle = True,\n    collate_fn = collate_fn\n)\nval_loader = DataLoader(\n    val_ds,\n    batch_size = 2,\n    shuffle = False,\n    collate_fn = collate_fn\n)\nim, tar = next(iter(train_loader))\n\ntar\n\n[{'class_labels': tensor([11, 11]),\n  'labels': tensor([11, 11]),\n  'boxes': tensor([[108.6400,  52.0800,  59.3600,  57.6800],\n          [ 77.2800,   0.0000,  55.4400,   1.6800]])},\n {'class_labels': tensor([16,  4]),\n  'labels': tensor([16,  4]),\n  'boxes': tensor([[ 17.3600,  68.8800, 138.3200,  98.5600],\n          [  0.0000,  37.5200, 131.6000, 122.6400]])}]\n\n\n\n\nTraining\nAll the training is shown here. The full training is happening here.\nthe full training has not been done yet but no bug has been encountered in first batches.\n\nmodel.train()\ntrainer=  L.Trainer(\n    max_epochs=3,\n    precision = \"16-mixed\",\n    enable_checkpointing=True,\n    num_sanity_val_steps=2,\n    log_every_n_steps=50,\n    check_val_every_n_epoch=1,\n)\n\n# trainer.fit(model, train_loader, val_loader)\n\nUsing 16bit Automatic Mixed Precision (AMP)\nUsing default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs"
  },
  {
    "objectID": "posts/uv_ml/index.html",
    "href": "posts/uv_ml/index.html",
    "title": "UV setup for computer vision using deep learning",
    "section": "",
    "text": "UV is a drop-in replacement for pip and global python installation. I allows the management of python versions and packages.\n#Â What is uv\nuv is a python package manager that can replace pip and the installation of python itself on any machine. Its coded in rust and is make the management of python project very fast and robust.\nAll the example provided in this article are assuming you run on a linux system.\nit is installable with a simple line in the terminal\nThis should make the uv command available through your shell.\nyou can check the python versions already available by running\nif the version of python you want is not already installed you can run the next command (change the 3.14 to the required python version)"
  },
  {
    "objectID": "posts/uv_ml/index.html#example-of-requirement.txt-allowing-training-of-deep-neural-network",
    "href": "posts/uv_ml/index.html#example-of-requirement.txt-allowing-training-of-deep-neural-network",
    "title": "UV setup for computer vision using deep learning",
    "section": "Example of requirement.txt allowing training of deep neural network",
    "text": "Example of requirement.txt allowing training of deep neural network\nnumpy\npandas\nseaborn\nmatplotlib\n\n\ntorch\ntorchvision\nopencv-python\nlightning\nalbumentations"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog will serve as a self-reminder and a repository for useful software code that I can easily share with others.\nI am currently in the first year of a PhD in Applied Mathematics. My research focuses on active learning applied to computer vision. I am doing my PhD at Michelin, where there is a high demand for machine vision and not enough experts to label the data.\nI love learning and am very curious but software development is far from being a strength."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julien Combes",
    "section": "",
    "text": "D-Fine on pascal Voc\n\n\n\n\n\n\nPhd\n\n\ncode\n\n\nDeepLearning\n\n\nComputerVision\n\n\nENG\n\n\n\n\n\n\n\n\n\nJun 9, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nTools for Phd\n\n\n\n\n\n\nPhd\n\n\nproductivity\n\n\nENG\n\n\n\n\n\n\n\n\n\nMay 27, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nUV setup for computer vision using deep learning\n\n\n\n\n\n\nDeepLearning\n\n\ncode\n\n\nComputerVision\n\n\npython\n\n\nlinux\n\n\n\n\n\n\n\n\n\nApr 13, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nClassify CIFAR\n\n\n\n\n\n\nDeepLearning\n\n\ncode\n\n\nImageClassification\n\n\nComputerVision\n\n\nENG\n\n\n\n\n\n\n\n\n\nApr 13, 2025\n\n\nJulien Combes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/phd_tools/index.html",
    "href": "posts/phd_tools/index.html",
    "title": "Tools for Phd",
    "section": "",
    "text": "Phd is a long and likely hard journey, if some tools can make the way easier we wonâ€™t complain about it. I want to gather tools that i find useful in this post.\nIt can be related to bibliography, writing, coding or anything that pass to mind, it should be updated at some point when new things come to my mind.\n#Â Research"
  },
  {
    "objectID": "posts/phd_tools/index.html#scholar-inbox",
    "href": "posts/phd_tools/index.html#scholar-inbox",
    "title": "Tools for Phd",
    "section": "Scholar inbox",
    "text": "Scholar inbox\nThis tool is so nice to be updated anytime an article in our field of research is published. You need to create an account an train a recommendation system based on literature that you are interested in. When itâ€™s done, you will have new article pre-published in the past week that relates to your research.\nS/O to MG that let me know this tool !"
  },
  {
    "objectID": "posts/phd_tools/index.html#bibliography",
    "href": "posts/phd_tools/index.html#bibliography",
    "title": "Tools for Phd",
    "section": "Bibliography",
    "text": "Bibliography\nI canâ€™t recommend Zotero enough. You can save article from the browser in one click. Read and annotate all in one place."
  },
  {
    "objectID": "posts/phd_tools/index.html#note-taking",
    "href": "posts/phd_tools/index.html#note-taking",
    "title": "Tools for Phd",
    "section": "Note taking",
    "text": "Note taking\nI like to use obsidian with the citation extension. It connects the notes with the zotero library and allows you to link all the article and is very helpful when you want to write about any topic."
  },
  {
    "objectID": "posts/phd_tools/index.html#literature-graph",
    "href": "posts/phd_tools/index.html#literature-graph",
    "title": "Tools for Phd",
    "section": "Literature Graph",
    "text": "Literature Graph\nConnected papers : not free. Lit map could be an alternative."
  },
  {
    "objectID": "posts/phd_tools/index.html#redaction",
    "href": "posts/phd_tools/index.html#redaction",
    "title": "Tools for Phd",
    "section": "Redaction",
    "text": "Redaction\nmy favorite software of this stack Quarto. It allow you to write anything in markdown and generate pdfs (with latex or typst), beamers or revealjs and this blog your are currently reading !\nIt is an amazing piece of software based on Pandoc the Haskell unicorn.\nIf you are in stats or working with any type of data, quarto is nice since you can code in the same file as the one you write in. It is like a notebook but plain text, that allows version control systems to follow it nicely."
  },
  {
    "objectID": "posts/cifar_classif/index.html",
    "href": "posts/cifar_classif/index.html",
    "title": "Classify CIFAR",
    "section": "",
    "text": "CIFAR is a trivial problem in image classification. We will be using Pytorch and lightning in order to do the training.\nThe advantage of this approach, is that the workflow can be done locally one the cpu of your computer or on ten H100 of any cloud you could get access to.\nLightning handles the location of data and optimization related objects (model, optimizer, scheduler etcâ€¦), and last be not least, the metrics computation done with torchmetrics.\nThe metrics have the gathering across gpus/device already implemented so you just have to decide of which ones you want to add to your project. If some computations are not already present in the library, you can add your own metric very easily."
  },
  {
    "objectID": "posts/cifar_classif/index.html#the-data",
    "href": "posts/cifar_classif/index.html#the-data",
    "title": "Classify CIFAR",
    "section": "The data",
    "text": "The data\n\nimport torchvision\nimport torch\nimport matplotlib.pyplot as plt\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\n#Â to make the transform usable by torchvision dataset it needs to be a function that takes an image as input and return an image as well\n\n\n\ndef train_trans(image)-&gt;torch.tensor:\n    transform = A.Compose([\n        A.HorizontalFlip(),\n        A.Normalize(),\n        ToTensorV2()\n    ]) \n\n    transformed = transform(image = np.array(image))\n\n    return transformed[\"image\"]\n\ndef test_trans(image)-&gt;torch.tensor:\n    transform = A.Compose([\n        A.Normalize(),\n        ToTensorV2()\n    ]) \n\n    transformed = transform(image = np.array(image))\n\n    return transformed[\"image\"]\n\ntrain_set = torchvision.datasets.CIFAR10(\n    root=\"data\", \n    download=True, \n    train=True,\n    transform=train_trans)\n\nval_set = torchvision.datasets.CIFAR10(\n    root=\"data\", \n    download=True, \n    train=False,\n    transform=test_trans)\n\ntrain_loader= torch.utils.data.DataLoader(\n    train_set,\n    # shuffle=True,\n    sampler = torch.utils.data.SubsetRandomSampler(np.random.choice(len(train_set), 10000)),\n    batch_size=64,\n    num_workers=5,\n\n)\n\nval_loader= torch.utils.data.DataLoader(\n    val_set,\n    shuffle=False,\n    batch_size=64*2,\n    num_workers=5,\n)"
  },
  {
    "objectID": "posts/cifar_classif/index.html#the-model",
    "href": "posts/cifar_classif/index.html#the-model",
    "title": "Classify CIFAR",
    "section": "The model",
    "text": "The model\n\n\nCode\nimport lightning as L\nfrom typing import Optional, List\nimport torchmetrics\nfrom omegaconf import DictConfig, OmegaConf\n\nclass ClassificationModule(L.LightningModule):\n    def __init__(\n        self, \n        categories :List[str],\n        config:DictConfig,\n        model: Optional[torch.nn.Module] = None, \n        ):\n        \n        super().__init__()\n        self.categories = categories\n        num_classes = len(categories)\n        self.config = config\n        if model is None:\n            self.model = torchvision.models.resnet18(num_classes=num_classes)\n\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n        metrics = torchmetrics.MetricCollection([\n            torchmetrics.classification.Accuracy(task = \"multiclass\", num_classes = num_classes),\n            torchmetrics.F1Score(task = \"multiclass\", num_classes = num_classes),\n            torchmetrics.Precision(task = \"multiclass\", num_classes = num_classes),\n            torchmetrics.Recall(task = \"multiclass\", num_classes = num_classes),\n            torchmetrics.CalibrationError(task = \"multiclass\", num_classes = num_classes),\n        ])\n\n        self.train_metric = metrics.clone(prefix=\"Train/\")\n        self.val_metrics = metrics.clone(prefix=\"Validation/\")\n        self.test_metrics = metrics.clone(prefix=\"Test/\")\n\n        self.per_category_metrics = torchmetrics.MetricCollection([\n            torchmetrics.classification.Accuracy(task = \"multiclass\", num_classes = num_classes, average = None),\n            torchmetrics.F1Score(task = \"multiclass\", num_classes = num_classes, average = None),\n            torchmetrics.Precision(task = \"multiclass\", num_classes = num_classes, average = None),\n            torchmetrics.Recall(task = \"multiclass\", num_classes = num_classes, average = None),\n        ])\n\n    def forward(self, X):\n        return self.model(X)\n\n    def configure_optimizers(self):\n\n        #Â Define Optimizer here\n        optimizer = torch.optim.Adam(self.parameters(), lr = self.config.lr, weight_decay=1e-5)\n\n        #Â you cna add a scheduler here as well and return it as \n        # return [optimizer], [scheduler]\n        # \n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        images, targets = batch\n\n        outputs = self(images)\n\n        loss = self.criterion(outputs, targets)\n        \n        self.train_metric(outputs, targets)\n\n        self.log(\"Train/Loss\",loss, on_epoch=True, on_step=True, prog_bar=True)\n\n        return loss\n    \n    def on_train_epoch_end(self):\n\n        train_metrics=  self.train_metric.compute()\n\n        self.log_dict(train_metrics)\n\n        self.train_metric.reset()\n    \n    def validation_step(self, batch, batch_idx):\n        images, targets = batch\n\n        outputs = self(images)\n\n        loss = self.criterion(outputs, targets)\n        self.log(\"Validation/Loss\", loss, on_epoch=True, on_step=False)\n\n        self.val_metrics(outputs, targets)\n        self.per_category_metrics(outputs, targets)\n\n        \n    \n    def on_validation_epoch_end(self):\n\n        val_metrics =  self.val_metrics.compute()\n\n        self.log_dict(val_metrics)\n\n        m = self.per_category_metrics.compute()\n        for mname, mresults in m.items():\n            for i, catname in enumerate(self.categories):\n                self.log(f\"Validation/{mname}_{catname}\", mresults[i])\n\n        self.val_metrics.reset()\n        self.per_category_metrics.reset()\n    \n\n    def test_step(self, batch, batch_idx):\n        images, targets = batch\n\n        outputs = self(images)\n\n        loss = self.criterion(outputs, targets)\n        self.log(\"Test/Loss\", loss, on_epoch=True, on_step=False)\n\n        self.test_metrics(outputs, targets)\n        self.per_category_metrics(outputs, targets)\n\n        \n    \n    def on_test_epoch_end(self):\n\n        test_metrics =  self.test_metrics.compute()\n\n        self.log_dict(test_metrics)\n        m = self.per_category_metrics.compute()\n        for mname, mresults in m.items():\n            for i, catname in enumerate(self.categories):\n                self.log(f\"Validation/{mname}_{catname}\", mresults[i])\n\n        self.test_metrics.reset()\n        self.per_category_metrics.reset()\n\n\nconfig = OmegaConf.create({\n    \"lr\": 1e-5\n})\n\nmodel = ClassificationModule(\n    categories=train_set.classes,\n    config=config\n)\n\n\n##Â Use everything for train\n\ntrainer=  L.Trainer(\n    max_epochs=3,\n    precision = \"16-mixed\",\n    enable_checkpointing=True,\n    num_sanity_val_steps=2,\n    log_every_n_steps=50,\n    check_val_every_n_epoch=1,\n)\n\n# trainer.fit(\n#     model,\n#     train_loader,\n#     val_loader\n# )\n\nUsing 16bit Automatic Mixed Precision (AMP)\nUsing default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs"
  },
  {
    "objectID": "posts/cifar_classif/index.html#and-it-is-done",
    "href": "posts/cifar_classif/index.html#and-it-is-done",
    "title": "Classify CIFAR",
    "section": "And it is Done !",
    "text": "And it is Done !\nThe weights of the model are saved with the config that produced them."
  }
]