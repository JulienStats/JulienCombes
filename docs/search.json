[
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Bio",
    "section": "",
    "text": "I am currently pursuing a Phd under the supervision of Jean-Francois Coeurjolly and Alexandre Dervill√© which takes place between the R&D headquarters of Michelin in Clermont-Ferrand, and the LJK in Grenoble. I am interested in active learning and uncertainty quantification applied to machine vision.\nI graduated from Grenoble University in Statistics and data science.\nI had a one year break in Omuta, Japan."
  },
  {
    "objectID": "linux.html",
    "href": "linux.html",
    "title": "Linux",
    "section": "",
    "text": "Bonjour √† tous !\nMerci de vous int√©resser a des alternatives √† Microsoft. Je compte r√©pertorier ici tous les probl√®mes qui arrivent lors des installations de syst√®mes linux. Je me restreins aujourd‚Äôhui √† Linux Mint, c‚Äôest la distribution que je trouve la plus stable et la plus compatible avec le mat√©riel de toutes les g√©n√©rations.\nLa version que je conseille d‚Äôinstaller si vous n‚Äôavez pas de carte NVIDIA est LMDE, c‚Äôest une version qui sera tr√®s stable et modulable via l‚Äôinstallation des logiciel avec flatpak.\n\nBootable device not found please insert a bootable media, press enter to shutdown the computer\nJ‚Äôai eu cette erreur une fois sur un tr√®s vieil appareil. Le probl√®me est que le bios est incompatible avec les OS d‚Äôapr√®s 2010. J‚Äôai trouv√© la solution dans cet article (en anglais).\nLes √©tapes succintes :\n\npasser le bios en legacy mode\ncr√©er une table de partition de type msdos dans GPARTED (lorsque vous etes boot√© sur la cl√©)\nformatez la partition classique en Ext4.\n\nEt l√§ tout va rouler\n\n\nInstaller des logiciels windows non disponibles sur Linux nativement\nPour les enseignants, vous aurez probablement besoin du client pronote (le service Web n‚Äôas pas toutes les fonctionalit√©s) d‚Äô√©mulateur de calculatrice graphique ou autre. Ces logiciels ne sont fournit que sur windows, c‚Äôest √† dire que vous aurez un fichier .exe √† executer.\nPas de probl√®mes, Linux dispose d‚Äôun outil appell√© bouteilles, qui permet d‚Äô√©muler un syst√®me windows et de lire vos ex√©cutables. Vous pouvez la t√©l√©charger via le flathub en passant par cette ligne de commande :\nflatpak install flathub com.usebottles.bottles\n\n\n\n\n\n\nWarning\n\n\n\nPour que le client pronote fonctionne, il ne faut pas prendre le lanceur par d√©faut (soda) mais choisir wine. Tout devrait fonctionner sans probl√®mes.\n\n\nPour les traitement de texte j‚Äôavais l‚Äôhabitude de conseiller la suite libreoffice, mais si vous venez de windows, l‚Äôid√©al est la suite onlyoffice qui est beaucoup plus accessible, elle est disponible dans la plupart des logit√®ques :)."
  },
  {
    "objectID": "posts/research_ECAS2025/index.html",
    "href": "posts/research_ECAS2025/index.html",
    "title": "ECAS 2025",
    "section": "",
    "text": "Winter school where i learned about Conformal prediction (CP) and Transfert Learning. ECAS : European course in advanced statistics."
  },
  {
    "objectID": "posts/research_ECAS2025/index.html#conformal-prediction",
    "href": "posts/research_ECAS2025/index.html#conformal-prediction",
    "title": "ECAS 2025",
    "section": "Conformal Prediction",
    "text": "Conformal Prediction\nThis is the field i want to become good at. Indeed, my PA fell in love with this field and im afraid i did too during this week. I met so nice people working on it and making the best use of this theoretical tool in the industry i want to make our computer vision more reliable as well.\nAll the course and the slides are in this github repo. The slides are clear and all the proofs are written as well.\nComplete book with theoretical proofs and explanations : (A. N. Angelopoulos, Barber, and Bates 2025).\nShort Summary:\nLinear regression with gaussian hypothesis allow a good uncertainty quantification (UQ) using gaussian hypotheses for parameters and target variable. Some situation where models are bigger and does not follow any assumptions over parameter values makes UQ impossible using any assumption on model or errors. That is the problem CP is solving, given one only hypotheses (exchangeability of non conformity scores) it allows a good UQ which is distribution free (no assumpotion over the data distribution) and model agnostic (independent on the estimator). This is very good because in deep learning we litteraly have no tool to quantify uncertainty with statistical guarantees.\nGiven a labeled dataset of size n \\((X_i,Y_i)^n_{i=1}\\), we want to be able to predict a new point \\(X_{n+1}\\) with confidence.\nMeaning that for any user defined risk \\(\\alpha \\in [0,1]\\), \\(\\mathcal{P}\\{Y_{n+1} \\in \\mathcal{C}_{\\alpha}(X_{n+1})\\} \\geq 1 - \\alpha\\). \\(\\mathcal{C}_{\\alpha}\\) should be as small as possible to be informative. Indeed the case where \\(\\mathcal{C}_{\\alpha} = \\mathcal{Y}\\) verifies the \\(1-\\alpha\\) coverage but provides no information about the predictive uncertainty.\n\n\n\n\n\n\nNoteExchangeability\n\n\n\n\\((Z_i)^n_{i=1}\\) a random vector taking values in \\(\\mathcal{Z}^n\\) is exchangeable if, for any permutation \\(\\sigma\\) of \\([0,1]\\) :\n\\[(Z_1, ..., Z_n) \\stackrel{d}{=} (Z_{\\sigma(1)}, ..., Z_{\\sigma(n)})\\]\n\n\nhow does it work ? (Split conformal Prediction setting)\nIn machine vision, our models are so big we can‚Äôt afford doing LOO Cross validation or cross validation with Full Conformal Paradigm. We will focus on the Split Case.\n\nSplit your date into Train, Calibration and Test splits\nTrain A to get \\(\\hat{\\mu}\\) on Train\nCompute \\(\\hat{\\mu}\\) on Cal\nObtain a set of non conformity scores\n\n\\[S = \\{S_i = |\\hat{\\mu}(X_i) - Y_i|, i \\in Cal \\} \\cup \\{+\\infty \\}\\]\n\nCompute the \\(1 - \\alpha\\) quantile of these scores, noted \\(q_{1-\\alpha}(S)\\)\nFor a new point \\(X_{n+1}\\), return :\n\n\\[\\hat{C}_{\\alpha}(X_{n+1}) = [\\hat{\\mu}(X_{n+1})- q_{1-\\alpha}(S), \\hat{\\mu}(X_{n+1})+ q_{1-\\alpha}(S)]\\]\nThe guarantee of those predictor are the following :\n\\(\\mathbb{P}\\{ Y_{n+1} \\in \\mathcal{C}_{\\alpha}(X_{n+1}) \\} = 1 - \\alpha\\) This is the marginal guarantee, meaning that for all our predictions, at least \\((1-\\alpha)\\%\\) of the test points will be covered. But there might be some sub categories that are not covered at all.\n\n\n\n\n\n\nNote\n\n\n\nIn active learning we want to estimate the uncertainty of the model for one specific data point. What we would like to have is conditional converage, being :\n\\(\\mathbb{P}\\{ Y_{n+1} \\in \\mathcal{C}_{\\alpha}(X_{n+1})| X_{n+1} \\} = 1 - \\alpha\\).\nThis is adaptative methods. Some paper made adaptative method for classification with very strong guarantees. (see section discussion with litterature recommendations recarding this matter).\n\n\n\n\n\nIllustration of different coverages. (Again taken from the notes of Margaux)"
  },
  {
    "objectID": "posts/research_ECAS2025/index.html#transfer-learning",
    "href": "posts/research_ECAS2025/index.html#transfer-learning",
    "title": "ECAS 2025",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nThis field sounds specific but actually any statistician working with real world data will faced generalization problem one day or another.\nAll the material is in this github repo"
  },
  {
    "objectID": "posts/research_ECAS2025/index.html#discussions",
    "href": "posts/research_ECAS2025/index.html#discussions",
    "title": "ECAS 2025",
    "section": "Discussions",
    "text": "Discussions\nFor classification (Romano, Sesia, and Cand√®s 2020) provide very strong guarantee with adaptivity. That is what we need in active learning. RAPS Extensions with (A. Angelopoulos et al. 2022) improve this with smaller prediction sets. This gives hope in the use of CP in AL. When some defects are very rare conformal prediction garanteeing only marginal coverage some classes might not be covered (no conditional coverage in general cases), she recommanded me the work of Tiffany Ding (Ding, Fermanian, and Salmon 2025) in collaboration with Plantnet.\nShe suggest Y-conditional conformal algorithms to guarantee that the coverage is verified for any classes with this paper (Ding et al. 2023).\nThis is still an area at the research stage but some solution exists to tackle our problems. We shall not forget that most of people using big neural network do not quantify the uncertainty at all. So it is nice to do the first step.\n\nAntoine was so nice to give his insight about his past with active learning and domain transfert i really want to thank him again. He agree that active learning doesn‚Äôt beat random in most cases, but he believes more in AL than in domain adaptation‚Ä¶. Let‚Äôs do our best !\nIt is normal for Al to bias the distribution, because if we didn‚Äôt want to bias it we would have stayed with random sampling.\n\n\n\n\n\n\nNote\n\n\n\nHe prefers k-medoids over core-set selection. It must be similar to typiclust so i will work on that and see how i can make my best of it.\n\n\nHe liked the idea of the potato project and was surprised that the AL could work that well in some cases. (Might be a bug ? i hope not‚Ä¶). He is not surprised that transfert in strategy independent. Indeed, when there is a distribution shift it is likely that nothing will help you in most real data cases. He notes that even for \\(\\pi^u=10%\\) AL can generalize well and beat random."
  },
  {
    "objectID": "posts/cifar_classif/index.html",
    "href": "posts/cifar_classif/index.html",
    "title": "Classify CIFAR",
    "section": "",
    "text": "CIFAR is a trivial problem in image classification. We will be using Pytorch and lightning in order to do the training.\nThe advantage of this approach, is that the workflow can be done locally on the cpu of your computer or on ten H100 of any cloud you could get access to.\nLightning handles the location of data and optimization related objects (model, optimizer, scheduler etc‚Ä¶), and last be not least, the metrics computation done with torchmetrics.\nThe metrics have the gathering across gpus/devices already implemented so you just have to decide of which ones you want to add to your project. If some computations are not already present in the library, you can add your own metric very easily."
  },
  {
    "objectID": "posts/cifar_classif/index.html#the-data",
    "href": "posts/cifar_classif/index.html#the-data",
    "title": "Classify CIFAR",
    "section": "The data",
    "text": "The data\n\nimport torchvision\nimport torch\nimport matplotlib.pyplot as plt\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\nimport os\n#¬†to make the transform usable by torchvision dataset it needs to be a function that takes an image as input and return an image as well\n\ndatarootdir = os.environ.get(\"DATA_ROOTPATH\")\n\ndef train_trans(image)-&gt;torch.tensor:\n    transform = A.Compose([\n        A.HorizontalFlip(),\n        A.Normalize(),\n        ToTensorV2()\n    ]) \n\n    transformed = transform(image = np.array(image))\n\n    return transformed[\"image\"]\n\ndef test_trans(image)-&gt;torch.tensor:\n    transform = A.Compose([\n        A.Normalize(),\n        ToTensorV2()\n    ]) \n\n    transformed = transform(image = np.array(image))\n\n    return transformed[\"image\"]\n\ntrain_set = torchvision.datasets.CIFAR10(\n    root=os.path.join(datarootdir, \"cifar10\"), \n    download=True, \n    train=True,\n    transform=train_trans)\n\nval_set = torchvision.datasets.CIFAR10(\n    root=os.path.join(datarootdir, \"cifar10\"), \n    download=True, \n    train=False,\n    transform=test_trans)\n\ntrain_loader= torch.utils.data.DataLoader(\n    train_set,\n    # shuffle=True,\n    sampler = torch.utils.data.SubsetRandomSampler(np.random.choice(len(train_set), 10000)),\n    batch_size=64,\n    num_workers=5,\n\n)\n\nval_loader= torch.utils.data.DataLoader(\n    val_set,\n    shuffle=False,\n    batch_size=64*2,\n    num_workers=5,\n)"
  },
  {
    "objectID": "posts/cifar_classif/index.html#the-model",
    "href": "posts/cifar_classif/index.html#the-model",
    "title": "Classify CIFAR",
    "section": "The model",
    "text": "The model\n\n\nCode\nimport lightning as L\nfrom typing import Optional, List\nimport torchmetrics\nfrom omegaconf import DictConfig, OmegaConf\n\nclass ClassificationModule(L.LightningModule):\n    def __init__(\n        self, \n        categories :List[str],\n        config:DictConfig,\n        model: Optional[torch.nn.Module] = None, \n        ):\n        \n        super().__init__()\n        self.categories = categories\n        num_classes = len(categories)\n        self.config = config\n        \n        self.model = model\n        if model is None:\n            self.model = torchvision.models.resnet18(num_classes=num_classes)\n\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n        metrics = torchmetrics.MetricCollection([\n            torchmetrics.classification.Accuracy(task = \"multiclass\", num_classes = num_classes),\n            torchmetrics.F1Score(task = \"multiclass\", num_classes = num_classes),\n            torchmetrics.Precision(task = \"multiclass\", num_classes = num_classes),\n            torchmetrics.Recall(task = \"multiclass\", num_classes = num_classes),\n            torchmetrics.CalibrationError(task = \"multiclass\", num_classes = num_classes),\n        ])\n\n        self.train_metric = metrics.clone(prefix=\"Train/\")\n        self.val_metrics = metrics.clone(prefix=\"Validation/\")\n        self.test_metrics = metrics.clone(prefix=\"Test/\")\n        \n\n        # conditional performances of our estimator\n        self.per_category_metrics = torchmetrics.MetricCollection([\n            torchmetrics.classification.Accuracy(task = \"multiclass\", num_classes = num_classes, average = None),\n            torchmetrics.F1Score(task = \"multiclass\", num_classes = num_classes, average = None),\n            torchmetrics.Precision(task = \"multiclass\", num_classes = num_classes, average = None),\n            torchmetrics.Recall(task = \"multiclass\", num_classes = num_classes, average = None),\n        ])\n\n    def forward(self, X):\n        return self.model(X)\n\n    def configure_optimizers(self):\n\n        optimizer = torch.optim.Adam(self.parameters(), lr = self.config.lr, weight_decay=1e-5)\n\n        #¬†you can add a scheduler here as well and return it as \n        # return [optimizer], [scheduler]\n        # \n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        images, targets = batch\n\n        outputs = self(images)\n\n        loss = self.criterion(outputs, targets)\n        \n        self.train_metric(outputs, targets)\n\n        self.log(\"Train/Loss\",loss, on_epoch=True, on_step=True, prog_bar=True)\n\n        return loss\n    \n    def on_train_epoch_end(self):\n\n        train_metrics=  self.train_metric.compute()\n\n        self.log_dict(train_metrics)\n\n        self.train_metric.reset()\n    \n    def validation_step(self, batch, batch_idx):\n        images, targets = batch\n\n        outputs = self(images)\n\n        loss = self.criterion(outputs, targets)\n        self.log(\"Validation/Loss\", loss, on_epoch=True, on_step=False)\n\n        self.val_metrics(outputs, targets)\n        self.per_category_metrics(outputs, targets)\n\n        \n    \n    def on_validation_epoch_end(self):\n\n        val_metrics =  self.val_metrics.compute()\n\n        self.log_dict(val_metrics)\n\n        m = self.per_category_metrics.compute()\n        for mname, mresults in m.items():\n            for i, catname in enumerate(self.categories):\n                self.log(f\"Validation/{mname}_{catname}\", mresults[i])\n\n        self.val_metrics.reset()\n        self.per_category_metrics.reset()\n    \n\n    def test_step(self, batch, batch_idx):\n        images, targets = batch\n\n        outputs = self(images)\n\n        loss = self.criterion(outputs, targets)\n        self.log(\"Test/Loss\", loss, on_epoch=True, on_step=False)\n\n        self.test_metrics(outputs, targets)\n        self.per_category_metrics(outputs, targets)\n\n        \n    \n    def on_test_epoch_end(self):\n\n        test_metrics =  self.test_metrics.compute()\n\n        self.log_dict(test_metrics)\n        m = self.per_category_metrics.compute()\n        for mname, mresults in m.items():\n            for i, catname in enumerate(self.categories):\n                self.log(f\"Validation/{mname}_{catname}\", mresults[i])\n\n        self.test_metrics.reset()\n        self.per_category_metrics.reset()\n\n\nconfig = OmegaConf.create({\n    \"lr\": 1e-5\n})\n\nmodel = ClassificationModule(\n    categories=train_set.classes,\n    config=config\n)"
  },
  {
    "objectID": "posts/cifar_classif/index.html#use-everything-for-training",
    "href": "posts/cifar_classif/index.html#use-everything-for-training",
    "title": "Classify CIFAR",
    "section": "Use everything for training !",
    "text": "Use everything for training !\n\ntrainer=  L.Trainer(\n    max_epochs=3,\n    precision = \"16-mixed\",\n    enable_checkpointing=True,\n    num_sanity_val_steps=2,\n    log_every_n_steps=50,\n    check_val_every_n_epoch=1,\n)\n\n# trainer.fit(\n#     model,\n#     train_loader,\n#     val_loader\n# )\n\nUsing 16bit Automatic Mixed Precision (AMP)\nüí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores"
  },
  {
    "objectID": "posts/cifar_classif/index.html#and-it-is-done",
    "href": "posts/cifar_classif/index.html#and-it-is-done",
    "title": "Classify CIFAR",
    "section": "And it is Done !",
    "text": "And it is Done !\nThe weights of the model are saved with the config that produced them."
  },
  {
    "objectID": "posts/research_PAISS2025/index.html",
    "href": "posts/research_PAISS2025/index.html",
    "title": "PAISS 2025",
    "section": "",
    "text": "Summer school about Machine Learning Where i presented a poster about the effect of data imbalance on active using on two industrial open source semantic segmentation datasets."
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#talk-1-progress-and-prospects-in-learning-optimization-control-and-simulation-for-robotics-justin-carpentier",
    "href": "posts/research_PAISS2025/index.html#talk-1-progress-and-prospects-in-learning-optimization-control-and-simulation-for-robotics-justin-carpentier",
    "title": "PAISS 2025",
    "section": "Talk 1 : Progress and Prospects in Learning, Optimization, Control and Simulation for Robotics (Justin Carpentier)",
    "text": "Talk 1 : Progress and Prospects in Learning, Optimization, Control and Simulation for Robotics (Justin Carpentier)\nhttps://mybox.inria.fr/f/7f7567f241434eb9a0c2/?dl=1\nThree way of designing software to solve robotics problems :\n\nOptimal control : Require clear definition fo the problem we want to solve, give the best solutions\nPolicy Learning : Reinforcement Learning (Not Deep)\nVision language action flow model for general robot control\n\nControl policies require simulation to train policies. A good reprensentation the world the robot will live in is essential. Trainng Deep RL is cotly and involve huge carbon footprint.\nSolution proposed : Less data + exploiting gradients\nGJK algorithm (compute distance between 2 object of any arbitrary shape efficiently) (GJK++)"
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#talk-2-retrieving-generating-and-refining-for-web-scale-ahmet-iscen",
    "href": "posts/research_PAISS2025/index.html#talk-2-retrieving-generating-and-refining-for-web-scale-ahmet-iscen",
    "title": "PAISS 2025",
    "section": "Talk 2 : Retrieving, Generating, and Refining for Web-Scale (Ahmet Iscen)",
    "text": "Talk 2 : Retrieving, Generating, and Refining for Web-Scale (Ahmet Iscen)\nhttps://paiss.inria.fr/files/2025/09/PAISS-2025-Summer-School.pdf\nFine grained classification is classification with a precise description of what there is in an image (not bird but what specy of bird.)\n\nMultimodle situations (Text/image) (Iscen et al. 2024)\nGERALD (Caron, Iscen, et al. 2024)\nLLM data curation (Caron, Fathi, et al. 2024)"
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#talk-3-llm-training-and-inference-efficiency-jeremy-reizenstein",
    "href": "posts/research_PAISS2025/index.html#talk-3-llm-training-and-inference-efficiency-jeremy-reizenstein",
    "title": "PAISS 2025",
    "section": "Talk 3 : LLM training and inference efficiency (Jeremy Reizenstein)",
    "text": "Talk 3 : LLM training and inference efficiency (Jeremy Reizenstein)\n\n[x]\n\n\nDon‚Äôt do LLMs (Yann LeCun)"
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#poster-session",
    "href": "posts/research_PAISS2025/index.html#poster-session",
    "title": "PAISS 2025",
    "section": "Poster session",
    "text": "Poster session\nI was presenting so i could not see the other ones. Advice from the people who camed to see me :\n\ntry to get the embeddings from images using Dinov3 without SSL pretraining\nchange the backbone of the mask rcnn to be able to plug in a pre trained backbone.\nadding other SOTA method (BALD) and benchmark on SOTA datasets instead of potatoes\nAdding SSL methods to see how the imbalance affects ssl pre-training (MAE, I-JEIPA)"
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#talk-1-diffusion-flows-and-optimal-transport-in-machine-learning-gabriel-peyr√©",
    "href": "posts/research_PAISS2025/index.html#talk-1-diffusion-flows-and-optimal-transport-in-machine-learning-gabriel-peyr√©",
    "title": "PAISS 2025",
    "section": "Talk 1 : Diffusion Flows and Optimal Transport in Machine Learning (Gabriel Peyr√©)",
    "text": "Talk 1 : Diffusion Flows and Optimal Transport in Machine Learning (Gabriel Peyr√©)\n\nSemiSlides : https://speakerdeck.com/gpeyre/computational-ot-number-4-gradient-flow-and-diffusion-models?slide=22 Code : https://github.com/gpeyre/ot4ml/blob/main/README.md\nDistribution of what ?\n\npoints (flow matching)\nneurons\ntokens ‚Ä¶\n\nTways of representing distributions :\nDistribution of points (eulerian \\(\\alpha_t\\)) VS Distribution of vector fields \\(v_t\\) (Lagrangian : points + behaviour).\nGo from lagrangian to eulerian is easy, not the opposite. \\[div(\\alpha_t v_t) + \\frac{\\partial \\alpha_t}{dt} = 0\\]\nHow to go from \\(\\alpha_t\\) to \\(v_t\\) ?\n\nOtto Calculus (having \\(\\alpha_t\\))\nStochastic interpolant (i don‚Äôt have \\(\\alpha_t\\))\nWasserstein distance\nDiffusion\nWasserstein Gradient Flow"
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#talk-2-learning-to-control-an-introduction-to-reinforcement-learning-claire-vernade",
    "href": "posts/research_PAISS2025/index.html#talk-2-learning-to-control-an-introduction-to-reinforcement-learning-claire-vernade",
    "title": "PAISS 2025",
    "section": "Talk 2 : Learning to Control: An Introduction to Reinforcement Learning (Claire Vernade)",
    "text": "Talk 2 : Learning to Control: An Introduction to Reinforcement Learning (Claire Vernade)\nRL is not always Deep !\nControl theory\nRL approximate bellmann operations"
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#talk-3-a-collectivist-economic-perspective-on-ai-michael-jordan",
    "href": "posts/research_PAISS2025/index.html#talk-3-a-collectivist-economic-perspective-on-ai-michael-jordan",
    "title": "PAISS 2025",
    "section": "Talk 3 : A Collectivist, Economic Perspective on AI (Michael Jordan)",
    "text": "Talk 3 : A Collectivist, Economic Perspective on AI (Michael Jordan)\nWhat is intelligence ? The most basic form of intelligence are free markets."
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#poster-session-1",
    "href": "posts/research_PAISS2025/index.html#poster-session-1",
    "title": "PAISS 2025",
    "section": "Poster session",
    "text": "Poster session\n\nlink knowledge graph with image embedding to give meaning to images\nCreating a method to compare the power efficency for a given model to select the one that has required performance with the least amount of energy consumed. use of evchenko measure"
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#talk-1-ai-security-l√™-nguy√™n-hoang",
    "href": "posts/research_PAISS2025/index.html#talk-1-ai-security-l√™-nguy√™n-hoang",
    "title": "PAISS 2025",
    "section": "Talk 1 : AI Security (L√™ Nguy√™n Hoang)",
    "text": "Talk 1 : AI Security (L√™ Nguy√™n Hoang)\nIt‚Äôs possible to recover training data from any trained DL model. Three weekness of DL models :\n\nData Exfiltration\nEvasion\nPoisoning\n\nAgentic AI dramaticaly increases those risks.\n5 things to do to protect DL systems to break\n\nContinuous monitoring\nSandboxing with least privilege\nRedundancy (Byzantine aggregation rule)\nReducing the attack surface (Data Taggant)\nHR upskilling"
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#talk-2-governance-of-ai-carina-prunkl",
    "href": "posts/research_PAISS2025/index.html#talk-2-governance-of-ai-carina-prunkl",
    "title": "PAISS 2025",
    "section": "Talk 2 : governance of AI (Carina Prunkl)",
    "text": "Talk 2 : governance of AI (Carina Prunkl)\n4 risks of using AI systems :\n\nmisusage\nunexpected behaviour\nsystemic risks\nFairness\n\nWho is accountable if an AI system breaks ?\nRuling : EU AI Act\nRisk scale : minimal &lt; &lt; high &lt; unacceptable\nNIST AI : risk management framework\nRegulation is not always the answer for AI risks :\n\nhigh risk uncertainty\ncultural norms\ncomplex or context dependant issues\nenforcement impossible\n\nCorporate governance is not recommandable unless :\n\nCommittment free of ‚Ä¶\nthird party monitoring\nthird party enforcement\nPublic scrutiny\n\nMidstream Governance :\nNeurips add a section where researchers have to suggest what uses (good or bad) could be done by their research."
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#talk-3-intro-to-stat-fairness-solenne-gaucher",
    "href": "posts/research_PAISS2025/index.html#talk-3-intro-to-stat-fairness-solenne-gaucher",
    "title": "PAISS 2025",
    "section": "Talk 3 : Intro to stat fairness (Solenne Gaucher)",
    "text": "Talk 3 : Intro to stat fairness (Solenne Gaucher)\nFairness with awareness or unawareness. Awareness require the discriminant data to be collect to check of a discrimination is in place regarding thos criterion.\nIndividual fairness != Group level fairness;\nFairness properties in classification : \\[(X, S, Y) \\in \\mathbb{R}^d \\times [K] \\times \\{0, 1 \\}\\]\nX : Resume, S : group, Y : is the person qualified\n\nDemographic parity (DP):\n\n\\[P(g(Z)=1|S=s) = P(g(Z)=1) \\forall s \\in K\\]\n\nEquality of opportunity (EO)\n\n\\[P(g(Z)=1|S=s, Y=1) =  P(g(Z)=1|Y=1)\\]\nFairness properties in Regression :\n\nDP, Separation, Sufficiency"
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#talk-4-ai-ethics-in-practice-mariia-vladimirova",
    "href": "posts/research_PAISS2025/index.html#talk-4-ai-ethics-in-practice-mariia-vladimirova",
    "title": "PAISS 2025",
    "section": "talk 4 : AI ethics in practice (Mariia Vladimirova)",
    "text": "talk 4 : AI ethics in practice (Mariia Vladimirova)\nhttps://paiss.inria.fr/files/2025/09/vladimirova_paiss25_tutorial.pdf\nhttps://github.com/fairlearn/fairlearn"
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#talk-1-data-driven-3d-vision-jerome-revaud",
    "href": "posts/research_PAISS2025/index.html#talk-1-data-driven-3d-vision-jerome-revaud",
    "title": "PAISS 2025",
    "section": "Talk 1 : Data-Driven 3D Vision (Jerome revaud)",
    "text": "Talk 1 : Data-Driven 3D Vision (Jerome revaud)\nhttps://paiss.inria.fr/files/2025/09/3dpres.pdf\nSSl methods that works for 3d representation with a 2d acquisition.\nThey made impossible matching possible ! What the fuck guys ! Insane\nTraditional CV (COLMAP): Establishing correspondances amoung multive views of the same scene : Structure from motion\n Require mulstple image for correspondance !!\n\n\n\nImpossible mathching\n\n\nNeed for a fundational model of 3D reconstruction, what we want this model to do ?\n\nestablish correspondance between images\ninfer 3D geometry\ninfer relative camera poes\ndecompose motion and lighting\n\nWe need a pretext task to make sure the model will become able to solve those tasks !\n\n\n\nCroco : Cross view completion\n\n\nNow need to fine tune this ! Because fondation models are useles, they only have BIG BRAIN\n\n\n\n\nmamacita la team CVPR BIG BRAIN\n\n\netc etc‚Ä¶.\n\n\n\n\n\n\nNote\n\n\n\nEuh ???? Why MAE when you know its shit ? MDR Jeipa ü§£"
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#talk-2-world-models-yann-lecun",
    "href": "posts/research_PAISS2025/index.html#talk-2-world-models-yann-lecun",
    "title": "PAISS 2025",
    "section": "Talk 2 : World Models (Yann LeCun)",
    "text": "Talk 2 : World Models (Yann LeCun)\nAutoregressive models SUCKS and are DOOMED.\nThe errors increase whith the number of steps ahead you are trying to predict. that is why we have to work on the prediction on the full sequence instead of the next token.\n\n\n\nMAE generate all the data and details including irrelvant ones. The SSL models need to thinks like HUMANS in an abstract space\n\n\nAGI is a bad namingand should be replaced by AMI (Advanced Machine Inteligence)\nI didn‚Äôt understand that well but the second part is about about energy based models :\n\nThe last part is about world models, basically i understood the folowing, autoregressive methods have the problem of increasing error with the number of predicted steps. While the world models predict the full sequence until the end of the experiment.\n\n\n\n\n\n\n\nNote\n\n\n\nFor SSl pre-training its preferable to use IJEPA models."
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#talk-3-video-understanding-out-of-the-frame---an-egocentric-perspective-dima-damen",
    "href": "posts/research_PAISS2025/index.html#talk-3-video-understanding-out-of-the-frame---an-egocentric-perspective-dima-damen",
    "title": "PAISS 2025",
    "section": "Talk 3 : Video Understanding Out of the Frame - an Egocentric Perspective (Dima Damen)",
    "text": "Talk 3 : Video Understanding Out of the Frame - an Egocentric Perspective (Dima Damen)\nhttps://dimadamen.github.io/pdfs/PAISS2025-90min-Tutorial-DimaDamen.pdf\nEgocentric dataset and study for the use case of 2050 technologies (glasses cameras, augmented reality)\nHow to label egocentric videos/objects ? What is an egg ? full egg ? cracked egg ?\n\nMulti modal learning,\nCooking-recipe linked to egocentric videos of people doing the recipe while explaining what they do."
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#poster-session-2",
    "href": "posts/research_PAISS2025/index.html#poster-session-2",
    "title": "PAISS 2025",
    "section": "Poster session",
    "text": "Poster session\nModel assisted Labeling for small objects with pretrained transformers.\nSolving the traveling salesman problem (TSP) with DL. Worse and less efficient.\nGreenIT compare model performance with energy efficiency to limit carbon footprint. \n\n\n\n\n\n\nNote\n\n\n\nManage specular reflection with by comparing a gaussian splatted 3d version of an healthy referecnce\n\n\n\n\n\nClean specular reflexion in machine vision using gaussian splatting."
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#causal-effect-estimation-with-context-and-confounders-arthurgretton",
    "href": "posts/research_PAISS2025/index.html#causal-effect-estimation-with-context-and-confounders-arthurgretton",
    "title": "PAISS 2025",
    "section": "Causal Effect Estimation with Context and Confounders (ArthurGretton)",
    "text": "Causal Effect Estimation with Context and Confounders (ArthurGretton)\nDoing DAG helps !"
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#weakly-supervised-multi-label-plant-species-prediction-with-multimodal-data-luk√°≈°-picek",
    "href": "posts/research_PAISS2025/index.html#weakly-supervised-multi-label-plant-species-prediction-with-multimodal-data-luk√°≈°-picek",
    "title": "PAISS 2025",
    "section": "Weakly Supervised Multi-Label Plant Species Prediction with Multimodal Data (Luk√°≈° Picek)",
    "text": "Weakly Supervised Multi-Label Plant Species Prediction with Multimodal Data (Luk√°≈° Picek)\nPlantNet, kaggle challenge not bad but hard to get on foot with it. Nice to hear the plantnet project by the way, lets contribute !"
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#experiences-from-training-magistral-reasoning-and-voxtral-audio-models-at-mistral-timoth√©e-lacroix",
    "href": "posts/research_PAISS2025/index.html#experiences-from-training-magistral-reasoning-and-voxtral-audio-models-at-mistral-timoth√©e-lacroix",
    "title": "PAISS 2025",
    "section": "Experiences from training Magistral (reasoning) and Voxtral (audio) models at Mistral (Timoth√©e Lacroix)",
    "text": "Experiences from training Magistral (reasoning) and Voxtral (audio) models at Mistral (Timoth√©e Lacroix)\nLLM = shit (df YLC)"
  },
  {
    "objectID": "posts/2025_12_10_al_doesn_work/index.html",
    "href": "posts/2025_12_10_al_doesn_work/index.html",
    "title": "Active Learning does not work",
    "section": "",
    "text": "The title is provocative, this short post will be used to compile published results regarding active learning and the unreachable quest of beating random sampling. I will focus on application on computer vision since it is my area of research, but i think the results will be extrapolatable. ALl results will be attached to the paper they come from. I will try to organise and update it on a regular basis, this is not to make fun or critize anything, i work in AL and i know how hard it is to beat random sampling.\n\nIntroduction\nActive Learning (AL) is the field that is interested in finding the msot informative data points from an unlabeled pool of data. There are two families of selection philosophy, the first one is the uncertainty based selection, where we select point for which the estimator is the least certain about, it is like focusing on model‚Äôs weaknesses. And there are the representation based selection where we select data points that are as different of each other as possible. The other methods are a combination of those two.\n\n\nRepresentation based\nI put representation based selection in a separate section because they are completly independent of the problem. For images, they are based on images embeddings hooked after the encoder part of the neural network, and this is independent on the task. The other particularities of this type of selection is that you could get embeddings of your data from any model. So one classical approach is to train an encoder in a self-supervised fashion with SimCLR(Chen et al. 2020) or LeJEpa(Balestriero and LeCun 2025) and use those embeddings to select the first batch of data when no label is available.\nGOAT of representation based selection, Core-Set (Sener and Savarese 2018), this is a repulsive sampling of latent space based on some distances, the default is the euclidean distance, but since dimension can get quite large, any distance could work.\n\n\n\nCore set result for fully supervised learning\n\n\nSome extensions have been published using other type of algorithms based on point-cloud sampling.\nDeterminantal point processes (DPP) (Bƒ±yƒ±k et al. 2019), k-medoids (de Mathelin et al. 2021) \nSelecting the most typical item from k-means clustered data TypiClust (Hacohen, Dekel, and Weinshall 2022) \n\n\nImage Classification\nI will start with the big baselines of AL, these are the most famous acquisition function that shapes the AL world.\nBALD, use of dropout layers to quantify information gain by quantifying aleatoric and epistemic uncertainty, i like this strat because it leads to beautiful visualisation of multiple different prediction at inference time, but it is computationaly costly. (Gal, Islam, and Ghahramani 2017) \nBadge is a strategy i did not implement yet but is in most benchmarks.\n\nIn order to quantify uncertainty, somme professional cooks buildt a auxialliary neural net that learn the loss function during the supervised training, and estimate the uncertainty about the data point prediction using their estimated loss (Yoo and Kweon 2019). \n\n\nObject detection\nThe object detection problem (and instance segmentation) poses some question. Image classification output are probability vector representing the uncalibrated confidence the network has for the image belonging to each class. But in object detection, the prediction are done at the region level in the image, and the predictions are variables, there can be no prediction or hundreds, and each detection is characterized by a bounding box and a classification. Each classification has a score in \\([0,1]\\) that represents the confidence the network has in the classification. We usually don‚Äôt have access to the complete vector of probabilities.\nThe learning Loss framework is adapted to any architechture and they also developed an experiment on object detection here : \nPlug And play active learning (PPAL) (Yang, Huang, and Crowley 2024) \n\n\nSemantic Segmentation\nHere it is very interesting, each pixel is a classifier. So you can apply each algorithm in the image classification category indepently at the pixel level. You can then use some computer vision algorithm to aggregate the prediction to label the full image. Some strategy focus on labeling only part of the image, and other operates at the image level.\nSome uses advanced models like diffusion (Kim et al. 2025) \n\n\nConclusion\nAs you can see, all the results are differents, some contains confidence bands some dont, some have different performace for the same models some dont. But the common denominator amoung all those work is that whatever advanced the selection is, the gain against random sampling is never that big. Of course in the long term it will add up and everything, but i am wondering what could be the maximum achievable gain the best AL selection could provide.\nIf you work in AL, let‚Äôs do our best to make it work :).\n\n\n\n\n\n\n\nReferences\n\nBalestriero, Randall, and Yann LeCun. 2025. ‚ÄúLeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics.‚Äù arXiv. https://doi.org/10.48550/arXiv.2511.08544.\n\n\nBƒ±yƒ±k, Erdem, Kenneth Wang, Nima Anari, and Dorsa Sadigh. 2019. ‚ÄúBatch Active Learning Using Determinantal Point Processes.‚Äù arXiv. https://doi.org/10.48550/arXiv.1906.07975.\n\n\nChen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. ‚ÄúA Simple Framework for Contrastive Learning of Visual Representations.‚Äù arXiv. https://doi.org/10.48550/arXiv.2002.05709.\n\n\nde Mathelin, Antoine, Francois Deheeger, Mathilde Mougeot, and Nicolas Vayatis. 2021. ‚ÄúDiscrepancy-Based Active Learning for Domain Adaptation.‚Äù arXiv.org. https://arxiv.org/abs/2103.03757v3.\n\n\nGal, Yarin, Riashat Islam, and Zoubin Ghahramani. 2017. ‚ÄúDeep Bayesian Active Learning with Image Data.‚Äù In Proceedings of the 34th International Conference on Machine Learning - Volume 70, 1183‚Äì92. ICML‚Äô17. Sydney, NSW, Australia: JMLR.org.\n\n\nHacohen, Guy, Avihu Dekel, and Daphna Weinshall. 2022. ‚ÄúActive Learning on a Budget: Opposite Strategies Suit High and Low Budgets.‚Äù arXiv. https://doi.org/10.48550/arXiv.2202.02794.\n\n\nKim, Jeongin, Wonho Bae, YouLee Han, Giyeong Oh, Youngjae Yu, Danica J. Sutherland, and Junhyug Noh. 2025. ‚ÄúDiffusion-Driven Two-Stage Active Learning for Low-Budget Semantic Segmentation.‚Äù arXiv. https://doi.org/10.48550/arXiv.2510.22229.\n\n\nSener, Ozan, and Silvio Savarese. 2018. ‚ÄúActive Learning for Convolutional Neural Networks: A Core-Set Approach.‚Äù arXiv. https://doi.org/10.48550/arXiv.1708.00489.\n\n\nYang, Chenhongyi, Lichao Huang, and Elliot J. Crowley. 2024. ‚ÄúPlug and Play Active Learning for Object Detection.‚Äù arXiv. https://doi.org/10.48550/arXiv.2211.11612.\n\n\nYoo, Donggeun, and In So Kweon. 2019. ‚ÄúLearning Loss for Active Learning.‚Äù arXiv. https://doi.org/10.48550/arXiv.1905.03677."
  },
  {
    "objectID": "posts/research_JRAF2025/index.html",
    "href": "posts/research_JRAF2025/index.html",
    "title": "JRAF 2025",
    "section": "",
    "text": "JRAF : Journ√©e de Recherche en Apprentissage Frugal (Days of research on Frugal Learning.)\nI had the opportunity to attend this conference at the end of november 2025."
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#exhaustive-measure-of-hardware",
    "href": "posts/research_JRAF2025/index.html#exhaustive-measure-of-hardware",
    "title": "JRAF 2025",
    "section": "Exhaustive measure of hardware",
    "text": "Exhaustive measure of hardware\nThe first part of the conf was about how to measure the consumption of our applications. We know that the bigger the models, the bigger the requirements in environmental resources (abiotic, water, energy). This particular issues with AI and deep learning applications is that most of the computing is not done on the research site, but the compute resources are rent to cloud providers like azure, aws etc‚Ä¶ So the measurements need to be estimated one way or another.\nSome researcher tried to measure the impact of computing devices (Falk et al. 2025) but the information from manufacters are quite lacking, so researchers have to use so tricks like retro engineering but in adds more uncertainty to the computations. LLM providers did provide some information about their environemental impact like mistral in this article but we cannnot do anything more than believing them.\nWhat society needs is:\n\nTransparency\nClarity of the functionnal unit : Mistral gives its impact for 400 tokens, Google for a median prompt. But does it relate to the usage of the tool ? What are the difference if we generate an image or a video ? how about text only ? Will 400 be enough for most usage or will i need more ? If so why did they choose to give the number for 400 tokens.\nOn what perimeter does the number relate ? [Construction|usage|end of life] of the product. scopes (sorry to cite mckinsey :‚Äô( )\nLimits : What functions or activities could have been forgotten or were not taken into account ?\n\nIt is indeed important to measure but if the measure does not comply with this criterion it can be called green washing or a unethic use of numbers. It is better to not give anything than on purpose incomplete numbers.\nThank you Danilo !"
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#measure-of-software",
    "href": "posts/research_JRAF2025/index.html#measure-of-software",
    "title": "JRAF 2025",
    "section": "Measure of software",
    "text": "Measure of software\nSome tools exists to measure the impacts of your own applications. This could be of interest for example if you have multiple ways of solving a problem, and you want to check which one require the least energy.\nFour tools exists for this purpose :\n\nEcoLogits : give an estimation of the impacts when a request is made to an LLM provider\nGreen Algorithm Project : Measure should not be the goal but the way of making people change their behavior, that is way their association work on making their tools as user friendly as possible. They can give certificate to teamas that work on green IT practices through their green disc certification\nCodeCarbon : Uses the drivers of the machine running the computations to track their emmisions in real time.\nAlumet : Provide by this nice guy and coded in rust. This package require a little bit more knowledge but as i understood it, it works at the OS level and track the emission of each process running on a machine.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteInteresting\n\n\n\nEach tool has its own public, i think for us data scientist, code carbon is the best. It integrates on existing code and provide detatiled report on the program executions consumptions."
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#conclusion-on-the-first-day",
    "href": "posts/research_JRAF2025/index.html#conclusion-on-the-first-day",
    "title": "JRAF 2025",
    "section": "Conclusion on the first day",
    "text": "Conclusion on the first day\nNow there is no incentives to work on frugality, on the contrary it is cool to do AI agentic AI or anything with as much buzzwords as possible. Some states are starting to debate about it like France but it will require ruling at an higher level.\nChristoph Becker ended the day with some social sciences notes. Computer scientist are good at solving problems with technical solution of their expertise. But in some situations, technical knowlege is not the right tool to solve societal issues. His arguments are from his book in open access : the insolvent.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWine and Cheese\n\n\n\nThe first evening was the occasion to talk with some old acquaintaces it was a pleasure !\nYoann is working on some interesting topics related to computer vision and sensor fusion. (Dupas et al. 2025). He will defend his Phd in January 2026 i look forward to it :) !!!"
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#good-benchmark-is-all-you-need",
    "href": "posts/research_JRAF2025/index.html#good-benchmark-is-all-you-need",
    "title": "JRAF 2025",
    "section": "(Good) Benchmark is all you need",
    "text": "(Good) Benchmark is all you need\nThomas Moreau Presented a library he works on Benchopt. ML field is a very experimental field, we are interested by making good results and spend less time on methods and formalism. That is why benchmark is a key component in ML research, if you want to publish your new algorithm, you want to show that it is better than the SOTA. So every researcher run the same computations again and again each time they want to test a new program.\n\n\n\n\nShort term\nLong term\n\n\n\n\nTask specific\nChallenges/Competitions\nSOTA Tracking\n\n\nGeneralizable\nResearch question\nBenchmarkFramework\n\n\n\nMost attention is on short term advanced for specific tasks but reliable science needs BenchMark framework that are task independent and makes you run only your own contribution and not the previous baselines."
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#if-you-own-the-computation-devices",
    "href": "posts/research_JRAF2025/index.html#if-you-own-the-computation-devices",
    "title": "JRAF 2025",
    "section": "If you own the computation devices",
    "text": "If you own the computation devices\nif you own computation clusters and you have the power to choose the sequencing of jobs Bruno Gaugal presented his work (Gaujal, Girault, and Plassart 2019). He uses job duration estimation to predict the duration of each job and modify the speed in order to get CPU to work at constant speed. If you have enough compute nodes you might prefer auto-scaling, which is scaling the number of active nodes depending on the demand. (Kambale et al. 2025)."
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#increase-inference-speed",
    "href": "posts/research_JRAF2025/index.html#increase-inference-speed",
    "title": "JRAF 2025",
    "section": "Increase inference speed",
    "text": "Increase inference speed\nHow can we make the network not use its full neurons if they are not needed ? That is the work of Martial who work on early-exit networks (Guidez et al. 2025).\n\n\n\n\n\n\nNote\n\n\n\nThis is interesting, it allows the network to be big enought at the training, and be as fast as possible at inference. The NN will have a inference time bounded by the prediction of the most uncertain data points. It is an alternative to distillation regarding the reduction of inference time."
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#llm",
    "href": "posts/research_JRAF2025/index.html#llm",
    "title": "JRAF 2025",
    "section": "LLM",
    "text": "LLM\nIncrease capabilities with smaller models using knowledge graphs (KG). (Mimouni, Moissinac, and Tuan 2020).\nHow do we link KG with LLm you will ask ? idk, but they do (Pan et al. 2024)."
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#learning-rate-scheduling-is-cheating",
    "href": "posts/research_JRAF2025/index.html#learning-rate-scheduling-is-cheating",
    "title": "JRAF 2025",
    "section": "Learning Rate scheduling is cheating",
    "text": "Learning Rate scheduling is cheating\nLR scheduling make the learning of the network very variable and it is an hyperparameter very hard to work with, i don‚Äôt that it is a bad idea but i tend to dislike their usage in order to be as parcimonious as possible.\nHere Bogdan showed us how they used control theory to dynamicaly adjust the learning rate to make the loss decrease as fast as possible. (Zhao et al. 2019)"
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#conclusion-day-2",
    "href": "posts/research_JRAF2025/index.html#conclusion-day-2",
    "title": "JRAF 2025",
    "section": "Conclusion Day 2",
    "text": "Conclusion Day 2\nWe should integrate frugality measurement and try to avoid using big models if what they make us gain is so small comparatively with the increase of computational power required.\ni am very happy i attended this event. I could meet nice people and be exposed to what we never see in industry where we are more oriented on results than on externalities.\nShould we continue to work on AI ? If not, will we loose competitiveness against our neighbours ? Who should take this kind of decisions ?\nShould we accelerate the end or prefer a slow death to share it future generations ?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julien Combes",
    "section": "",
    "text": "Reliable Image classifiers\n\n\n\n\n\n\nDeepLearning\n\nENG\n\nConformalPrediction\n\ncode\n\n\n\n\n\n\n\n\n\nDec 13, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nActive Learning does not work\n\n\n\n\n\n\nDeepLearning\n\nResearch\n\nENG\n\n\n\n\n\n\n\n\n\nDec 10, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nECAS 2025\n\n\nWinter School\n\n\n\nPhd\n\nproductivity\n\nENG\n\n\n\n\n\n\n\n\n\nDec 5, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nJRAF 2025\n\n\nAutumn School\n\n\n\nPhd\n\nconference\n\nENG\n\nFrugality\n\n\n\n\n\n\n\n\n\nDec 1, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nPAISS 2025\n\n\nSummer SChool\n\n\n\nPhd\n\ncode\n\nDeepLearning\n\nComputerVision\n\nENG\n\n\n\n\n\n\n\n\n\nSep 1, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nIJCNN 2025\n\n\nConference\n\n\n\nPhd\n\ncode\n\nDeepLearning\n\nComputerVision\n\nENG\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nD-Fine on pascal Voc\n\n\n\n\n\n\nPhd\n\ncode\n\nDeepLearning\n\nComputerVision\n\nENG\n\n\n\n\n\n\n\n\n\nJun 9, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nTools for Phd\n\n\n\n\n\n\nPhd\n\nproductivity\n\nENG\n\n\n\n\n\n\n\n\n\nMay 27, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nUV setup for computer vision using deep learning\n\n\n\n\n\n\nDeepLearning\n\ncode\n\nComputerVision\n\npython\n\nlinux\n\n\n\n\n\n\n\n\n\nApr 13, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nClassify CIFAR\n\n\n\n\n\n\nDeepLearning\n\ncode\n\nImageClassification\n\nComputerVision\n\nENG\n\n\n\n\n\n\n\n\n\nApr 13, 2025\n\n\nJulien Combes\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog will serve as a self-reminder and a repository for useful software code that I can easily share with others.\nI am currently in the second year of a PhD in Applied Mathematics. My research focuses on active learning and uncertainty quantification applied to computer vision.\nI am doing my PhD at Michelin, where there is a high demand for machine vision and not enough experts to label the data.\nI love learning and am very curious but software development is far from being a strength.\nMy passions :)\n\n\n\n\n\n\nboard games : Top3 (Brass Brirmingham, Magic the gathering, Ascencion)\n\n\n\n\n\n\n\nWine : top3 cepages (syrah, marsanne, riesling)\n\n\n\n\n\n\n\n\n\nEuphorbia characias"
  },
  {
    "objectID": "posts/research_ijcnn/index.html",
    "href": "posts/research_ijcnn/index.html",
    "title": "IJCNN 2025",
    "section": "",
    "text": "Conference about advances in deep learning. I had the immense to attend this event thanks to Michelin to support my friend and colleague Thomas who presented his work on multilabel instance segmentation applied to machine vision applications."
  },
  {
    "objectID": "posts/phd_tools/index.html",
    "href": "posts/phd_tools/index.html",
    "title": "Tools for Phd",
    "section": "",
    "text": "Phd is a long and likely hard journey, if some tools can make the way easier we won‚Äôt complain about it. I want to gather tools that i find useful in this post.\nIt can be related to bibliography, writing, coding or anything that pass to mind, it should be updated at some point when new things come to my mind."
  },
  {
    "objectID": "posts/phd_tools/index.html#scholar-inbox",
    "href": "posts/phd_tools/index.html#scholar-inbox",
    "title": "Tools for Phd",
    "section": "Scholar inbox",
    "text": "Scholar inbox\nThis tool is so nice to be updated anytime an article in our field of research is published. You need to create an account an train a recommendation system based on literature that you are interested in. When it‚Äôs done, you will have new article pre-published in the past week that relates to your research.\nS/O to MG that let me know this tool !\nedit-0 : It makes the finding of new article very easy, but the rate at which new article are brought to you is higher than the reading speed unfortunately. So it just make your reading list increases haha. Still need to see how to manage it."
  },
  {
    "objectID": "posts/phd_tools/index.html#bibliography",
    "href": "posts/phd_tools/index.html#bibliography",
    "title": "Tools for Phd",
    "section": "Bibliography",
    "text": "Bibliography\nI can‚Äôt recommend Zotero enough. You can save article from the browser in one click. Read and annotate all in one place.\nFolder VS Tags debate: For the document management, i would recommend you to think of all the folders and subfolders you would make and not create them. Usually, one document belongs to multiple categories, and you don‚Äôt want to duplicate all your sources et each folders/subfolders. That is why i would assign tags to each document at the time you add them to you library. That way, you can search your document by tag and it makes it very easy."
  },
  {
    "objectID": "posts/phd_tools/index.html#note-taking",
    "href": "posts/phd_tools/index.html#note-taking",
    "title": "Tools for Phd",
    "section": "Note taking",
    "text": "Note taking\nI like to use obsidian with the citation extension. It connects the notes with the zotero library and allows you to link all the article and is very helpful when you want to write about any topic."
  },
  {
    "objectID": "posts/phd_tools/index.html#literature-graph",
    "href": "posts/phd_tools/index.html#literature-graph",
    "title": "Tools for Phd",
    "section": "Literature Graph",
    "text": "Literature Graph\nConnected papers : not free. Lit map could be an alternative."
  },
  {
    "objectID": "posts/phd_tools/index.html#redaction",
    "href": "posts/phd_tools/index.html#redaction",
    "title": "Tools for Phd",
    "section": "Redaction",
    "text": "Redaction\nmy favorite software of this stack Quarto. It allow you to write anything in markdown and generate pdfs (with latex or typst), beamers or revealjs and this blog your are currently reading !\nIt is an amazing piece of software based on Pandoc the Haskell unicorn.\nIf you are in stats or working with any type of data, quarto is nice since you can code in the same file as the one you write in. It is like a notebook but plain text, that allows version control systems to follow it nicely."
  },
  {
    "objectID": "posts/uv_ml/index.html",
    "href": "posts/uv_ml/index.html",
    "title": "UV setup for computer vision using deep learning",
    "section": "",
    "text": "UV is a drop-in replacement for pip and global python installation. I allows the management of python versions and packages. So far i only used it as a replacement for conda/venv so i only have a shallow understanding of basic feature. The build capabilities are not handled in this page yet."
  },
  {
    "objectID": "posts/uv_ml/index.html#local-quick-environement",
    "href": "posts/uv_ml/index.html#local-quick-environement",
    "title": "UV setup for computer vision using deep learning",
    "section": "Local quick environement",
    "text": "Local quick environement\nOnly use a local environement that will be used only for this project. This option is good for prototyping.\nIn order to manage the part with GPU compatibilities, uv provides an help page to get the torch versions matching your locally installed cuda :\n1uv venv --python 3.12\nuv pip install torch torchvision --torch-backend=auto\n2uv pip install -r requirements.txt\n\n1\n\nCreating the virtual environement in the current directory with the specified python version.\n\n2\n\nInstall all packages listed in requirement.txt in the local env.\n\n\nomegaconf\n\nnumpy\npandas\nseaborn\nmatplotlib\n\ntyper\n\npycocotools\nalbumentationsX\nopencv-python\ntorchmetrics\nlightning\ntransformers"
  },
  {
    "objectID": "posts/uv_ml/index.html#complete-and-reproducible-dev",
    "href": "posts/uv_ml/index.html#complete-and-reproducible-dev",
    "title": "UV setup for computer vision using deep learning",
    "section": "Complete and reproducible dev",
    "text": "Complete and reproducible dev\nIf you want a complete reproducible management of dependencies you should use the project features of uv.\nuv init --python 3.14 .\nuv pip install torch torchvision --torch-backend=auto\nuv add -r requirements.txt\nHere, uv will create a full pyproject.toml where the python version and all the dependencies whith their versions are saved."
  },
  {
    "objectID": "posts/voc_detection/index.html",
    "href": "posts/voc_detection/index.html",
    "title": "D-Fine on pascal Voc",
    "section": "",
    "text": "Object detection is the second type of problem we will be solving in this blog using lightning and pascal voc. This problem is very common because it allow to locate a variable number of objects in images.\nSince it is used in case of autonomous driving, real time object counting object detection architectures are grouped into two categories. The real time object detection where the boss is the YOLO version 11 at the time i write this article. The more recent networks that can fight with yolo in RTOD (Real time object detection) is the D-Fine architecture which is an improved DETR.\nWe implemented it in this article, we chose to train this architecture in lightning since the training loop can be re-used with other models and dataset. I don‚Äôt know what is the best training paradigm but i want one that keeps as agnostic as possible to any framework, and for me lighting allow strong customization and proximity to pure torchscript while allowing easy multi device training and metrics logging etc..\n\nData\nWe will model pascal voc images in this project because its very easily accessible from torchvision repos.\n\nfrom torchvision.datasets import VOCDetection\nfrom torchvision.transforms import v2 \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import DataLoader\nimport torch \nfrom omegaconf import OmegaConf\nfrom torchvision.ops import box_convert\nfrom torchvision.utils import draw_bounding_boxes\nimport albumentations as A\nfrom transformers import DFineForObjectDetection, AutoImageProcessor\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision.ops import batched_nms\nimport torchmetrics\nimport lightning as L\nfrom torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\nfrom typing import Dict, List, Optional\nimport os\n\nd-fine expectS bbox in coco format so we convert it in the get_item part.\n\ncategories = ['pottedplant', 'bottle', 'chair', 'diningtable', 'person', 'car', 'train', 'bus', 'bicycle', 'cat', 'aeroplane', 'tvmonitor', 'sofa', 'sheep', 'dog', 'bird', 'motorbike', 'horse', 'boat', 'cow', \"bg\"]\n\n\ndef unpack_box(box_dict:Dict):\n    \"\"\"\n    Unpack the box dictionary into a list of coordinates\n    \"\"\"\n    return torch.tensor(np.array([\n        box_dict[\"xmin\"],\n        box_dict[\"ymin\"],\n        box_dict[\"xmax\"],\n        box_dict[\"ymax\"]\n    ], dtype = float))\n\ndef annotation_to_torch(target:Dict):\n    rep = {}\n    detections = target[\"annotation\"][\"object\"]\n    \n    rep[\"labels\"] = np.array([categories.index(i[\"name\"]) for i in detections])\n    # xmin ymin xmax ymax\n    rep[\"boxes\"] = torch.stack([unpack_box(i[\"bndbox\"]) for i in detections])\n\n    return rep    \n\nimage_processor = AutoImageProcessor.from_pretrained(\"ustc-community/dfine_x_coco\", use_fast=True)\n\nclass MyVoc(VOCDetection):\n    def __getitem__(self, index):\n        image, target = super().__getitem__(index)\n        # Apply your transformations here\n        target = annotation_to_torch(target)\n\n        transform = A.Compose([\n            A.PadIfNeeded(500,500),\n            A.HorizontalFlip(),\n            A.RandomCrop(400,400),\n            A.Resize(224, 224),\n            A.Normalize(normalization=\"min_max\"),\n            ToTensorV2()\n        ],\n        bbox_params=A.BboxParams(\n            format='pascal_voc', # Specify input format\n            label_fields=['class_labels'],\n            filter_invalid_bboxes=True)\n        )\n\n        transformed = transform(\n            image=np.array(image), \n            bboxes=target[\"boxes\"], \n            class_labels=target[\"labels\"]\n        )\n\n        transformed[\"labels\"] = transformed[\"class_labels\"]\n        transformed[\"boxes\"] = transformed[\"bboxes\"]\n        transformed.pop(\"bboxes\")\n\n        image = transformed.pop(\"image\")\n        transformed[\"boxes\"] = box_convert(\n            torch.from_numpy(transformed[\"boxes\"],\n            ),\n            \"xyxy\",\n            \"xywh\",).float()\n        transformed[\"labels\"] = torch.from_numpy(transformed[\"labels\"])\n        transformed[\"class_labels\"] = torch.from_numpy(transformed[\"class_labels\"])\n        return image.float(), transformed\n\n    def draw_item(self, index:Optional[int] = None, n=5):\n        if index is None:\n            index = np.random.randint(0, len(self))\n\n        image, labels = self[index]\n\n\n        with_boxes = draw_bounding_boxes(\n            image =  image,\n            boxes= box_convert(labels[\"boxes\"], \"xywh\", \"xyxy\"),\n            labels = [categories[i] for i in labels[\"labels\"]],\n            colors = \"red\"\n\n        )\n\n\n        plt.figure(figsize=(10, 10))\n        plt.imshow(with_boxes.permute(1, 2, 0).numpy())\n        plt.axis('off')\n        plt.show()\n        return \n\nNothing special here, we show what the data looks like.\n\ndatarootdir = os.environ.get(\"DATA_ROOTPATH\", \"~/data_wsl/voc\")\nds = MyVoc(\n    root = os.path.join(datarootdir, \"voc\"), \n    download = False,\n    image_set=\"train\"\n)\nval_ds = MyVoc(\n    root = os.path.join(datarootdir, \"voc\"), \n    download = False,\n    image_set=\"val\",\n)\nds.draw_item()\n\n\n\n\n\n\n\n\n\n\nModeling\nModeling part is here, with the lightning module.\n\n\nCode\nconfig = OmegaConf.create({\n    \"lr\": 1e-4,\n    \"batch_size\":2,\n    \"epochs\":3,\n    \"world_size\":1\n})\ndef apply_nms(preds:Dict, iou_thr:float = .5):\n    nms_indices = batched_nms(\n        preds[\"boxes\"],\n        scores = preds[\"scores\"],\n        idxs=preds[\"labels\"],\n        iou_threshold=iou_thr\n    )\n    preds_nms = {}\n    preds_nms[\"boxes\"] = preds[\"boxes\"][nms_indices,:]\n    preds_nms[\"scores\"] = preds[\"scores\"][nms_indices]\n    preds_nms[\"labels\"] = preds[\"labels\"][nms_indices]\n    \n    # high_scores_indices = preds_nms[\"scores\"] &gt; .3\n    # preds_nms[\"boxes\"] = preds_nms[\"boxes\"][high_scores_indices]\n    # preds_nms[\"scores\"] = preds_nms[\"scores\"][high_scores_indices]\n    # preds_nms[\"labels\"] = preds_nms[\"labels\"][high_scores_indices]\n\n    return preds_nms\n\nclass odModule(L.LightningModule):\n    def __init__(self, config, categories:List[str], nms_thr:float = .5):\n        super().__init__()\n        # if config.checkpoint is not None:\n        # print(f\"checkpoint from {config.checkpoint}\")\n        num_classes = len(categories)\n        self.categories = categories\n\n        self.nms_thr = nms_thr\n        self.config = config\n        # self.model = fasterrcnn_mobilenet_v3_large_fpn(pretrained = False, num_classes=num_classes)\n        self.model = DFineForObjectDetection.from_pretrained(\n            \"ustc-community/dfine_x_coco\",\n            id2label= {i:cat for i,cat in enumerate(categories)},\n            label2id={cat:i for i,cat in enumerate(categories)},\n            ignore_mismatched_sizes=True,\n        )\n\n        metrics = torchmetrics.MetricCollection(\n            [\n                torchmetrics.detection.mean_ap.MeanAveragePrecision(\n                    # extended_summary=True, \n                    iou_thresholds=np.linspace(0,1,20).tolist(),\n                    class_metrics=True, \n                    iou_type=\"bbox\",\n                ),\n            ]\n        )   \n        self.val_metrics = metrics.clone(prefix=\"Validation/\")\n        self.test_metrics = metrics.clone(prefix=\"Test/\")\n    \n        self.save_hyperparameters(ignore=[\"train_ds\"])\n\n    @staticmethod\n    def prepare_batch(batch):\n        images, targets = batch\n        return images, targets\n\n    def forward(self, x, y=None):\n        if y is not None:\n            return self.model(pixel_values = x, labels = y)\n        else:\n            preds = self.model(x)\n            n, c, h, w = x.shape\n            preds = image_processor.post_process_object_detection(\n                preds, \n                target_sizes=[(h,w) for _ in range(n)], \n                threshold=0.5)\n            return preds\n\n    def predict(self, x):\n        \"\"\"Forward the model then run NMS (for evaluation)\n\n        Args:\n            x (_type_): _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"        \n        preds:List = self(x)\n        # preds_nms = [apply_nms(i, self.nms_thr) for i in preds]\n        return preds\n\n\n    def training_step(self, batch, batch_idx):\n        img_b, target_b = self.prepare_batch(batch)\n        bs = len(img_b)\n\n        dfine_output = self(img_b, target_b)\n\n        self.log_dict(\n            dfine_output.loss_dict,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n            batch_size=bs,\n            prog_bar=True,\n        )\n\n        return {\"loss\": dfine_output.loss}\n\n    def validation_step(self, batch, batch_idx):\n        img_b, target_b = self.prepare_batch(batch)\n        output_nms = self.predict(img_b)\n\n        self.val_metrics(output_nms, target_b)\n        return\n\n    def on_validation_epoch_end(self):\n        \n        m = self.val_metrics.compute()\n        m_single= {i:j for i,j in m.items() if j.nelement() ==1}\n        \n        self.log_dict(m_single, on_epoch=True, sync_dist=False)\n\n        for i, class_id in enumerate(m[\"Validation/classes\"]):\n            self.log(f\"Validation/MAP {self.categories[class_id]}\", m[\"Validation/map_per_class\"][i])\n\n        self.val_metrics.reset()\n        return\n\n    def test_step(self, batch, batch_idx):\n        img_b, target_b = self.prepare_batch(batch)\n\n        output_nms = self.predict(img_b)\n\n        self.test_metrics(output_nms, target_b)\n        return\n\n    def on_test_epoch_end(self):\n        m = self.test_metrics.compute()\n        m_single= {i:j for i,j in m.items() if j.nelement() ==1}\n\n        self.log_dict(m_single, on_epoch=True, sync_dist=False)\n        \n        for i, class_id in enumerate(m[\"Test/classes\"]):\n            self.log(f\"Test/MAP {self.categories[class_id]}\", m[\"Test/map_per_class\"][i])\n\n        self.test_metrics.reset()\n\n        return \n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(\n            self.parameters(),\n            lr=self.config.lr,\n            weight_decay=1e-4 * self.config.batch_size / 16,\n        )\n\n        # scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(\n        #     optimizer, T_max=scheduler_nsteps, eta_min=self.config.lr / 10\n        # )\n\n        # sched_config1 = {\"scheduler\": scheduler1, \"interval\": \"epoch\"}\n\n\n        return [optimizer]#, [sched_config1]\n\nmodel = odModule(\n    config,\n    categories\n)\n\n\n\ndef collate_fn(batch):\n    # Separate the images and targets\n    images = []\n    targets = []\n    \n    for image, target in batch:\n        images.append(image)  # Assuming each item has an 'image' key\n        targets.append(target)  # Assuming each item has a 'target' key\n\n    # Stack images into a single tensor\n    images = torch.stack(images, dim=0)\n\n    # Convert targets to a list of dictionaries or tensors\n    # This depends on how your targets are structured\n    # For example, if targets are dictionaries with bounding boxes and labels\n    return images, targets\ntrain_loader = DataLoader(\n    ds,\n    batch_size = 2,\n    shuffle = True,\n    collate_fn = collate_fn\n)\nval_loader = DataLoader(\n    val_ds,\n    batch_size = 2,\n    shuffle = False,\n    collate_fn = collate_fn\n)\nim, tar = next(iter(train_loader))\n\ntar\n\n[{'class_labels': tensor([9]),\n  'labels': tensor([9]),\n  'boxes': tensor([[ 84.5600,  82.3200,  85.1200, 125.4400]])},\n {'class_labels': tensor([17, 17,  4]),\n  'labels': tensor([17, 17,  4]),\n  'boxes': tensor([[  0.0000,  66.6400, 196.0000, 157.3600],\n          [  0.0000, 117.6000, 124.3200, 106.4000],\n          [129.3600, 168.5600,  22.4000,  24.6400]])}]\n\n\n\n\nTraining\nAll the training is shown here. The full training is happening here.\nthe full training has not been done yet but no bug has been encountered in first batches.\n\nmodel.train()\ntrainer=  L.Trainer(\n    max_epochs=3,\n    max_steps = 10, # limit training to make sure it works.\n    precision = \"16-mixed\",\n    enable_checkpointing=True,\n    num_sanity_val_steps=0,\n    log_every_n_steps=50,\n    check_val_every_n_epoch=1,\n)\n\ntrainer.fit(model, train_loader, val_loader)\n\nUsing 16bit Automatic Mixed Precision (AMP)\nüí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\n/mnt/big_partition/projet/JulienCombes/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/mnt/big_partition/projet/JulienCombes/.venv/lib/python3.13/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n\n  | Name         | Type                    | Params | Mode \n-----------------------------------------------------------------\n0 | model        | DFineForObjectDetection | 62.5 M | train\n1 | val_metrics  | MetricCollection        | 0      | train\n2 | test_metrics | MetricCollection        | 0      | train\n-----------------------------------------------------------------\n62.5 M    Trainable params\n2         Non-trainable params\n62.5 M    Total params\n250.001   Total estimated model params size (MB)\n1420      Modules in train mode\n0         Modules in eval mode\n/mnt/big_partition/projet/JulienCombes/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n/mnt/big_partition/projet/JulienCombes/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n\n\n\n\n\n`Trainer.fit` stopped: `max_steps=10` reached."
  },
  {
    "objectID": "posts/2025_12_11_cpintrocifar/index.html",
    "href": "posts/2025_12_11_cpintrocifar/index.html",
    "title": "Reliable Image classifiers",
    "section": "",
    "text": "Let‚Äôs make a resnet18 make reliable predictions on cifars datasets !"
  },
  {
    "objectID": "posts/2025_12_11_cpintrocifar/index.html#more-classes",
    "href": "posts/2025_12_11_cpintrocifar/index.html#more-classes",
    "title": "Reliable Image classifiers",
    "section": "More classes",
    "text": "More classes\nHere we could try to run this but with more classes to see how this affect the size fo prediction sets etc‚Ä¶\n\n\nCode\nbs = 32\ntrain_set = torchvision.datasets.CIFAR100(\n    root=os.path.join(datarootdir, \"cifar100\"), \n    download=False, \n    train=True,\n    transform=train_trans)\n\nval_set = torchvision.datasets.CIFAR100(\n    root=os.path.join(datarootdir, \"cifar100\"), \n    download=False, \n    train=False,\n    transform=test_trans)\n\ntrain_loader= torch.utils.data.DataLoader(\n    train_set,\n    shuffle=True,\n    batch_size=bs,\n    num_workers=5,\n\n)\n\nval_indices = np.random.choice(10000, 5000, replace= False)\ntest_indices= np.setdiff1d(np.arange(10000), val_indices)\n\nval_loader= torch.utils.data.DataLoader(\n    torch.utils.data.Subset(val_set,val_indices),\n    shuffle=False,\n    batch_size=bs*2,\n    num_workers=5,\n)\n\ntest_loader= torch.utils.data.DataLoader(\n    torch.utils.data.Subset(val_set,test_indices),\n    shuffle=False,\n    batch_size=bs*2,\n    num_workers=5,\n)\n\n\n\nmodel = ClassificationModule(\n    categories=train_set.classes,\n    config=config\n)\n\ntrainer=  L.Trainer(\n    max_epochs=3,\n    precision = \"16-mixed\",\n    enable_checkpointing=True,\n    num_sanity_val_steps=0,\n    log_every_n_steps=50,\n    check_val_every_n_epoch=1,\n    callbacks=[\n        EarlyStopping(\"Validation/Loss\", patience=2,mode=\"min\"),\n        ModelCheckpoint(monitor=\"Validation/Loss\", filename='cifar100-{epoch:02d}')\n    ]\n)\n\n# I wont train it each time \n# trainer.fit(\n#     model,\n#     train_loader,\n#     val_loader\n# )\n\n# trainer.test(model,val_loader)\n\nmodel= ClassificationModule.load_from_checkpoint(\n    \"lightning_logs/version_73/checkpoints/cifar100-epoch=02.ckpt\",\n    categories=train_set.classes,\n    config=config\n)\n\nfor score_fn in [\"aps\", \"raps\", \"saps\"]:\n    trainer=  L.Trainer(\n        max_epochs=1,\n        precision = \"16-mixed\",\n        enable_checkpointing=False,\n        num_sanity_val_steps=0,\n        log_every_n_steps=0,\n        check_val_every_n_epoch=0,\n        callbacks=[\n            CalibratorCallback(alpha= 0.1, score_function=score_fn)\n        ]\n    )\n\n    trainer.validate(model, val_loader, verbose = False);\n    trainer.test(model, test_loader);\n\n\nUsing 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nUsing 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\nUsing 16bit Automatic Mixed Precision (AMP)\n\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n       Test metric             DataLoader 0\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n        Test/Loss            1.754132628440857\n      Test/covered          0.8984000086784363\n     Test/singletons        0.2231999933719635\n       Test/sizes            10.50100040435791\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\nUsing 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n       Test metric             DataLoader 0\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n        Test/Loss            1.754132628440857\n      Test/covered          0.9020000100135803\n     Test/singletons        0.2280000001192093\n       Test/sizes            10.4197998046875\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n       Test metric             DataLoader 0\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n        Test/Loss            1.754132628440857\n      Test/covered          0.9053999781608582\n     Test/singletons                0.0\n       Test/sizes            9.005399703979492\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\nHere the number of possible predicted classes is 100 so as we can see the proportion of singleton predicted is 20% at max or 0 for the SAPS scoring function. We can see than while providing no singleton, the average size of prediction sets is smaller. Relatively to \\(\\#\\mathcal{Y}\\) being 100 of course, it is still large sets. We can see that the coverage requested of 90% is still verifies that is very good."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "ECAS 2025\n\n\n\n\n\n\nJulien Combes\n\n\nDec 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nJRAF 2025\n\n\n\n\n\n\nJulien Combes\n\n\nDec 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPAISS 2025\n\n\n\n\n\n\nJulien Combes\n\n\nSep 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nIJCNN 2025\n\n\n\n\n\n\nJulien Combes\n\n\nJun 25, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Hope it will not be empty at the end of the Phd üò¢.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]