[
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Bio",
    "section": "",
    "text": "I am currently pursuing a Phd under the supervision of Jean-Francois Coeurjolly and Alexandre Dervill√© which takes place between the R&D headquarters of Michelin in Clermont-Ferrand, and the LJK in Grenoble. I am interested in active learning and uncertainty quantification applied to machine vision.\nI graduated from Grenoble University in Statistics and data science.\nI had a one year break in Omuta, Japan."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "ECAS 2025\n\n\n\n\n\n\nJulien Combes\n\n\nDec 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nJRAF 2025\n\n\n\n\n\n\nJulien Combes\n\n\nDec 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPAISS 2025\n\n\n\n\n\n\nJulien Combes\n\n\nSep 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nIJCNN 2025\n\n\n\n\n\n\nJulien Combes\n\n\nJun 25, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/research_ECAS2025/index.html",
    "href": "posts/research_ECAS2025/index.html",
    "title": "ECAS 2025",
    "section": "",
    "text": "Winter school where i learned about Conformal prediction (CP) and Transfert Learning."
  },
  {
    "objectID": "posts/research_ECAS2025/index.html#conformal-prediction",
    "href": "posts/research_ECAS2025/index.html#conformal-prediction",
    "title": "ECAS 2025",
    "section": "Conformal Prediction",
    "text": "Conformal Prediction\nThis is the field i want to become good at. Indeed, my PA fell in love with this field and im afraid i did too during this week. I met so nice people working on it and making the best use of this theoretical tool in the industry i want to make our computer vision more reliable as well.\nAll the course and the slides are in this github repo. The slides are clear and all the proofs are written as well.\nComplete book with theoretical proofs and explanations : (A. N. Angelopoulos, Barber, and Bates 2025)."
  },
  {
    "objectID": "posts/research_ECAS2025/index.html#transfer-learning",
    "href": "posts/research_ECAS2025/index.html#transfer-learning",
    "title": "ECAS 2025",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nThis field sounds specific but actually any statistician working with real world data will faced generalization problem one day or another.\nAll the material is in this github repo"
  },
  {
    "objectID": "posts/research_ECAS2025/index.html#discussions",
    "href": "posts/research_ECAS2025/index.html#discussions",
    "title": "ECAS 2025",
    "section": "Discussions",
    "text": "Discussions\nFor classification (Romano, Sesia, and Cand√®s 2020) provide very strong guarantee with adaptivity. That is what we need in active learning. RAPS Extensions with (A. Angelopoulos et al. 2022) improve this with smaller prediction sets. This gives hope in the use of CP in AL. When some defects are very rare conformal prediction garateeing only marginal coverage some classes might not be covered (no conditional coverage in general cases), she recommanded me the work of Tiffany Ding (Ding, Fermanian, and Salmon 2025) in collaboration with Plantnet.\nShe suggest Y-conditional conformal algorithms to guarantee that the coverage is verified for any classes with this paper (Ding et al. 2023).\nThis is still a area at the research stage but some solution exists to tackle our problems. We shall not forget that most of people using big neural network do not quantify the uncertainty at all. So it is nice to do the first step.\n\nAntoine was so nice to give his insight about his past with active learning and domain transfert i really want to thank him again. He agree that active learning doesn‚Äôt beat random in most cases, but he believes more in AL than in domain adaptation‚Ä¶. Let‚Äôs do our best !\nIt is normal for Al to bias the distribution, because if we didn‚Äôt want to bias it we would have stayed with random sampling.\n\n\n\n\n\n\nNote\n\n\n\nHe prefers k-medoids over core-set selection. It must be similar to typiclust so i will work on that and see how i can make my best of it.\n\n\nHe liked the idea of the potato project and was surprised that the AL could work that well in some cases. (Might be a bug ? i hope not‚Ä¶). He is not surprised that transfert in strategy independent. Indeed, when there is a distribution shift it is likely that nothing will help you in most real data cases. He notes that even for \\(\\pi^u=10%\\) AL can generalize well and beat random."
  },
  {
    "objectID": "posts/cifar_classif/index.html",
    "href": "posts/cifar_classif/index.html",
    "title": "Classify CIFAR",
    "section": "",
    "text": "CIFAR is a trivial problem in image classification. We will be using Pytorch and lightning in order to do the training.\nThe advantage of this approach, is that the workflow can be done locally on the cpu of your computer or on ten H100 of any cloud you could get access to.\nLightning handles the location of data and optimization related objects (model, optimizer, scheduler etc‚Ä¶), and last be not least, the metrics computation done with torchmetrics.\nThe metrics have the gathering across gpus/devices already implemented so you just have to decide of which ones you want to add to your project. If some computations are not already present in the library, you can add your own metric very easily."
  },
  {
    "objectID": "posts/cifar_classif/index.html#the-data",
    "href": "posts/cifar_classif/index.html#the-data",
    "title": "Classify CIFAR",
    "section": "The data",
    "text": "The data\n\nimport torchvision\nimport torch\nimport matplotlib.pyplot as plt\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\nimport os\n#¬†to make the transform usable by torchvision dataset it needs to be a function that takes an image as input and return an image as well\n\ndatarootdir = os.environ.get(\"DATA_ROOTPATH\")\n\ndef train_trans(image)-&gt;torch.tensor:\n    transform = A.Compose([\n        A.HorizontalFlip(),\n        A.Normalize(),\n        ToTensorV2()\n    ]) \n\n    transformed = transform(image = np.array(image))\n\n    return transformed[\"image\"]\n\ndef test_trans(image)-&gt;torch.tensor:\n    transform = A.Compose([\n        A.Normalize(),\n        ToTensorV2()\n    ]) \n\n    transformed = transform(image = np.array(image))\n\n    return transformed[\"image\"]\n\ntrain_set = torchvision.datasets.CIFAR10(\n    root=os.path.join(datarootdir, \"cifar10\"), \n    download=True, \n    train=True,\n    transform=train_trans)\n\nval_set = torchvision.datasets.CIFAR10(\n    root=os.path.join(datarootdir, \"cifar10\"), \n    download=True, \n    train=False,\n    transform=test_trans)\n\ntrain_loader= torch.utils.data.DataLoader(\n    train_set,\n    # shuffle=True,\n    sampler = torch.utils.data.SubsetRandomSampler(np.random.choice(len(train_set), 10000)),\n    batch_size=64,\n    num_workers=5,\n\n)\n\nval_loader= torch.utils.data.DataLoader(\n    val_set,\n    shuffle=False,\n    batch_size=64*2,\n    num_workers=5,\n)\n\n  0%|          | 0.00/170M [00:00&lt;?, ?B/s]  0%|          | 32.8k/170M [00:00&lt;43:18, 65.6kB/s]  0%|          | 65.5k/170M [00:00&lt;28:15, 101kB/s]   0%|          | 98.3k/170M [00:00&lt;23:30, 121kB/s]  0%|          | 131k/170M [00:01&lt;21:12, 134kB/s]   0%|          | 197k/170M [00:01&lt;14:40, 193kB/s]  0%|          | 295k/170M [00:01&lt;10:00, 283kB/s]  0%|          | 459k/170M [00:01&lt;08:22, 339kB/s]  0%|          | 721k/170M [00:02&lt;04:55, 574kB/s]  1%|          | 885k/170M [00:02&lt;05:02, 561kB/s]  1%|          | 1.15M/170M [00:02&lt;04:45, 593kB/s]  1%|          | 1.31M/170M [00:03&lt;04:24, 639kB/s]  1%|          | 1.57M/170M [00:03&lt;03:32, 794kB/s]  1%|          | 1.77M/170M [00:03&lt;04:13, 665kB/s]  1%|          | 1.97M/170M [00:03&lt;03:50, 732kB/s]  1%|          | 2.10M/170M [00:04&lt;03:57, 710kB/s]  1%|‚ñè         | 2.26M/170M [00:04&lt;03:50, 730kB/s]  1%|‚ñè         | 2.42M/170M [00:04&lt;04:13, 662kB/s]  2%|‚ñè         | 2.59M/170M [00:04&lt;04:00, 698kB/s]  2%|‚ñè         | 2.75M/170M [00:05&lt;03:51, 725kB/s]  2%|‚ñè         | 2.92M/170M [00:05&lt;03:17, 850kB/s]  2%|‚ñè         | 3.11M/170M [00:05&lt;02:43, 1.03MB/s]  2%|‚ñè         | 3.28M/170M [00:05&lt;02:28, 1.12MB/s]  2%|‚ñè         | 3.47M/170M [00:05&lt;02:11, 1.27MB/s]  2%|‚ñè         | 3.64M/170M [00:05&lt;02:05, 1.33MB/s]  2%|‚ñè         | 3.83M/170M [00:05&lt;02:16, 1.22MB/s]  2%|‚ñè         | 4.19M/170M [00:05&lt;01:45, 1.58MB/s]  3%|‚ñé         | 4.39M/170M [00:06&lt;01:42, 1.62MB/s]  3%|‚ñé         | 4.59M/170M [00:06&lt;01:51, 1.49MB/s]  3%|‚ñé         | 4.78M/170M [00:06&lt;01:47, 1.55MB/s]  3%|‚ñé         | 4.98M/170M [00:06&lt;01:43, 1.60MB/s]  3%|‚ñé         | 5.18M/170M [00:06&lt;01:40, 1.64MB/s]  3%|‚ñé         | 5.37M/170M [00:06&lt;01:38, 1.67MB/s]  3%|‚ñé         | 5.57M/170M [00:06&lt;01:37, 1.70MB/s]  3%|‚ñé         | 5.77M/170M [00:06&lt;01:36, 1.71MB/s]  3%|‚ñé         | 5.96M/170M [00:06&lt;01:35, 1.72MB/s]  4%|‚ñé         | 6.16M/170M [00:07&lt;01:34, 1.73MB/s]  4%|‚ñé         | 6.36M/170M [00:07&lt;01:35, 1.72MB/s]  4%|‚ñç         | 6.55M/170M [00:07&lt;01:35, 1.72MB/s]  4%|‚ñç         | 6.75M/170M [00:07&lt;01:34, 1.74MB/s]  4%|‚ñç         | 6.95M/170M [00:07&lt;01:34, 1.73MB/s]  4%|‚ñç         | 7.14M/170M [00:07&lt;01:34, 1.74MB/s]  4%|‚ñç         | 7.34M/170M [00:07&lt;01:33, 1.75MB/s]  4%|‚ñç         | 7.54M/170M [00:07&lt;01:44, 1.56MB/s]  5%|‚ñç         | 7.73M/170M [00:08&lt;01:43, 1.57MB/s]  5%|‚ñç         | 7.93M/170M [00:08&lt;01:41, 1.61MB/s]  5%|‚ñç         | 8.13M/170M [00:08&lt;01:38, 1.65MB/s]  5%|‚ñç         | 8.32M/170M [00:08&lt;01:43, 1.57MB/s]  5%|‚ñç         | 8.52M/170M [00:08&lt;01:39, 1.63MB/s]  5%|‚ñå         | 8.72M/170M [00:08&lt;01:38, 1.64MB/s]  5%|‚ñå         | 8.91M/170M [00:08&lt;01:43, 1.56MB/s]  5%|‚ñå         | 9.11M/170M [00:08&lt;01:39, 1.62MB/s]  5%|‚ñå         | 9.31M/170M [00:08&lt;01:37, 1.65MB/s]  6%|‚ñå         | 9.50M/170M [00:09&lt;01:35, 1.68MB/s]  6%|‚ñå         | 9.70M/170M [00:09&lt;01:34, 1.70MB/s]  6%|‚ñå         | 9.90M/170M [00:09&lt;01:34, 1.71MB/s]  6%|‚ñå         | 10.1M/170M [00:09&lt;01:33, 1.72MB/s]  6%|‚ñå         | 10.3M/170M [00:09&lt;01:32, 1.72MB/s]  6%|‚ñå         | 10.5M/170M [00:09&lt;01:32, 1.73MB/s]  6%|‚ñã         | 10.7M/170M [00:09&lt;01:31, 1.74MB/s]  6%|‚ñã         | 10.9M/170M [00:09&lt;01:31, 1.74MB/s]  6%|‚ñã         | 11.1M/170M [00:10&lt;01:37, 1.64MB/s]  7%|‚ñã         | 11.3M/170M [00:10&lt;01:37, 1.63MB/s]  7%|‚ñã         | 11.5M/170M [00:10&lt;01:35, 1.67MB/s]  7%|‚ñã         | 11.7M/170M [00:10&lt;01:34, 1.68MB/s]  7%|‚ñã         | 11.9M/170M [00:10&lt;01:32, 1.71MB/s]  7%|‚ñã         | 12.1M/170M [00:10&lt;01:32, 1.72MB/s]  7%|‚ñã         | 12.3M/170M [00:10&lt;01:30, 1.75MB/s]  7%|‚ñã         | 12.5M/170M [00:10&lt;01:30, 1.75MB/s]  7%|‚ñã         | 12.6M/170M [00:10&lt;01:29, 1.75MB/s]  8%|‚ñä         | 12.8M/170M [00:11&lt;01:29, 1.76MB/s]  8%|‚ñä         | 13.0M/170M [00:11&lt;01:28, 1.77MB/s]  8%|‚ñä         | 13.2M/170M [00:11&lt;01:27, 1.80MB/s]  8%|‚ñä         | 13.4M/170M [00:11&lt;01:25, 1.83MB/s]  8%|‚ñä         | 13.6M/170M [00:11&lt;01:25, 1.84MB/s]  8%|‚ñä         | 13.9M/170M [00:11&lt;01:23, 1.88MB/s]  8%|‚ñä         | 14.1M/170M [00:11&lt;01:23, 1.88MB/s]  8%|‚ñä         | 14.3M/170M [00:11&lt;01:21, 1.92MB/s]  9%|‚ñä         | 14.5M/170M [00:11&lt;01:19, 1.96MB/s]  9%|‚ñä         | 14.7M/170M [00:12&lt;01:18, 1.99MB/s]  9%|‚ñâ         | 15.0M/170M [00:12&lt;01:17, 2.01MB/s]  9%|‚ñâ         | 15.2M/170M [00:12&lt;01:17, 2.01MB/s]  9%|‚ñâ         | 15.5M/170M [00:12&lt;01:13, 2.10MB/s]  9%|‚ñâ         | 15.7M/170M [00:12&lt;01:13, 2.10MB/s]  9%|‚ñâ         | 16.0M/170M [00:12&lt;01:11, 2.15MB/s]  9%|‚ñâ         | 16.2M/170M [00:12&lt;01:13, 2.11MB/s] 10%|‚ñâ         | 16.4M/170M [00:12&lt;01:11, 2.16MB/s] 10%|‚ñâ         | 16.7M/170M [00:12&lt;01:08, 2.23MB/s] 10%|‚ñâ         | 17.0M/170M [00:13&lt;01:07, 2.26MB/s] 10%|‚ñà         | 17.3M/170M [00:13&lt;01:05, 2.34MB/s] 10%|‚ñà         | 17.5M/170M [00:13&lt;01:05, 2.34MB/s] 10%|‚ñà         | 17.8M/170M [00:13&lt;01:04, 2.38MB/s] 11%|‚ñà         | 18.1M/170M [00:13&lt;01:01, 2.46MB/s] 11%|‚ñà         | 18.4M/170M [00:13&lt;01:00, 2.52MB/s] 11%|‚ñà         | 18.7M/170M [00:13&lt;00:58, 2.59MB/s] 11%|‚ñà         | 19.0M/170M [00:13&lt;00:57, 2.64MB/s] 11%|‚ñà‚ñè        | 19.3M/170M [00:13&lt;00:55, 2.70MB/s] 12%|‚ñà‚ñè        | 19.6M/170M [00:14&lt;00:55, 2.72MB/s] 12%|‚ñà‚ñè        | 20.0M/170M [00:14&lt;00:53, 2.80MB/s] 12%|‚ñà‚ñè        | 20.3M/170M [00:14&lt;00:52, 2.85MB/s] 12%|‚ñà‚ñè        | 20.6M/170M [00:14&lt;00:51, 2.92MB/s] 12%|‚ñà‚ñè        | 21.0M/170M [00:14&lt;00:49, 3.00MB/s] 13%|‚ñà‚ñé        | 21.4M/170M [00:14&lt;00:48, 3.08MB/s] 13%|‚ñà‚ñé        | 21.8M/170M [00:14&lt;00:47, 3.15MB/s] 13%|‚ñà‚ñé        | 22.1M/170M [00:14&lt;00:45, 3.24MB/s] 13%|‚ñà‚ñé        | 22.5M/170M [00:14&lt;00:44, 3.33MB/s] 13%|‚ñà‚ñé        | 22.9M/170M [00:15&lt;00:43, 3.39MB/s] 14%|‚ñà‚ñé        | 23.3M/170M [00:15&lt;00:41, 3.51MB/s] 14%|‚ñà‚ñç        | 23.8M/170M [00:15&lt;00:41, 3.56MB/s] 14%|‚ñà‚ñç        | 24.2M/170M [00:15&lt;00:39, 3.71MB/s] 14%|‚ñà‚ñç        | 24.6M/170M [00:15&lt;00:38, 3.75MB/s] 15%|‚ñà‚ñç        | 25.1M/170M [00:15&lt;00:37, 3.86MB/s] 15%|‚ñà‚ñç        | 25.6M/170M [00:15&lt;00:36, 3.95MB/s] 15%|‚ñà‚ñå        | 26.0M/170M [00:15&lt;00:35, 4.09MB/s] 16%|‚ñà‚ñå        | 26.5M/170M [00:15&lt;00:34, 4.19MB/s] 16%|‚ñà‚ñå        | 27.0M/170M [00:16&lt;00:33, 4.23MB/s] 16%|‚ñà‚ñå        | 27.5M/170M [00:16&lt;00:32, 4.41MB/s] 16%|‚ñà‚ñã        | 28.0M/170M [00:16&lt;00:31, 4.52MB/s] 17%|‚ñà‚ñã        | 28.5M/170M [00:16&lt;00:30, 4.61MB/s] 17%|‚ñà‚ñã        | 29.1M/170M [00:16&lt;00:29, 4.76MB/s] 17%|‚ñà‚ñã        | 29.6M/170M [00:16&lt;00:28, 4.87MB/s] 18%|‚ñà‚ñä        | 30.2M/170M [00:16&lt;00:28, 4.94MB/s] 18%|‚ñà‚ñä        | 30.8M/170M [00:16&lt;00:27, 5.03MB/s] 18%|‚ñà‚ñä        | 31.4M/170M [00:16&lt;00:26, 5.18MB/s] 19%|‚ñà‚ñâ        | 32.0M/170M [00:17&lt;00:25, 5.36MB/s] 19%|‚ñà‚ñâ        | 32.6M/170M [00:17&lt;00:25, 5.42MB/s] 19%|‚ñà‚ñâ        | 33.2M/170M [00:17&lt;00:25, 5.37MB/s] 20%|‚ñà‚ñâ        | 33.7M/170M [00:17&lt;00:26, 5.22MB/s] 20%|‚ñà‚ñà        | 34.3M/170M [00:17&lt;00:25, 5.27MB/s] 21%|‚ñà‚ñà        | 35.0M/170M [00:17&lt;00:24, 5.63MB/s] 21%|‚ñà‚ñà        | 35.7M/170M [00:17&lt;00:22, 5.89MB/s] 21%|‚ñà‚ñà‚ñè       | 36.5M/170M [00:17&lt;00:21, 6.16MB/s] 22%|‚ñà‚ñà‚ñè       | 37.3M/170M [00:17&lt;00:20, 6.40MB/s] 22%|‚ñà‚ñà‚ñè       | 38.0M/170M [00:18&lt;00:20, 6.58MB/s] 23%|‚ñà‚ñà‚ñé       | 38.8M/170M [00:18&lt;00:19, 6.78MB/s] 23%|‚ñà‚ñà‚ñé       | 39.7M/170M [00:18&lt;00:18, 6.98MB/s] 24%|‚ñà‚ñà‚ñç       | 40.5M/170M [00:18&lt;00:18, 7.18MB/s] 24%|‚ñà‚ñà‚ñç       | 41.4M/170M [00:18&lt;00:17, 7.39MB/s] 25%|‚ñà‚ñà‚ñç       | 42.3M/170M [00:18&lt;00:16, 7.59MB/s] 25%|‚ñà‚ñà‚ñå       | 43.2M/170M [00:18&lt;00:16, 7.80MB/s] 26%|‚ñà‚ñà‚ñå       | 44.1M/170M [00:18&lt;00:15, 7.98MB/s] 26%|‚ñà‚ñà‚ñã       | 45.1M/170M [00:18&lt;00:15, 8.13MB/s] 27%|‚ñà‚ñà‚ñã       | 46.1M/170M [00:19&lt;00:14, 8.32MB/s] 28%|‚ñà‚ñà‚ñä       | 47.1M/170M [00:19&lt;00:14, 8.53MB/s] 28%|‚ñà‚ñà‚ñä       | 48.2M/170M [00:19&lt;00:13, 8.75MB/s] 29%|‚ñà‚ñà‚ñâ       | 49.2M/170M [00:19&lt;00:13, 9.15MB/s] 30%|‚ñà‚ñà‚ñâ       | 50.3M/170M [00:19&lt;00:12, 9.33MB/s] 30%|‚ñà‚ñà‚ñà       | 51.4M/170M [00:19&lt;00:12, 9.35MB/s] 31%|‚ñà‚ñà‚ñà       | 52.6M/170M [00:19&lt;00:12, 9.57MB/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 53.7M/170M [00:19&lt;00:11, 9.78MB/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 55.0M/170M [00:19&lt;00:11, 10.0MB/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 56.2M/170M [00:20&lt;00:11, 10.4MB/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 57.4M/170M [00:20&lt;00:10, 10.8MB/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 58.5M/170M [00:20&lt;00:10, 10.7MB/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 59.5M/170M [00:20&lt;00:10, 10.7MB/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 60.7M/170M [00:20&lt;00:10, 11.0MB/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 62.0M/170M [00:20&lt;00:09, 11.4MB/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 63.3M/170M [00:20&lt;00:09, 11.6MB/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 64.5M/170M [00:20&lt;00:08, 11.9MB/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 65.8M/170M [00:20&lt;00:08, 12.0MB/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 67.0M/170M [00:20&lt;00:08, 12.0MB/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 68.4M/170M [00:21&lt;00:08, 12.5MB/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 69.8M/170M [00:21&lt;00:07, 13.0MB/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 71.2M/170M [00:21&lt;00:07, 13.2MB/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 72.5M/170M [00:21&lt;00:07, 12.4MB/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 73.8M/170M [00:21&lt;00:08, 11.4MB/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 75.3M/170M [00:21&lt;00:07, 12.2MB/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 76.8M/170M [00:21&lt;00:07, 13.0MB/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 78.5M/170M [00:21&lt;00:06, 13.9MB/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 80.3M/170M [00:21&lt;00:06, 14.9MB/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 81.8M/170M [00:22&lt;00:06, 14.5MB/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 83.5M/170M [00:22&lt;00:05, 15.1MB/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 85.2M/170M [00:22&lt;00:05, 15.7MB/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 86.8M/170M [00:22&lt;00:05, 15.7MB/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 88.6M/170M [00:22&lt;00:05, 16.0MB/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 90.4M/170M [00:22&lt;00:04, 16.5MB/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 92.1M/170M [00:22&lt;00:04, 16.7MB/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 93.8M/170M [00:22&lt;00:04, 16.4MB/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 95.5M/170M [00:22&lt;00:04, 16.5MB/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 97.2M/170M [00:22&lt;00:04, 16.2MB/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 99.0M/170M [00:23&lt;00:04, 16.5MB/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 101M/170M [00:23&lt;00:04, 17.0MB/s]  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 103M/170M [00:23&lt;00:03, 17.1MB/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 104M/170M [00:23&lt;00:03, 16.6MB/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 106M/170M [00:23&lt;00:03, 16.7MB/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 108M/170M [00:23&lt;00:03, 16.5MB/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 110M/170M [00:23&lt;00:03, 16.6MB/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 112M/170M [00:23&lt;00:03, 17.2MB/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 113M/170M [00:23&lt;00:03, 17.1MB/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 115M/170M [00:24&lt;00:03, 17.1MB/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 117M/170M [00:24&lt;00:03, 17.1MB/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 119M/170M [00:24&lt;00:03, 16.9MB/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 120M/170M [00:24&lt;00:02, 16.8MB/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 122M/170M [00:24&lt;00:02, 16.5MB/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 124M/170M [00:24&lt;00:02, 16.5MB/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 126M/170M [00:24&lt;00:02, 16.8MB/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 127M/170M [00:24&lt;00:02, 17.3MB/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 129M/170M [00:24&lt;00:02, 17.8MB/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 131M/170M [00:24&lt;00:02, 17.6MB/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 133M/170M [00:25&lt;00:02, 17.0MB/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 135M/170M [00:25&lt;00:02, 16.8MB/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 136M/170M [00:25&lt;00:02, 16.3MB/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 138M/170M [00:25&lt;00:01, 16.3MB/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 140M/170M [00:25&lt;00:01, 16.6MB/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 142M/170M [00:25&lt;00:01, 16.6MB/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 144M/170M [00:25&lt;00:01, 17.2MB/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 146M/170M [00:25&lt;00:01, 17.4MB/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 147M/170M [00:25&lt;00:01, 17.4MB/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 149M/170M [00:26&lt;00:01, 17.3MB/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 151M/170M [00:26&lt;00:01, 17.2MB/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153M/170M [00:26&lt;00:01, 16.9MB/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 154M/170M [00:26&lt;00:00, 17.0MB/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 156M/170M [00:26&lt;00:00, 16.7MB/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 158M/170M [00:26&lt;00:00, 17.0MB/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160M/170M [00:26&lt;00:00, 16.9MB/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162M/170M [00:26&lt;00:00, 17.6MB/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 163M/170M [00:26&lt;00:00, 16.8MB/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 165M/170M [00:26&lt;00:00, 16.5MB/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 167M/170M [00:27&lt;00:00, 16.9MB/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 169M/170M [00:27&lt;00:00, 17.1MB/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:27&lt;00:00, 6.25MB/s]"
  },
  {
    "objectID": "posts/cifar_classif/index.html#the-model",
    "href": "posts/cifar_classif/index.html#the-model",
    "title": "Classify CIFAR",
    "section": "The model",
    "text": "The model\n\n\nCode\nimport lightning as L\nfrom typing import Optional, List\nimport torchmetrics\nfrom omegaconf import DictConfig, OmegaConf\n\nclass ClassificationModule(L.LightningModule):\n    def __init__(\n        self, \n        categories :List[str],\n        config:DictConfig,\n        model: Optional[torch.nn.Module] = None, \n        ):\n        \n        super().__init__()\n        self.categories = categories\n        num_classes = len(categories)\n        self.config = config\n        \n        self.model = model\n        if model is None:\n            self.model = torchvision.models.resnet18(num_classes=num_classes)\n\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n        metrics = torchmetrics.MetricCollection([\n            torchmetrics.classification.Accuracy(task = \"multiclass\", num_classes = num_classes),\n            torchmetrics.F1Score(task = \"multiclass\", num_classes = num_classes),\n            torchmetrics.Precision(task = \"multiclass\", num_classes = num_classes),\n            torchmetrics.Recall(task = \"multiclass\", num_classes = num_classes),\n            torchmetrics.CalibrationError(task = \"multiclass\", num_classes = num_classes),\n        ])\n\n        self.train_metric = metrics.clone(prefix=\"Train/\")\n        self.val_metrics = metrics.clone(prefix=\"Validation/\")\n        self.test_metrics = metrics.clone(prefix=\"Test/\")\n        \n\n        # conditional performances of our estimator\n        self.per_category_metrics = torchmetrics.MetricCollection([\n            torchmetrics.classification.Accuracy(task = \"multiclass\", num_classes = num_classes, average = None),\n            torchmetrics.F1Score(task = \"multiclass\", num_classes = num_classes, average = None),\n            torchmetrics.Precision(task = \"multiclass\", num_classes = num_classes, average = None),\n            torchmetrics.Recall(task = \"multiclass\", num_classes = num_classes, average = None),\n        ])\n\n    def forward(self, X):\n        return self.model(X)\n\n    def configure_optimizers(self):\n\n        optimizer = torch.optim.Adam(self.parameters(), lr = self.config.lr, weight_decay=1e-5)\n\n        #¬†you can add a scheduler here as well and return it as \n        # return [optimizer], [scheduler]\n        # \n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        images, targets = batch\n\n        outputs = self(images)\n\n        loss = self.criterion(outputs, targets)\n        \n        self.train_metric(outputs, targets)\n\n        self.log(\"Train/Loss\",loss, on_epoch=True, on_step=True, prog_bar=True)\n\n        return loss\n    \n    def on_train_epoch_end(self):\n\n        train_metrics=  self.train_metric.compute()\n\n        self.log_dict(train_metrics)\n\n        self.train_metric.reset()\n    \n    def validation_step(self, batch, batch_idx):\n        images, targets = batch\n\n        outputs = self(images)\n\n        loss = self.criterion(outputs, targets)\n        self.log(\"Validation/Loss\", loss, on_epoch=True, on_step=False)\n\n        self.val_metrics(outputs, targets)\n        self.per_category_metrics(outputs, targets)\n\n        \n    \n    def on_validation_epoch_end(self):\n\n        val_metrics =  self.val_metrics.compute()\n\n        self.log_dict(val_metrics)\n\n        m = self.per_category_metrics.compute()\n        for mname, mresults in m.items():\n            for i, catname in enumerate(self.categories):\n                self.log(f\"Validation/{mname}_{catname}\", mresults[i])\n\n        self.val_metrics.reset()\n        self.per_category_metrics.reset()\n    \n\n    def test_step(self, batch, batch_idx):\n        images, targets = batch\n\n        outputs = self(images)\n\n        loss = self.criterion(outputs, targets)\n        self.log(\"Test/Loss\", loss, on_epoch=True, on_step=False)\n\n        self.test_metrics(outputs, targets)\n        self.per_category_metrics(outputs, targets)\n\n        \n    \n    def on_test_epoch_end(self):\n\n        test_metrics =  self.test_metrics.compute()\n\n        self.log_dict(test_metrics)\n        m = self.per_category_metrics.compute()\n        for mname, mresults in m.items():\n            for i, catname in enumerate(self.categories):\n                self.log(f\"Validation/{mname}_{catname}\", mresults[i])\n\n        self.test_metrics.reset()\n        self.per_category_metrics.reset()\n\n\nconfig = OmegaConf.create({\n    \"lr\": 1e-5\n})\n\nmodel = ClassificationModule(\n    categories=train_set.classes,\n    config=config\n)\n\n\n##¬†Use everything for train\n\ntrainer=  L.Trainer(\n    max_epochs=3,\n    precision = \"16-mixed\",\n    enable_checkpointing=True,\n    num_sanity_val_steps=2,\n    log_every_n_steps=50,\n    check_val_every_n_epoch=1,\n)\n\n# trainer.fit(\n#     model,\n#     train_loader,\n#     val_loader\n# )\n\nUsing 16bit Automatic Mixed Precision (AMP)\nüí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\n/mnt/big_partition/projet/JulienCombes/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default"
  },
  {
    "objectID": "posts/cifar_classif/index.html#and-it-is-done",
    "href": "posts/cifar_classif/index.html#and-it-is-done",
    "title": "Classify CIFAR",
    "section": "And it is Done !",
    "text": "And it is Done !\nThe weights of the model are saved with the config that produced them."
  },
  {
    "objectID": "posts/research_PAISS2025/index.html",
    "href": "posts/research_PAISS2025/index.html",
    "title": "PAISS 2025",
    "section": "",
    "text": "Summer school about Machine Learning Where i presented a poster about the effect of data imbalance on active using on two industrial open source semantic segmentation datasets."
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#posters",
    "href": "posts/research_PAISS2025/index.html#posters",
    "title": "PAISS 2025",
    "section": "Posters",
    "text": "Posters\n\n\n\nQuantification of resource usage of multiple model by Matthieu\n\n\n\n\n\nClean specular rflexion in machine vision using gaussian splatting."
  },
  {
    "objectID": "posts/research_PAISS2025/index.html#discussions",
    "href": "posts/research_PAISS2025/index.html#discussions",
    "title": "PAISS 2025",
    "section": "Discussions",
    "text": "Discussions\nIt was the first time since the start of my Phd that i could talk with someone about active learning and how hard it is to fight with random selection. I particularly want to thank Maxime who told me that MC Dropout sampling worked (Gal, Islam, and Ghahramani 2017). I could not believe that he said that AL works haha. But besides being computationaly very expensive it looks like it might work as well for me. So thank you very much !"
  },
  {
    "objectID": "posts/research_ijcnn/index.html",
    "href": "posts/research_ijcnn/index.html",
    "title": "IJCNN 2025",
    "section": "",
    "text": "Conference about advances in deep learning. I had the immense to attend this event thanks to Michelin to support my friend and colleague Thomas who presented his work on multilabel instance segmentation applied to machine vision applications."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog will serve as a self-reminder and a repository for useful software code that I can easily share with others.\nI am currently in the second year of a PhD in Applied Mathematics. My research focuses on active learning and uncertainty quantification applied to computer vision.\nI am doing my PhD at Michelin, where there is a high demand for machine vision and not enough experts to label the data.\nI love learning and am very curious but software development is far from being a strength.\nMy passions :)\n\n\n\n\n\n\nboard games\n\n\n\n\n\n\n\nWine\n\n\n\n\n\n\n\n\n\nEuphorbes"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julien Combes",
    "section": "",
    "text": "ECAS 2025\n\n\nWinter School\n\n\n\nPhd\n\nproductivity\n\nENG\n\n\n\n\n\n\n\n\n\nDec 5, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nJRAF 2025\n\n\nAutumn School\n\n\n\nPhd\n\nconference\n\nENG\n\nFrugality\n\n\n\n\n\n\n\n\n\nDec 1, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nPAISS 2025\n\n\nSummer SChool\n\n\n\nPhd\n\ncode\n\nDeepLearning\n\nComputerVision\n\nENG\n\n\n\n\n\n\n\n\n\nSep 1, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nIJCNN 2025\n\n\nConference\n\n\n\nPhd\n\ncode\n\nDeepLearning\n\nComputerVision\n\nENG\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nD-Fine on pascal Voc\n\n\n\n\n\n\nPhd\n\ncode\n\nDeepLearning\n\nComputerVision\n\nENG\n\n\n\n\n\n\n\n\n\nJun 9, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nTools for Phd\n\n\n\n\n\n\nPhd\n\nproductivity\n\nENG\n\n\n\n\n\n\n\n\n\nMay 27, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nUV setup for computer vision using deep learning\n\n\n\n\n\n\nDeepLearning\n\ncode\n\nComputerVision\n\npython\n\nlinux\n\n\n\n\n\n\n\n\n\nApr 13, 2025\n\n\nJulien Combes\n\n\n\n\n\n\n\n\n\n\n\n\nClassify CIFAR\n\n\n\n\n\n\nDeepLearning\n\ncode\n\nImageClassification\n\nComputerVision\n\nENG\n\n\n\n\n\n\n\n\n\nApr 13, 2025\n\n\nJulien Combes\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/research_JRAF2025/index.html",
    "href": "posts/research_JRAF2025/index.html",
    "title": "JRAF 2025",
    "section": "",
    "text": "JRAF : Journ√©e de Recherche en Apprentissage Frugal (Days of research on Frugal Learning.)\nI had the opportunity to attend this conference at the end of november 2025."
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#exhaustive-measure-of-hardware",
    "href": "posts/research_JRAF2025/index.html#exhaustive-measure-of-hardware",
    "title": "JRAF 2025",
    "section": "Exhaustive measure of hardware",
    "text": "Exhaustive measure of hardware\nThe first part of the conf was about how to measure the consumption of our applications. We know that the bigger the models, the bigger the requirements in environmental resources (abiotic, water, energy). This particular issues with AI and deep learning applications is that most of the computing is not done on the research site, but the compute resources are rent to cloud providers like azure, aws etc‚Ä¶ So the measurements need to be estimated one way or another.\nSome researcher tried to measure the impact of computing devices (Falk et al. 2025) but the information from manufacters are quite lacking, so researchers have to use so tricks like retro engineering but in adds more uncertainty to the computations. LLM providers did provide some information about their environemental impact like mistral in this article but we cannnot do anything more than believing them.\nWhat society needs is:\n\nTransparency\nClarity of the functionnal unit : Mistral gives its impact for 400 tokens, Google for a median prompt. But does it relate to the usage of the tool ? What are the difference if we generate an image or a video ? how about text only ? Will 400 be enough for most usage or will i need more ? If so why did they choose to give the number for 400 tokens.\nOn what perimeter does the number relate ? [Construction|usage|end of life] of the product. scopes (sorry to cite mckinsey :‚Äô( )\nLimits : What functions or activities could have been forgotten or were not taken into account ?\n\nIt is indeed important to measure but if the measure does not comply with this criterion it can be called green washing or a unethic use of numbers. It is better to not give anything than on purpose incomplete numbers.\nThank you Danilo !"
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#measure-of-software",
    "href": "posts/research_JRAF2025/index.html#measure-of-software",
    "title": "JRAF 2025",
    "section": "Measure of software",
    "text": "Measure of software\nSome tools exists to measure the impacts of your own applications. This could be of interest for example if you have multiple ways of solving a problem, and you want to check which one require the least energy.\nFour tools exists for this purpose :\n\nEcoLogits : give an estimation of the impacts when a request is made to an LLM provider\nGreen Algorithm Project : Measure should not be the goal but the way of making people change their behavior, that is way their association work on making their tools as user friendly as possible. They can give certificate to teamas that work on green IT practices through their green disc certification\nCodeCarbon : Uses the drivers of the machine running the computations to track their emmisions in real time.\nAlumet : Provide by this nice guy and coded in rust. This package require a little bit more knowledge but as i understood it, it works at the OS level and track the emission of each process running on a machine.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteInteresting\n\n\n\nEach tool has its own public, i think for us data scientist, code carbon is the best. It integrates on existing code and provide detatiled report on the program executions consumptions."
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#conclusion-on-the-first-day",
    "href": "posts/research_JRAF2025/index.html#conclusion-on-the-first-day",
    "title": "JRAF 2025",
    "section": "Conclusion on the first day",
    "text": "Conclusion on the first day\nNow there is no incentives to work on frugality, on the contrary it is cool to do AI agentic AI or anything with as much buzzwords as possible. Some states are starting to debate about it like France but it will require ruling at an higher level.\nChristoph Becker ended the day with some social sciences notes. Computer scientist are good at solving problems with technical solution of their expertise. But in some situations, technical knowlege is not the right tool to solve societal issues. His arguments are from his book in open access : the insolvent.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWine and Cheese\n\n\n\nThe first evening was the occasion to talk with some old acquaintaces it was a pleasure !\nYoann is working on some interesting topics related to computer vision and sensor fusion. (Dupas et al. 2025). He will defend his Phd in January 2026 i look forward to it :) !!!"
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#good-benchmark-is-all-you-need",
    "href": "posts/research_JRAF2025/index.html#good-benchmark-is-all-you-need",
    "title": "JRAF 2025",
    "section": "(Good) Benchmark is all you need",
    "text": "(Good) Benchmark is all you need\nThomas Moreau Presented a library he works on Benchopt. ML field is a very experimental field, we are interested by making good results and spend less time on methods and formalism. That is why benchmark is a key component in ML research, if you want to publish your new algorithm, you want to show that it is better than the SOTA. So every researcher run the same computations again and again each time they want to test a new program.\n\n\n\n\nShort term\nLong term\n\n\n\n\nTask specific\nChallenges/Competitions\nSOTA Tracking\n\n\nGeneralizable\nResearch question\nBenchmarkFramework\n\n\n\nMost attention is on short term advanced for specific tasks but reliable science needs BenchMark framework that are task independent and makes you run only your own contribution and not the previous baselines."
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#if-you-own-the-computation-devices",
    "href": "posts/research_JRAF2025/index.html#if-you-own-the-computation-devices",
    "title": "JRAF 2025",
    "section": "If you own the computation devices",
    "text": "If you own the computation devices\nif you own computation clusters and you have the power to choose the sequencing of jobs Bruno Gaugal presented his work (Gaujal, Girault, and Plassart 2019). He uses job duration estimation to predict the duration of each job and modify the speed in order to get CPU to work at constant speed. If you have enough compute nodes you might prefer auto-scaling, which is scaling the number of active nodes depending on the demand. (Kambale et al. 2025)."
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#increase-inference-speed",
    "href": "posts/research_JRAF2025/index.html#increase-inference-speed",
    "title": "JRAF 2025",
    "section": "Increase inference speed",
    "text": "Increase inference speed\nHow can we make the network not use its full neurons if they are not needed ? That is the work of Martial who work on early-exit networks (Guidez et al. 2025).\n\n\n\n\n\n\nNote\n\n\n\nThis is interesting, it allows the network to be big enought at the training, and be as fast as possible at inference. The NN will have a inference time bounded by the prediction of the most uncertain data points. It is an alternative to distillation regarding the reduction of inference time."
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#llm",
    "href": "posts/research_JRAF2025/index.html#llm",
    "title": "JRAF 2025",
    "section": "LLM",
    "text": "LLM\nIncrease capabilities with smaller models using knowledge graphs (KG). (Mimouni, Moissinac, and Tuan 2020).\nHow do we link KG with LLm you will ask ? idk, but they do (Pan et al. 2024)."
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#learning-rate-scheduling-is-cheating",
    "href": "posts/research_JRAF2025/index.html#learning-rate-scheduling-is-cheating",
    "title": "JRAF 2025",
    "section": "Learning Rate scheduling is cheating",
    "text": "Learning Rate scheduling is cheating\nLR scheduling make the learning of the network very variable and it is an hyperparameter very hard to work with, i don‚Äôt that it is a bad idea but i tend to dislike their usage in order to be as parcimonious as possible.\nHere Bogdan showed us how they used control theory to dynamicaly adjust the learning rate to make the loss decrease as fast as possible. (Zhao et al. 2019)"
  },
  {
    "objectID": "posts/research_JRAF2025/index.html#conclusion-day-2",
    "href": "posts/research_JRAF2025/index.html#conclusion-day-2",
    "title": "JRAF 2025",
    "section": "Conclusion Day 2",
    "text": "Conclusion Day 2\nWe should integrate frugality measurement and try to avoid using big models if what they make us gain is so small comparatively with the increase of computational power required.\ni am very happy i attended this event. I could meet nice people and be exposed to what we never see in industry where we are more oriented on results than on externalities.\nShould we continue to work on AI ? If not, will we loose competitiveness against our neighbours ? Who should take this kind of decisions ?\nShould we accelerate the end or prefer a slow death to share it future generations ?"
  },
  {
    "objectID": "posts/phd_tools/index.html",
    "href": "posts/phd_tools/index.html",
    "title": "Tools for Phd",
    "section": "",
    "text": "Phd is a long and likely hard journey, if some tools can make the way easier we won‚Äôt complain about it. I want to gather tools that i find useful in this post.\nIt can be related to bibliography, writing, coding or anything that pass to mind, it should be updated at some point when new things come to my mind."
  },
  {
    "objectID": "posts/phd_tools/index.html#scholar-inbox",
    "href": "posts/phd_tools/index.html#scholar-inbox",
    "title": "Tools for Phd",
    "section": "Scholar inbox",
    "text": "Scholar inbox\nThis tool is so nice to be updated anytime an article in our field of research is published. You need to create an account an train a recommendation system based on literature that you are interested in. When it‚Äôs done, you will have new article pre-published in the past week that relates to your research.\nS/O to MG that let me know this tool !\nedit-0 : It makes the finding of new article very easy, but the rate at which new article are brought to you is higher than the reading speed unfortunately. So it just make your reading list increases haha. Still need to see how to manage it."
  },
  {
    "objectID": "posts/phd_tools/index.html#bibliography",
    "href": "posts/phd_tools/index.html#bibliography",
    "title": "Tools for Phd",
    "section": "Bibliography",
    "text": "Bibliography\nI can‚Äôt recommend Zotero enough. You can save article from the browser in one click. Read and annotate all in one place.\nFolder VS Tags debate: For the document management, i would recommend you to think of all the folders and subfolders you would make and not create them. Usually, one document belongs to multiple categories, and you don‚Äôt want to duplicate all your sources et each folders/subfolders. That is why i would assign tags to each document at the time you add them to you library. That way, you can search your document by tag and it makes it very easy."
  },
  {
    "objectID": "posts/phd_tools/index.html#note-taking",
    "href": "posts/phd_tools/index.html#note-taking",
    "title": "Tools for Phd",
    "section": "Note taking",
    "text": "Note taking\nI like to use obsidian with the citation extension. It connects the notes with the zotero library and allows you to link all the article and is very helpful when you want to write about any topic."
  },
  {
    "objectID": "posts/phd_tools/index.html#literature-graph",
    "href": "posts/phd_tools/index.html#literature-graph",
    "title": "Tools for Phd",
    "section": "Literature Graph",
    "text": "Literature Graph\nConnected papers : not free. Lit map could be an alternative."
  },
  {
    "objectID": "posts/phd_tools/index.html#redaction",
    "href": "posts/phd_tools/index.html#redaction",
    "title": "Tools for Phd",
    "section": "Redaction",
    "text": "Redaction\nmy favorite software of this stack Quarto. It allow you to write anything in markdown and generate pdfs (with latex or typst), beamers or revealjs and this blog your are currently reading !\nIt is an amazing piece of software based on Pandoc the Haskell unicorn.\nIf you are in stats or working with any type of data, quarto is nice since you can code in the same file as the one you write in. It is like a notebook but plain text, that allows version control systems to follow it nicely."
  },
  {
    "objectID": "posts/uv_ml/index.html",
    "href": "posts/uv_ml/index.html",
    "title": "UV setup for computer vision using deep learning",
    "section": "",
    "text": "UV is a drop-in replacement for pip and global python installation. I allows the management of python versions and packages. So far i only used it as a replacement for conda/venv so i only have a shallow understanding of basic feature. The build capabilities are not handled in this page yet."
  },
  {
    "objectID": "posts/uv_ml/index.html#local-quick-environement",
    "href": "posts/uv_ml/index.html#local-quick-environement",
    "title": "UV setup for computer vision using deep learning",
    "section": "Local quick environement",
    "text": "Local quick environement\nOnly use a local environement that will be used only for this project. This option is good for prototyping.\nIn order to manage the part with GPU compatibilities, uv provides an help page to get the torch versions matching your locally installed cuda :\n1uv venv --python 3.12\nuv pip install torch torchvision --torch-backend=auto\n2uv pip install -r requirements.txt\n\n1\n\nCreating the virtual environement in the current directory with the specified python version.\n\n2\n\nInstall all packages listed in requirement.txt in the local env.\n\n\nomegaconf\n\nnumpy\npandas\nseaborn\nmatplotlib\n\ntyper\n\npycocotools\nalbumentationsX\nopencv-python\ntorchmetrics\nlightning\ntransformers"
  },
  {
    "objectID": "posts/uv_ml/index.html#complete-and-reproducible-dev",
    "href": "posts/uv_ml/index.html#complete-and-reproducible-dev",
    "title": "UV setup for computer vision using deep learning",
    "section": "Complete and reproducible dev",
    "text": "Complete and reproducible dev\nIf you want a complete reproducible management of dependencies you should use the project features of uv.\nuv init --python 3.14 .\nuv pip install torch torchvision --torch-backend=auto\nuv add -r requirements.txt\nHere, uv will create a full pyproject.toml where the python version and all the dependencies whith their versions are saved."
  },
  {
    "objectID": "posts/voc_detection/index.html",
    "href": "posts/voc_detection/index.html",
    "title": "D-Fine on pascal Voc",
    "section": "",
    "text": "Object detection is the second type of problem we will be solving in this blog using lightning and pascal voc. This problem is very common because it allow to locate a variable number of objects in images.\nSince it is used in case of autonomous driving, real time object counting object detection architectures are grouped into two categories. The real time object detection where the boss is the YOLO version 11 at the time i write this article. The more recent networks that can fight with yolo in RTOD (Real time object detection) is the D-Fine architecture which is an improved DETR.\nWe implemented it in this article, we chose to train this architecture in lightning since the training loop can be re-used with other models and dataset. I don‚Äôt know what is the best training paradigm but i want one that keeps as agnostic as possible to any framework, and for me lighting allow strong customization and proximity to pure torchscript while allowing easy multi device training and metrics logging etc..\n\nData\nWe will model pascal voc images in this project because its very easily accessible from torchvision repos.\n\nfrom torchvision.datasets import VOCDetection\nfrom torchvision.transforms import v2 \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import DataLoader\nimport torch \nfrom omegaconf import OmegaConf\nfrom torchvision.ops import box_convert\nfrom torchvision.utils import draw_bounding_boxes\nimport albumentations as A\nfrom transformers import DFineForObjectDetection, AutoImageProcessor\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision.ops import batched_nms\nimport torchmetrics\nimport lightning as L\nfrom torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\nfrom typing import Dict, List, Optional\nimport os\n\nd-fine expectS bbox in coco format so we convert it in the get_item part.\n\ncategories = ['pottedplant', 'bottle', 'chair', 'diningtable', 'person', 'car', 'train', 'bus', 'bicycle', 'cat', 'aeroplane', 'tvmonitor', 'sofa', 'sheep', 'dog', 'bird', 'motorbike', 'horse', 'boat', 'cow', \"bg\"]\n\n\ndef unpack_box(box_dict:Dict):\n    \"\"\"\n    Unpack the box dictionary into a list of coordinates\n    \"\"\"\n    return torch.tensor(np.array([\n        box_dict[\"xmin\"],\n        box_dict[\"ymin\"],\n        box_dict[\"xmax\"],\n        box_dict[\"ymax\"]\n    ], dtype = float))\n\ndef annotation_to_torch(target:Dict):\n    rep = {}\n    detections = target[\"annotation\"][\"object\"]\n    \n    rep[\"labels\"] = np.array([categories.index(i[\"name\"]) for i in detections])\n    # xmin ymin xmax ymax\n    rep[\"boxes\"] = torch.stack([unpack_box(i[\"bndbox\"]) for i in detections])\n\n    return rep    \n\nimage_processor = AutoImageProcessor.from_pretrained(\"ustc-community/dfine_x_coco\", use_fast=True)\n\nclass MyVoc(VOCDetection):\n    def __getitem__(self, index):\n        image, target = super().__getitem__(index)\n        # Apply your transformations here\n        target = annotation_to_torch(target)\n\n        transform = A.Compose([\n            A.PadIfNeeded(500,500),\n            A.HorizontalFlip(),\n            A.RandomCrop(400,400),\n            A.Resize(224, 224),\n            A.Normalize(normalization=\"min_max\"),\n            ToTensorV2()\n        ],\n        bbox_params=A.BboxParams(\n            format='pascal_voc', # Specify input format\n            label_fields=['class_labels'],\n            filter_invalid_bboxes=True)\n        )\n\n        transformed = transform(\n            image=np.array(image), \n            bboxes=target[\"boxes\"], \n            class_labels=target[\"labels\"]\n        )\n\n        transformed[\"labels\"] = transformed[\"class_labels\"]\n        transformed[\"boxes\"] = transformed[\"bboxes\"]\n        transformed.pop(\"bboxes\")\n\n        image = transformed.pop(\"image\")\n        transformed[\"boxes\"] = box_convert(\n            torch.from_numpy(transformed[\"boxes\"],\n            ),\n            \"xyxy\",\n            \"xywh\",).float()\n        transformed[\"labels\"] = torch.from_numpy(transformed[\"labels\"])\n        transformed[\"class_labels\"] = torch.from_numpy(transformed[\"class_labels\"])\n        return image.float(), transformed\n\n    def draw_item(self, index:Optional[int] = None, n=5):\n        if index is None:\n            index = np.random.randint(0, len(self))\n\n        image, labels = self[index]\n\n\n        with_boxes = draw_bounding_boxes(\n            image =  image,\n            boxes= box_convert(labels[\"boxes\"], \"xywh\", \"xyxy\"),\n            labels = [categories[i] for i in labels[\"labels\"]],\n            colors = \"red\"\n\n        )\n\n\n        plt.figure(figsize=(10, 10))\n        plt.imshow(with_boxes.permute(1, 2, 0).numpy())\n        plt.axis('off')\n        plt.show()\n        return \n\nNothing special here, we show what the data looks like.\n\ndatarootdir = os.environ.get(\"DATA_ROOTPATH\", \"~/data_wsl/voc\")\nds = MyVoc(\n    root = os.path.join(datarootdir, \"voc\"), \n    download = False,\n    image_set=\"train\"\n)\nval_ds = MyVoc(\n    root = os.path.join(datarootdir, \"voc\"), \n    download = False,\n    image_set=\"val\",\n)\nds.draw_item()\n\n\n\n\n\n\n\n\n\n\nModeling\nModeling part is here, with the lightning module.\n\n\nCode\nconfig = OmegaConf.create({\n    \"lr\": 1e-4,\n    \"batch_size\":2,\n    \"epochs\":3,\n    \"world_size\":1\n})\ndef apply_nms(preds:Dict, iou_thr:float = .5):\n    nms_indices = batched_nms(\n        preds[\"boxes\"],\n        scores = preds[\"scores\"],\n        idxs=preds[\"labels\"],\n        iou_threshold=iou_thr\n    )\n    preds_nms = {}\n    preds_nms[\"boxes\"] = preds[\"boxes\"][nms_indices,:]\n    preds_nms[\"scores\"] = preds[\"scores\"][nms_indices]\n    preds_nms[\"labels\"] = preds[\"labels\"][nms_indices]\n    \n    # high_scores_indices = preds_nms[\"scores\"] &gt; .3\n    # preds_nms[\"boxes\"] = preds_nms[\"boxes\"][high_scores_indices]\n    # preds_nms[\"scores\"] = preds_nms[\"scores\"][high_scores_indices]\n    # preds_nms[\"labels\"] = preds_nms[\"labels\"][high_scores_indices]\n\n    return preds_nms\n\nclass odModule(L.LightningModule):\n    def __init__(self, config, categories:List[str], nms_thr:float = .5):\n        super().__init__()\n        # if config.checkpoint is not None:\n        # print(f\"checkpoint from {config.checkpoint}\")\n        num_classes = len(categories)\n        self.categories = categories\n\n        self.nms_thr = nms_thr\n        self.config = config\n        # self.model = fasterrcnn_mobilenet_v3_large_fpn(pretrained = False, num_classes=num_classes)\n        self.model = DFineForObjectDetection.from_pretrained(\n            \"ustc-community/dfine_x_coco\",\n            id2label= {i:cat for i,cat in enumerate(categories)},\n            label2id={cat:i for i,cat in enumerate(categories)},\n            ignore_mismatched_sizes=True,\n        )\n\n        metrics = torchmetrics.MetricCollection(\n            [\n                torchmetrics.detection.mean_ap.MeanAveragePrecision(\n                    # extended_summary=True, \n                    iou_thresholds=np.linspace(0,1,20).tolist(),\n                    class_metrics=True, \n                    iou_type=\"bbox\",\n                ),\n            ]\n        )   \n        self.val_metrics = metrics.clone(prefix=\"Validation/\")\n        self.test_metrics = metrics.clone(prefix=\"Test/\")\n    \n        self.save_hyperparameters(ignore=[\"train_ds\"])\n\n    @staticmethod\n    def prepare_batch(batch):\n        images, targets = batch\n        return images, targets\n\n    def forward(self, x, y=None):\n        if y is not None:\n            return self.model(pixel_values = x, labels = y)\n        else:\n            preds = self.model(x)\n            n, c, h, w = x.shape\n            preds = image_processor.post_process_object_detection(\n                preds, \n                target_sizes=[(h,w) for _ in range(n)], \n                threshold=0.5)\n            return preds\n\n    def predict(self, x):\n        \"\"\"Forward the model then run NMS (for evaluation)\n\n        Args:\n            x (_type_): _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"        \n        preds:List = self(x)\n        # preds_nms = [apply_nms(i, self.nms_thr) for i in preds]\n        return preds\n\n\n    def training_step(self, batch, batch_idx):\n        img_b, target_b = self.prepare_batch(batch)\n        bs = len(img_b)\n\n        dfine_output = self(img_b, target_b)\n\n        self.log_dict(\n            dfine_output.loss_dict,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n            batch_size=bs,\n            prog_bar=True,\n        )\n\n        return {\"loss\": dfine_output.loss}\n\n    def validation_step(self, batch, batch_idx):\n        img_b, target_b = self.prepare_batch(batch)\n        output_nms = self.predict(img_b)\n\n        self.val_metrics(output_nms, target_b)\n        return\n\n    def on_validation_epoch_end(self):\n        \n        m = self.val_metrics.compute()\n        m_single= {i:j for i,j in m.items() if j.nelement() ==1}\n        \n        self.log_dict(m_single, on_epoch=True, sync_dist=False)\n\n        for i, class_id in enumerate(m[\"Validation/classes\"]):\n            self.log(f\"Validation/MAP {self.categories[class_id]}\", m[\"Validation/map_per_class\"][i])\n\n        self.val_metrics.reset()\n        return\n\n    def test_step(self, batch, batch_idx):\n        img_b, target_b = self.prepare_batch(batch)\n\n        output_nms = self.predict(img_b)\n\n        self.test_metrics(output_nms, target_b)\n        return\n\n    def on_test_epoch_end(self):\n        m = self.test_metrics.compute()\n        m_single= {i:j for i,j in m.items() if j.nelement() ==1}\n\n        self.log_dict(m_single, on_epoch=True, sync_dist=False)\n        \n        for i, class_id in enumerate(m[\"Test/classes\"]):\n            self.log(f\"Test/MAP {self.categories[class_id]}\", m[\"Test/map_per_class\"][i])\n\n        self.test_metrics.reset()\n\n        return \n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(\n            self.parameters(),\n            lr=self.config.lr,\n            weight_decay=1e-4 * self.config.batch_size / 16,\n        )\n\n        # scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(\n        #     optimizer, T_max=scheduler_nsteps, eta_min=self.config.lr / 10\n        # )\n\n        # sched_config1 = {\"scheduler\": scheduler1, \"interval\": \"epoch\"}\n\n\n        return [optimizer]#, [sched_config1]\n\nmodel = odModule(\n    config,\n    categories\n)\n\n\n\ndef collate_fn(batch):\n    # Separate the images and targets\n    images = []\n    targets = []\n    \n    for image, target in batch:\n        images.append(image)  # Assuming each item has an 'image' key\n        targets.append(target)  # Assuming each item has a 'target' key\n\n    # Stack images into a single tensor\n    images = torch.stack(images, dim=0)\n\n    # Convert targets to a list of dictionaries or tensors\n    # This depends on how your targets are structured\n    # For example, if targets are dictionaries with bounding boxes and labels\n    return images, targets\ntrain_loader = DataLoader(\n    ds,\n    batch_size = 2,\n    shuffle = True,\n    collate_fn = collate_fn\n)\nval_loader = DataLoader(\n    val_ds,\n    batch_size = 2,\n    shuffle = False,\n    collate_fn = collate_fn\n)\nim, tar = next(iter(train_loader))\n\ntar\n\n[{'class_labels': tensor([9]),\n  'labels': tensor([9]),\n  'boxes': tensor([[ 84.5600,  82.3200,  85.1200, 125.4400]])},\n {'class_labels': tensor([17, 17,  4]),\n  'labels': tensor([17, 17,  4]),\n  'boxes': tensor([[  0.0000,  66.6400, 196.0000, 157.3600],\n          [  0.0000, 117.6000, 124.3200, 106.4000],\n          [129.3600, 168.5600,  22.4000,  24.6400]])}]\n\n\n\n\nTraining\nAll the training is shown here. The full training is happening here.\nthe full training has not been done yet but no bug has been encountered in first batches.\n\nmodel.train()\ntrainer=  L.Trainer(\n    max_epochs=3,\n    max_steps = 10, # limit training to make sure it works.\n    precision = \"16-mixed\",\n    enable_checkpointing=True,\n    num_sanity_val_steps=0,\n    log_every_n_steps=50,\n    check_val_every_n_epoch=1,\n)\n\ntrainer.fit(model, train_loader, val_loader)\n\nUsing 16bit Automatic Mixed Precision (AMP)\nüí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\n/mnt/big_partition/projet/JulienCombes/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/mnt/big_partition/projet/JulienCombes/.venv/lib/python3.13/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n\n  | Name         | Type                    | Params | Mode \n-----------------------------------------------------------------\n0 | model        | DFineForObjectDetection | 62.5 M | train\n1 | val_metrics  | MetricCollection        | 0      | train\n2 | test_metrics | MetricCollection        | 0      | train\n-----------------------------------------------------------------\n62.5 M    Trainable params\n2         Non-trainable params\n62.5 M    Total params\n250.001   Total estimated model params size (MB)\n1420      Modules in train mode\n0         Modules in eval mode\n/mnt/big_partition/projet/JulienCombes/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n/mnt/big_partition/projet/JulienCombes/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n\n\n\n\n\n`Trainer.fit` stopped: `max_steps=10` reached."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Hope it will not be empty at the end of the Phd üò¢.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]