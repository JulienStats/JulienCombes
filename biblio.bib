@misc{180209069ActiveLearning,
  title = {[1802.09069] {{Active Learning}} with {{Logged Data}}},
  urldate = {2025-09-19},
  howpublished = {https://arxiv.org/abs/1802.09069},
  file = {C:\Users\E097600\Zotero\storage\FUYCX9Y8\1802.html}
}

@misc{241115288v1ThereNo,
  title = {[2411.15288v1] {{There}} Is No {{SAMantics}}! {{Exploring SAM}} as a {{Backbone}} for {{Visual Understanding Tasks}}},
  urldate = {2025-11-17},
  howpublished = {https://arxiv.org/abs/2411.15288v1},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\DRQNRYR5\\[2411.15288v1] There is no SAMantics! Exploring SAM as a Backbone for Visual Understanding Tasks.pdf;C\:\\Users\\E097600\\Zotero\\storage\\FC76WMUQ\\2411.html}
}

@misc{abhishekWhatCanWe2025,
  title = {What {{Can We Learn}} from {{Inter-Annotator Variability}} in {{Skin Lesion Segmentation}}?},
  author = {Abhishek, Kumar and Kawahara, Jeremy and Hamarneh, Ghassan},
  year = 2025,
  month = aug,
  number = {arXiv:2508.09381},
  eprint = {2508.09381},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.09381},
  urldate = {2025-09-23},
  abstract = {Medical image segmentation exhibits intra- and inter-annotator variability due to ambiguous object boundaries, annotator preferences, expertise, and tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated or infiltrative nodules, or irregular borders per the ABCD rule, are particularly prone to disagreement and are often associated with malignancy. In this work, we curate IMA++, the largest multi-annotator skin lesion segmentation dataset, on which we conduct an in-depth study of variability due to annotator, malignancy, tool, and skill factors. We find a statistically significant (p{$<$}0.001) association between inter-annotator agreement (IAA), measured using Dice, and the malignancy of skin lesions. We further show that IAA can be accurately predicted directly from dermoscopic images, achieving a mean absolute error of 0.108. Finally, we leverage this association by utilizing IAA as a "soft" clinical feature within a multi-task learning objective, yielding a 4.2\% improvement in balanced accuracy averaged across multiple model architectures and across IMA++ and four public dermoscopic datasets. The code is available at https://github.com/sfu-mial/skin-IAV.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\HDHKNNIR\\Abhishek et al. - 2025 - What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\MGSKSPTQ\\2508.html}
}

@inproceedings{abrahamRebuildingTrustActive2020,
  title = {Rebuilding {{Trust}} in {{Active Learning}} with {{Actionable Metrics}}},
  booktitle = {2020 {{International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  author = {Abraham, Alexandre and {Dreyfus-Schmidt}, L{\'e}o},
  year = 2020,
  month = nov,
  pages = {836--843},
  issn = {2375-9259},
  doi = {10.1109/ICDMW51313.2020.00120},
  urldate = {2025-10-27},
  abstract = {Active Learning (AL) is an active domain of research, but is seldom used in the industry despite the pressing needs. This is in part due to a misalignment of objectives, while research strives at getting the best results on selected datasets, the industry wants guarantees that Active Learning will perform consistently and at least better than random labeling. The very one-off nature of Active Learning makes it crucial to understand how strategy selection can be carried out and what drives poor performance (lack of exploration, selection of samples that are too hard to classify, \dots ). To help rebuild trust of industrial practitioners in Active Learning, we present various actionable metrics. Through extensive experiments on reference datasets such as CIFAR100, Fashion-MNIST, and 20Newsgroups, we show that those metrics brings interpretability to AL strategies that can be leveraged by the practitioner.},
  keywords = {Active learning,Data mining,incremental test,Industries,Labeling,Libraries,Machine learning,Measurement,method,Monitoring,Performance measures,Pressing,Software libraries},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\TYCDUYWE\\Abraham et Dreyfus-Schmidt - 2020 - Rebuilding Trust in Active Learning with Actionable Metrics.pdf;C\:\\Users\\E097600\\Zotero\\storage\\5RTBJW3T\\9346300.html}
}

@misc{ackermannMaskomalyZeroShotMask2023,
  title = {Maskomaly:{{Zero-Shot Mask Anomaly Segmentation}}},
  shorttitle = {Maskomaly},
  author = {Ackermann, Jan and Sakaridis, Christos and Yu, Fisher},
  year = 2023,
  month = aug,
  number = {arXiv:2305.16972},
  eprint = {2305.16972},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.16972},
  urldate = {2025-09-17},
  abstract = {We present a simple and practical framework for anomaly segmentation called Maskomaly. It builds upon mask-based standard semantic segmentation networks by adding a simple inference-time post-processing step which leverages the raw mask outputs of such networks. Maskomaly does not require additional training and only adds a small computational overhead to inference. Most importantly, it does not require anomalous data at training. We show top results for our method on SMIYC, RoadAnomaly, and StreetHazards. On the most central benchmark, SMIYC, Maskomaly outperforms all directly comparable approaches. Further, we introduce a novel metric that benefits the development of robust anomaly segmentation methods and demonstrate its informativeness on RoadAnomaly.},
  archiveprefix = {arXiv},
  keywords = {anomaly detection,Computer Science - Computer Vision and Pattern Recognition,semantic segmentation,zero-shot},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\FAKQCZ35\\Ackermann et al. - 2023 - MaskomalyZero-Shot Mask Anomaly Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\BDLE4J4F\\2305.html}
}

@misc{agarwalContextualDiversityActive2020,
  title = {Contextual {{Diversity}} for {{Active Learning}}},
  author = {Agarwal, Sharat and Arora, Himanshu and Anand, Saket and Arora, Chetan},
  year = 2020,
  month = aug,
  number = {arXiv:2008.05723},
  eprint = {2008.05723},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.05723},
  urldate = {2025-10-29},
  abstract = {Requirement of large annotated datasets restrict the use of deep convolutional neural networks (CNNs) for many practical applications. The problem can be mitigated by using active learning (AL) techniques which, under a given annotation budget, allow to select a subset of data that yields maximum accuracy upon fine tuning. State of the art AL approaches typically rely on measures of visual diversity or prediction uncertainty, which are unable to effectively capture the variations in spatial context. On the other hand, modern CNN architectures make heavy use of spatial context for achieving highly accurate predictions. Since the context is difficult to evaluate in the absence of ground-truth labels, we introduce the notion of contextual diversity that captures the confusion associated with spatially co-occurring classes. Contextual Diversity (CD) hinges on a crucial observation that the probability vector predicted by a CNN for a region of interest typically contains information from a larger receptive field. Exploiting this observation, we use the proposed CD measure within two AL frameworks: (1) a core-set based strategy and (2) a reinforcement learning based policy, for active frame selection. Our extensive empirical evaluation establish state of the art results for active learning on benchmark datasets of Semantic Segmentation, Object Detection and Image Classification. Our ablation studies show clear advantages of using contextual diversity for active learning. The source code and additional results are available at https://github.com/sharat29ag/CDAL.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Computer Vision and Pattern Recognition,Image classification,Object detection,semantic segmentation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\WB8URPMB\\Agarwal et al. - 2020 - Contextual Diversity for Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\MV62SARC\\2008.html}
}

@inproceedings{aggarwalActiveLearningImbalanced2020,
  title = {Active {{Learning}} for {{Imbalanced Datasets}}},
  booktitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Aggarwal, Umang and Popescu, Adrian and Hudelot, C{\'e}line},
  year = 2020,
  month = mar,
  pages = {1417--1426},
  issn = {2642-9381},
  doi = {10.1109/WACV45572.2020.9093475},
  urldate = {2025-04-23},
  abstract = {Active learning increases the effectiveness of labeling when only subsets of unlabeled datasets can be processed manually. To our knowledge, existing algorithms are designed under the assumption that datasets are balanced. However, many real-life datasets are actually imbalanced and we propose two adaptations of active learning to tackle imbalance. First, we modify acquisition functions to select samples by taking advantage of a deep model pretrained on a source domain. Second, we introduce a balancing step in the acquisition process to reduce the imbalance of the labeled subset. Evaluation is done with four imbalanced datasets using existing active learning methods and their modifications introduced here. Results show that our adaptations are useful as long as knowledge from the source domain is transferable to target domains.},
  keywords = {Adaptation models,Class imbalance,Entropy,Labeling,Machine Learning,Manuals,Predictive models,Uncertainty},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\J9CMYS7X\\Aggarwal et al. - 2020 - Active Learning for Imbalanced Datasets.pdf;C\:\\Users\\E097600\\Zotero\\storage\\VRY9P59A\\9093475.html}
}

@misc{aggarwalMinorityClassOriented2022,
  title = {Minority {{Class Oriented Active Learning}} for {{Imbalanced Datasets}}},
  author = {Aggarwal, Umang and Popescu, Adrian and Hudelot, C{\'e}line},
  year = 2022,
  month = feb,
  journal = {arXiv.org},
  doi = {10.1109/ICPR48806.2021.9412182},
  urldate = {2025-05-26},
  abstract = {Active learning aims to optimize the dataset annotation process when resources are constrained. Most existing methods are designed for balanced datasets. Their practical applicability is limited by the fact that a majority of real-life datasets are actually imbalanced. Here, we introduce a new active learning method which is designed for imbalanced datasets. It favors samples likely to be in minority classes so as to reduce the imbalance of the labeled subset and create a better representation for these classes. We also compare two training schemes for active learning: (1) the one commonly deployed in deep active learning using model fine tuning for each iteration and (2) a scheme which is inspired by transfer learning and exploits generic pre-trained models and train shallow classifiers for each iteration. Evaluation is run with three imbalanced datasets. Results show that the proposed active learning method outperforms competitive baselines. Equally interesting, they also indicate that the transfer learning training scheme outperforms model fine tuning if features are transferable from the generic dataset to the unlabeled one. This last result is surprising and should encourage the community to explore the design of deep active learning methods.},
  howpublished = {https://arxiv.org/abs/2202.00390v1},
  langid = {english},
  keywords = {Active learning,Class imbalance,Image classification},
  file = {C:\Users\E097600\Zotero\storage\8I3UVV8R\Aggarwal et al. - 2022 - Minority Class Oriented Active Learning for Imbalanced Datasets.pdf}
}

@article{ahmadFusionMetadataDermoscopic2025,
  title = {Fusion of Metadata and Dermoscopic Images for Melanoma Detection: {{Deep}} Learning and Feature Importance Analysis},
  shorttitle = {Fusion of Metadata and Dermoscopic Images for Melanoma Detection},
  author = {Ahmad, Misbah and Ahmed, Imran and Chehri, Abdellah and Jeon, Gwangill},
  year = 2025,
  month = dec,
  journal = {Information Fusion},
  volume = {124},
  pages = {103304},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2025.103304},
  urldate = {2025-11-17},
  abstract = {In the era of smart healthcare, integrating multimodal data is essential for improving diagnostic accuracy and enabling personalized care. This study presented a deep learning-based multimodal approach for melanoma detection, leveraging both dermoscopic images and clinical metadata to enhance classification performance. The proposed model integrated a multi-layer convolutional neural network (CNN) to extract image features and combined them with structured metadata, including patient age, gender, and lesion location, through feature-level fusion. The fusion process occurred at the final CNN layer, where high-dimensional image feature vectors were concatenated with processed metadata. The metadata was handled separately through a fully connected neural network comprising multiple dense layers. The final fused representation was passed through additional dense layers, culminating in a classification layer that outputted the probability of melanoma presence. The model was trained end-to-end using the SIIM-ISIC dataset, allowing it to learn a joint representation of image and metadata features for optimal classification. Various data augmentation techniques were applied to dermoscopic images to mitigate class imbalance and improve model robustness. Additionally, exploratory data analysis (EDA) and feature importance analysis were conducted to assess the contribution of each metadata feature to the overall classification. Our fusion-based deep learning architecture outperformed single-modality models, boosting classification accuracy. The presented model achieved an accuracy of 94.5\% and an overall F1-score of 0.94, validating its effectiveness in melanoma detection. This study aims to highlight the potential of deep learning-based multimodal fusion in enhancing diagnostic precision, offering a scalable and reliable solution for improved melanoma detection in smart healthcare systems.},
  keywords = {Clinical metadata,Deep learning,Dermoscopic images,fusion model,Melanoma detection,metadata,Multi-modal data fusion,Smart healthcare},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\CGXYKSJ5\\Ahmad et al. - 2025 - Fusion of metadata and dermoscopic images for melanoma detection Deep learning and feature importan.pdf;C\:\\Users\\E097600\\Zotero\\storage\\CYNW8UPW\\S156625352500377X.html}
}

@article{al-emadiAnalysingSatelliteImagery2025,
  title = {Analysing {{Satellite Imagery Classification}} under {{Spatial Domain Shift}} across {{Geographic Regions}}},
  author = {{Al-Emadi}, Sara A. and Yang, Yin and Ofli, Ferda},
  year = 2025,
  month = aug,
  journal = {International Journal of Computer Vision},
  issn = {1573-1405},
  doi = {10.1007/s11263-025-02518-z},
  urldate = {2025-09-17},
  abstract = {Deep learning models are designed based on the i.i.d. assumption; consequently, they experience a significant performance drop due to the distribution shifts when deployed in real environments. Domain Generalisation (DG) aims to bridge the distribution shift between the source and target domains by improving the generalisability of the model to Out-Of-Distribution (OOD) data. This challenge is prominent in satellite imagery classification due to the scarcity of data from underrepresented regions such as Africa and Oceania. In this paper, we address the limitations of existing datasets in capturing distribution shifts caused by geospatial differences between geographic regions by constructing a new, large-scale dataset called Domain Shift across Geographic Regions (DSGR). This dataset aims to help researchers better understand the impact of distribution shifts on satellite imagery classification. Furthermore, we perform rigorous experiments on DSGR to investigate and benchmark the robustness of existing DG techniques under single- and multi-source domain settings and the role of foundation models in enhancing the DG techniques. Our evaluations reveal that recent DG techniques have a comparable, yet weak, performance on DSGR. However, when combined with a foundation model like CLIP, ERM (introduced in 1999) achieves highly competitive results, surpassing even recent state-of-the-art DG solutions in enhancing the generalisability of deep learning models across different geographic regions. Our dataset and code are available at https://github.com/RWGAI/DSGR.},
  langid = {english},
  keywords = {Computer vision,Distribution shift,Domain Generalisation,Domain Shift,Image classification,Land Use Classification,Out-of-Distribution Generalisation,Remote Sensing},
  file = {C:\Users\E097600\Zotero\storage\RQJUHSUT\Al-Emadi et al. - 2025 - Analysing Satellite Imagery Classification under Spatial Domain Shift across Geographic Regions.pdf}
}

@inproceedings{alcantarillaKAZEFeatures2012,
  title = {{{KAZE Features}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2012},
  author = {Alcantarilla, Pablo Fern{\'a}ndez and Bartoli, Adrien and Davison, Andrew J.},
  editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  year = 2012,
  pages = {214--227},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-33783-3_16},
  abstract = {In this paper, we introduce KAZE features, a novel multiscale 2D feature detection and description algorithm in nonlinear scale spaces. Previous approaches detect and describe features at different scale levels by building or approximating the Gaussian scale space of an image. However, Gaussian blurring does not respect the natural boundaries of objects and smoothes to the same degree both details and noise, reducing localization accuracy and distinctiveness. In contrast, we detect and describe 2D features in a nonlinear scale space by means of nonlinear diffusion filtering. In this way, we can make blurring locally adaptive to the image data, reducing noise but retaining object boundaries, obtaining superior localization accuracy and distinctiviness. The nonlinear scale space is built using efficient Additive Operator Splitting (AOS) techniques and variable conductance diffusion. We present an extensive evaluation on benchmark datasets and a practical matching application on deformable surfaces. Even though our features are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space, but comparable to SIFT, our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods.},
  isbn = {978-3-642-33783-3},
  langid = {english},
  keywords = {Feature Detection,Nonlinear Diffusion,Scale Invariant Feature Transform,Scale Level,Scale Space},
  file = {C:\Users\E097600\Zotero\storage\8KTN76TB\Alcantarilla et al. - 2012 - KAZE Features.pdf}
}

@article{aliExplainableArtificialIntelligence2023,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{What}} We Know and What Is Left to Attain {{Trustworthy Artificial Intelligence}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Ali, Sajid and Abuhmed, Tamer and {El-Sappagh}, Shaker and Muhammad, Khan and {Alonso-Moral}, Jose M. and Confalonieri, Roberto and Guidotti, Riccardo and Del Ser, Javier and {D{\'i}az-Rodr{\'i}guez}, Natalia and Herrera, Francisco},
  year = 2023,
  month = nov,
  journal = {Information Fusion},
  volume = {99},
  pages = {101805},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2023.101805},
  urldate = {2024-06-04},
  abstract = {Artificial intelligence (AI) is currently being utilized in a wide range of sophisticated applications, but the outcomes of many AI models are challenging to comprehend and trust due to their black-box nature. Usually, it is essential to understand the reasoning behind an AI model's decision-making. Thus, the need for eXplainable AI (XAI) methods for improving trust in AI models has arisen. XAI has become a popular research subject within the AI field in recent years. Existing survey papers have tackled the concepts of XAI, its general terms, and post-hoc explainability methods but there have not been any reviews that have looked at the assessment methods, available tools, XAI datasets, and other related aspects. Therefore, in this comprehensive study, we provide readers with an overview of the current research and trends in this rapidly emerging area with a case study example. The study starts by explaining the background of XAI, common definitions, and summarizing recently proposed techniques in XAI for supervised machine learning. The review divides XAI techniques into four axes using a hierarchical categorization system: (i) data explainability, (ii) model explainability, (iii) post-hoc explainability, and (iv) assessment of explanations. We also introduce available evaluation metrics as well as open-source packages and datasets with future research directions. Then, the significance of explainability in terms of legal demands, user viewpoints, and application orientation is outlined, termed as XAI concerns. This paper advocates for tailoring explanation content to specific user types. An examination of XAI techniques and evaluation was conducted by looking at 410 critical articles, published between January 2016 and October 2022, in reputed journals and using a wide range of research databases as a source of information. The article is aimed at XAI researchers who are interested in making their AI models more trustworthy, as well as towards researchers from other disciplines who are looking for effective XAI methods to complete tasks with confidence while communicating meaning from data.},
  keywords = {AI principles,Data Fusion,Deep learning,Explainable Artificial Intelligence,Interpretable machine learning,Post-hoc explainability,Trustworthy AI,XAI assessment},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\BVLZ3QAT\\Ali et al. - 2023 - Explainable Artificial Intelligence (XAI) What we.pdf;C\:\\Users\\E097600\\Zotero\\storage\\IARQ58R6\\S1566253523001148.html}
}

@misc{andeolConfidentObjectDetection2023,
  title = {Confident {{Object Detection}} via {{Conformal Prediction}} and {{Conformal Risk Control}}: An {{Application}} to {{Railway Signaling}}},
  shorttitle = {Confident {{Object Detection}} via {{Conformal Prediction}} and {{Conformal Risk Control}}},
  author = {And{\'e}ol, L{\'e}o and Fel, Thomas and De Grancey, Florence and Mossina, Luca},
  year = 2023,
  month = apr,
  journal = {arXiv.org},
  urldate = {2025-06-02},
  abstract = {Deploying deep learning models in real-world certified systems requires the ability to provide confidence estimates that accurately reflect their uncertainty. In this paper, we demonstrate the use of the conformal prediction framework to construct reliable and trustworthy predictors for detecting railway signals. Our approach is based on a novel dataset that includes images taken from the perspective of a train operator and state-of-the-art object detectors. We test several conformal approaches and introduce a new method based on conformal risk control. Our findings demonstrate the potential of the conformal prediction framework to evaluate model performance and provide practical guidance for achieving formally guaranteed uncertainty bounds.},
  howpublished = {https://arxiv.org/abs/2304.06052v2},
  langid = {english},
  keywords = {Conformal prediction,Object detection},
  file = {C:\Users\E097600\Zotero\storage\JZMDHXPI\Andéol et al. - 2023 - Confident Object Detection via Conformal Prediction and Conformal Risk Control an Application to Ra.pdf}
}

@misc{andeolConformalPredictionTrustworthy2023,
  title = {Conformal {{Prediction}} for {{Trustworthy Detection}} of {{Railway Signals}}},
  author = {And{\'e}ol, L{\'e}o and Fel, Thomas and Grancey, Florence De and Mossina, Luca},
  year = 2023,
  month = jan,
  number = {arXiv:2301.11136},
  eprint = {2301.11136},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.11136},
  urldate = {2025-06-02},
  abstract = {We present an application of conformal prediction, a form of uncertainty quantification with guarantees, to the detection of railway signals. State-of-the-art architectures are tested and the most promising one undergoes the process of conformalization, where a correction is applied to the predicted bounding boxes (i.e. to their height and width) such that they comply with a predefined probability of success. We work with a novel exploratory dataset of images taken from the perspective of a train operator, as a first step to build and validate future trustworthy machine learning models for the detection of railway signals.},
  archiveprefix = {arXiv},
  keywords = {Conformal prediction,Object detection,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\399TKTZU\\Andéol et al. - 2023 - Conformal Prediction for Trustworthy Detection of Railway Signals.pdf;C\:\\Users\\E097600\\Zotero\\storage\\2CRFUDXW\\2301.html}
}

@misc{angelopoulosConformalRiskControl2025,
  title = {Conformal {{Risk Control}}},
  author = {Angelopoulos, Anastasios N. and Bates, Stephen and Fisch, Adam and Lei, Lihua and Schuster, Tal},
  year = 2025,
  month = jun,
  number = {arXiv:2208.02814},
  eprint = {2208.02814},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.02814},
  urldate = {2025-10-31},
  abstract = {We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an \$\textbackslash mathcal\textbraceleft O\textbraceright (1/n)\$ factor. We also introduce extensions of the idea to distribution shift, quantile risk control, multiple and adversarial risk control, and expectations of U-statistics. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Conformal prediction,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology,Statistics - Statistics Theory},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\5BG78XPF\\Angelopoulos et al. - 2025 - Conformal Risk Control.pdf;C\:\\Users\\E097600\\Zotero\\storage\\UMVL5XZN\\2208.html}
}

@misc{angelopoulosGentleIntroductionConformal2022,
  title = {A {{Gentle Introduction}} to {{Conformal Prediction}} and {{Distribution-Free Uncertainty Quantification}}},
  author = {Angelopoulos, Anastasios N. and Bates, Stephen},
  year = 2022,
  month = dec,
  number = {arXiv:2107.07511},
  eprint = {2107.07511},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.07511},
  urldate = {2024-06-04},
  abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed to provide the reader a working understanding of conformal prediction and related distribution-free uncertainty quantification techniques with one self-contained document. We lead the reader through practical theory for and examples of conformal prediction and describe its extensions to complex machine learning tasks involving structured outputs, distribution shift, time-series, outliers, models that abstain, and more. Throughout, there are many explanatory illustrations, examples, and code samples in Python. With each code sample comes a Jupyter notebook implementing the method on a real-data example; the notebooks can be accessed and easily run using our codebase.},
  archiveprefix = {arXiv},
  keywords = {101,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Conformal prediction,domain adaptation,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\3ZV7WHKD\\Angelopoulos et Bates - 2022 - A Gentle Introduction to Conformal Prediction and .pdf;C\:\\Users\\E097600\\Zotero\\storage\\WQX8FWQQ\\2107.html}
}

@article{angelopoulosLearnThenTest2025,
  title = {Learn Then Test: {{Calibrating}} Predictive Algorithms to Achieve Risk Control},
  shorttitle = {Learn Then Test},
  author = {Angelopoulos, Anastasios N. and Bates, Stephen and Cand{\`e}s, Emmanuel J. and Jordan, Michael I. and Lei, Lihua},
  year = 2025,
  month = jun,
  journal = {The Annals of Applied Statistics},
  volume = {19},
  number = {2},
  pages = {1641--1662},
  publisher = {Institute of Mathematical Statistics},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/24-AOAS1998},
  urldate = {2025-10-21},
  abstract = {We introduce a framework for calibrating machine learning models to satisfy finite-sample statistical guarantees. Our calibration algorithms work with any model and (unknown) data-generating distribution and do not require retraining. The algorithms address, among other examples, false discovery rate control in multilabel classification, intersection-over-union control in instance segmentation, and simultaneous control of the type-1 outlier error and confidence set coverage in classification or regression. Our main insight is to reframe risk control as multiple hypothesis testing, enabling different mathematical arguments. We demonstrate our algorithms with detailed worked examples in computer vision and tabular medical data. The computer vision experiments demonstrate the utility of our approach in calibrating state-of-the-art predictive architectures that have been deployed widely, such as the detectron2 object detection system.},
  keywords = {Computer vision,Conformal prediction,deep learning,Image classification,Instance segmentation,machine learning},
  file = {C:\Users\E097600\Zotero\storage\TDFCKGU3\Angelopoulos et al. - 2025 - Learn then test Calibrating predictive algorithms to achieve risk control.pdf}
}

@misc{angelopoulosTheoreticalFoundationsConformal2024,
  title = {Theoretical {{Foundations}} of {{Conformal Prediction}}},
  author = {Angelopoulos, Anastasios N. and Barber, Rina Foygel and Bates, Stephen},
  year = 2024,
  month = nov,
  number = {arXiv:2411.11824},
  eprint = {2411.11824},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.11824},
  urldate = {2024-12-28},
  abstract = {This book is about conformal prediction and related inferential techniques that build on permutation tests and exchangeability. These techniques are useful in a diverse array of tasks, including hypothesis testing and providing uncertainty quantification guarantees for machine learning systems. Much of the current interest in conformal prediction is due to its ability to integrate into complex machine learning workflows, solving the problem of forming prediction sets without any assumptions on the form of the data generating distribution. Since contemporary machine learning algorithms have generally proven difficult to analyze directly, conformal prediction's main appeal is its ability to provide formal, finite-sample guarantees when paired with such methods. The goal of this book is to teach the reader about the fundamental technical arguments that arise when researching conformal prediction and related questions in distribution-free inference. Many of these proof strategies, especially the more recent ones, are scattered among research papers, making it difficult for researchers to understand where to look, which results are important, and how exactly the proofs work. We hope to bridge this gap by curating what we believe to be some of the most important results in the literature and presenting their proofs in a unified language, with illustrations, and with an eye towards pedagogy.},
  archiveprefix = {arXiv},
  keywords = {Conformal prediction,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology,Statistics - Statistics Theory},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\RMGIG9JK\\Angelopoulos et al. - 2024 - Theoretical Foundations of Conformal Prediction.pdf;C\:\\Users\\E097600\\Zotero\\storage\\X6BLIJK6\\2411.html}
}

@misc{angelopoulosUncertaintySetsImage2022,
  title = {Uncertainty {{Sets}} for {{Image Classifiers}} Using {{Conformal Prediction}}},
  author = {Angelopoulos, Anastasios and Bates, Stephen and Malik, Jitendra and Jordan, Michael I.},
  year = 2022,
  month = sep,
  number = {arXiv:2009.14193},
  eprint = {2009.14193},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.14193},
  urldate = {2025-01-14},
  abstract = {Convolutional image classifiers can achieve high predictive accuracy, but quantifying their uncertainty remains an unresolved challenge, hindering their deployment in consequential settings. Existing uncertainty quantification techniques, such as Platt scaling, attempt to calibrate the network's probability estimates, but they do not have formal guarantees. We present an algorithm that modifies any classifier to output a predictive set containing the true label with a user-specified probability, such as 90\%. The algorithm is simple and fast like Platt scaling, but provides a formal finite-sample coverage guarantee for every model and dataset. Our method modifies an existing conformal prediction algorithm to give more stable predictive sets by regularizing the small scores of unlikely classes after Platt scaling. In experiments on both Imagenet and Imagenet-V2 with ResNet-152 and other classifiers, our scheme outperforms existing approaches, achieving coverage with sets that are often factors of 5 to 10 smaller than a stand-alone Platt scaling baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Conformal prediction,Image classification,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\53IMPSEP\\Angelopoulos et al. - 2022 - Uncertainty Sets for Image Classifiers using Conformal Prediction.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ZFELPUK6\\2009.html}
}

@misc{anupPotato_diseaseDataset2024,
  title = {Potato\_disease {{Dataset}}},
  author = {Anup},
  year = 2024,
  month = jun,
  publisher = {Roboflow Universe},
  urldate = {2025-02-05},
  abstract = {6711 open source potato-diseases images plus a pre-trained potato\_disease model and API. Created by Anup},
  file = {C:\Users\E097600\Zotero\storage\XYZMEPQA\potato_disease-binb3.html}
}

@article{arnabConditionalRandomFields2018,
  title = {Conditional {{Random Fields Meet Deep Neural Networks}} for {{Semantic Segmentation}}: {{Combining Probabilistic Graphical Models}} with {{Deep Learning}} for {{Structured Prediction}}},
  shorttitle = {Conditional {{Random Fields Meet Deep Neural Networks}} for {{Semantic Segmentation}}},
  author = {Arnab, Anurag and Zheng, Shuai and Jayasumana, Sadeep and {Romera-Paredes}, Bernardino and Larsson, M{\aa}ns and Kirillov, Alexander and Savchynskyy, Bogdan and Rother, Carsten and Kahl, Fredrik and Torr, Philip H.S.},
  year = 2018,
  month = jan,
  journal = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {1},
  pages = {37--52},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2762355},
  urldate = {2025-03-24},
  abstract = {Semantic segmentation is the task of labeling every pixel in an image with a predefined object category. It has numerous applications in scenarios where the detailed understanding of an image is required, such as in autonomous vehicles and medical diagnosis. This problem has traditionally been solved with probabilistic models known as conditional random fields (CRFs) due to their ability to model the relationships between the pixels being predicted. However, deep neural networks (DNNs) recently have been shown to excel at a wide range of computer vision problems due to their ability to automatically learn rich feature representations from data, as opposed to traditional handcrafted features. The idea of combining CRFs and DNNs have achieved state-of-the-art results in a number of domains. We review the literature on combining the modeling power of CRFs with the representation-learning ability of DNNs, ranging from early work that combines these two techniques as independent stages of a common pipeline to recent approaches that embed inference of probabilistic models directly in the neural network itself. Finally, we summarize future research directions.},
  keywords = {Computational modeling,Computer vision,Feature extraction,Image segmentation,Semantics,Uncertainty,Visualization},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\VAZX98AZ\\Arnab et al. - 2018 - Conditional Random Fields Meet Deep Neural Networks for Semantic Segmentation Combining Probabilist.pdf;C\:\\Users\\E097600\\Zotero\\storage\\3U8Y6748\\8254255.html;C\:\\Users\\E097600\\Zotero\\storage\\U6L3MVJB\\8254255.html}
}

@misc{ashDeepBatchActive2020,
  title = {Deep {{Batch Active Learning}} by {{Diverse}}, {{Uncertain Gradient Lower Bounds}}},
  author = {Ash, Jordan T. and Zhang, Chicheng and Krishnamurthy, Akshay and Langford, John and Agarwal, Alekh},
  year = 2020,
  month = feb,
  number = {arXiv:1906.03671},
  eprint = {1906.03671},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.03671},
  urldate = {2024-03-08},
  abstract = {We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high-magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between diversity and uncertainty without requiring any hand-tuned hyperparameters. We show that while other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a versatile option for practical active learning problems.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,dpp,Image classification,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\GISIVSNL\\Ash et al. - 2020 - Deep Batch Active Learning by Diverse, Uncertain G.pdf;C\:\\Users\\E097600\\Zotero\\storage\\TLATP9TZ\\1906.html}
}

@misc{ashouritaklimiActiveLearningTaskDriven2025,
  title = {Active {{Learning}} with {{Task-Driven Representations}} for {{Messy Pools}}},
  author = {Ashouritaklimi, Kianoosh and Rainforth, Tom},
  year = 2025,
  month = oct,
  number = {arXiv:2510.25926},
  eprint = {2510.25926},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.25926},
  urldate = {2025-10-31},
  abstract = {Active learning has the potential to be especially useful for messy, uncurated pools where datapoints vary in relevance to the target task. However, state-of-the-art approaches to this problem currently rely on using fixed, unsupervised representations of the pool, focusing on modifying the acquisition function instead. We show that this model setup can undermine their effectiveness at dealing with messy pools, as such representations can fail to capture important information relevant to the task. To address this, we propose using task-driven representations that are periodically updated during the active learning process using the previously collected labels. We introduce two specific strategies for learning these representations, one based on directly learning semi-supervised representations and the other based on supervised fine-tuning of an initial unsupervised representation. We find that both significantly improve empirical performance over using unsupervised or pretrained representations.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,Image classification},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\U672M9PD\\Ashouritaklimi et Rainforth - 2025 - Active Learning with Task-Driven Representations for Messy Pools.pdf;C\:\\Users\\E097600\\Zotero\\storage\\BJEPWJMB\\2510.html}
}

@inproceedings{assranSelfSupervisedLearningImages2023,
  title = {Self-{{Supervised Learning From Images With}} a {{Joint-Embedding Predictive Architecture}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Assran, Mahmoud and Duval, Quentin and Misra, Ishan and Bojanowski, Piotr and Vincent, Pascal and Rabbat, Michael and LeCun, Yann and Ballas, Nicolas},
  year = 2023,
  pages = {15619--15629},
  publisher = {arXiv},
  urldate = {2025-08-28},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,ssl,Vision transformer},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\7BL9LE8I\\Assran et al. - 2023 - Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.pdf;C\:\\Users\\E097600\\Zotero\\storage\\FZJZMT5J\\Assran et al. - 2023 - Self-Supervised Learning From Images With a Joint-Embedding Predictive Architecture.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ZMV7NZIN\\2301.html}
}

@misc{azadMedicalImageSegmentation2022,
  title = {Medical {{Image Segmentation Review}}: {{The}} Success of {{U-Net}}},
  shorttitle = {Medical {{Image Segmentation Review}}},
  author = {Azad, Reza and Aghdam, Ehsan Khodapanah and Rauland, Amelie and Jia, Yiwei and Avval, Atlas Haddadi and Bozorgpour, Afshin and Karimijafarbigloo, Sanaz and Cohen, Joseph Paul and Adeli, Ehsan and Merhof, Dorit},
  year = 2022,
  month = nov,
  number = {arXiv:2211.14830},
  eprint = {2211.14830},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.14830},
  urldate = {2025-01-24},
  abstract = {Automatic medical image segmentation is a crucial topic in the medical domain and successively a critical counterpart in the computer-aided diagnosis paradigm. U-Net is the most widespread image segmentation architecture due to its flexibility, optimized modular design, and success in all medical image modalities. Over the years, the U-Net model achieved tremendous attention from academic and industrial researchers. Several extensions of this network have been proposed to address the scale and complexity created by medical tasks. Addressing the deficiency of the naive U-Net model is the foremost step for vendors to utilize the proper U-Net variant model for their business. Having a compendium of different variants in one place makes it easier for builders to identify the relevant research. Also, for ML researchers it will help them understand the challenges of the biological tasks that challenge the model. To address this, we discuss the practical aspects of the U-Net model and suggest a taxonomy to categorize each network variant. Moreover, to measure the performance of these strategies in a clinical application, we propose fair evaluations of some unique and famous designs on well-known datasets. We provide a comprehensive implementation library with trained models for future research. In addition, for ease of future studies, we created an online list of U-Net papers with their possible official implementation. All information is gathered in https://github.com/NITR098/Awesome-U-Net repository.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\8URG8F5R\\Azad et al. - 2022 - Medical Image Segmentation Review The success of U-Net.pdf;C\:\\Users\\E097600\\Zotero\\storage\\YMRA37EN\\2211.html}
}

@article{azizianRegularizationWassersteinDistributionally2023,
  title = {Regularization for {{Wasserstein}} Distributionally Robust Optimization},
  author = {Azizian, Wa{\"i}ss and Iutzeler, Franck and Malick, J{\'e}r{\^o}me},
  year = 2023,
  journal = {ESAIM: Control, Optimisation and Calculus of Variations},
  volume = {29},
  pages = {33},
  issn = {1292-8119, 1262-3377},
  doi = {10.1051/cocv/2023019},
  urldate = {2024-06-28},
  abstract = {Optimal transport has recently proved to be a useful tool in various machine learning applications needing comparisons of probability measures. Among these, applications of distributionally robust optimization naturally involve Wasserstein distances in their models of uncertainty, capturing data shifts or worst-case scenarios. Inspired by the success of the regularization of Wasserstein distances in optimal transport, we study in this paper the regularization of Wasserstein distributionally robust optimization. First, we derive a general strong duality result of regularized Wasserstein distributionally robust problems. Second, we refine this duality result in the case of entropic regularization and provide an approximation result when the regularization parameters vanish.},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  file = {C:\Users\E097600\Zotero\storage\Y5GGAFLY\Azizian et al. - 2023 - Regularization for Wasserstein distributionally ro.pdf}
}

@misc{badrinarayananSegNetDeepConvolutional2015,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder-Decoder Architecture}} for {{Robust Semantic Pixel-Wise Labelling}}},
  shorttitle = {{{SegNet}}},
  author = {Badrinarayanan, Vijay and Handa, Ankur and Cipolla, Roberto},
  year = 2015,
  month = may,
  number = {arXiv:1505.07293},
  eprint = {1505.07293},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1505.07293},
  urldate = {2025-02-11},
  abstract = {We propose a novel deep architecture, SegNet, for semantic pixel wise image labelling. SegNet has several attractive properties; (i) it only requires forward evaluation of a fully learnt function to obtain smooth label predictions, (ii) with increasing depth, a larger context is considered for pixel labelling which improves accuracy, and (iii) it is easy to visualise the effect of feature activation(s) in the pixel label space at any depth. SegNet is composed of a stack of encoders followed by a corresponding decoder stack which feeds into a soft-max classification layer. The decoders help map low resolution feature maps at the output of the encoder stack to full input image size feature maps. This addresses an important drawback of recent deep learning approaches which have adopted networks designed for object categorization for pixel wise labelling. These methods lack a mechanism to map deep layer feature maps to input dimensions. They resort to ad hoc methods to upsample features, e.g. by replication. This results in noisy predictions and also restricts the number of pooling layers in order to avoid too much upsampling and thus reduces spatial context. SegNet overcomes these problems by learning to map encoder outputs to image pixel labels. We test the performance of SegNet on outdoor RGB scenes from CamVid, KITTI and indoor scenes from the NYU dataset. Our results show that SegNet achieves state-of-the-art performance even without use of additional cues such as depth, video frames or post-processing with CRF models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\R8F9SABU\\Badrinarayanan et al. - 2015 - SegNet A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling.pdf;C\:\\Users\\E097600\\Zotero\\storage\\RBVW7HSC\\1505.html}
}

@misc{baeGeneralizedCoverageMore2024,
  title = {Generalized {{Coverage}} for {{More Robust Low-Budget Active Learning}}},
  author = {Bae, Wonho and Noh, Junhyug and Sutherland, Danica J.},
  year = 2024,
  month = jul,
  number = {arXiv:2407.12212},
  eprint = {2407.12212},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.12212},
  urldate = {2025-02-06},
  abstract = {The ProbCover method of Yehuda et al. is a well-motivated algorithm for active learning in low-budget regimes, which attempts to "cover" the data distribution with balls of a given radius at selected data points. We demonstrate, however, that the performance of this algorithm is extremely sensitive to the choice of this radius hyper-parameter, and that tuning it is quite difficult, with the original heuristic frequently failing. We thus introduce (and theoretically motivate) a generalized notion of "coverage," including ProbCover's objective as a special case, but also allowing smoother notions that are far more robust to hyper-parameter choice. We propose an efficient greedy method to optimize this coverage, generalizing ProbCover's algorithm; due to its close connection to kernel herding, we call it "MaxHerding." The objective can also be optimized non-greedily through a variant of \$k\$-medoids, clarifying the relationship to other low-budget active learning methods. In comprehensive experiments, MaxHerding surpasses existing active learning methods across multiple low-budget image classification benchmarks, and does so with less computational cost than most competitive methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\DPRSEEQT\\Bae et al. - 2024 - Generalized Coverage for More Robust Low-Budget Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\XRC8LASL\\2407.html}
}

@misc{baeUncertaintyHerdingOne2024,
  title = {Uncertainty {{Herding}}: {{One Active Learning Method}} for {{All Label Budgets}}},
  shorttitle = {Uncertainty {{Herding}}},
  author = {Bae, Wonho and Oliveira, Gabriel L. and Sutherland, Danica J.},
  year = 2024,
  month = dec,
  number = {arXiv:2412.20644},
  eprint = {2412.20644},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.20644},
  urldate = {2025-01-16},
  abstract = {Most active learning research has focused on methods which perform well when many labels are available, but can be dramatically worse than random selection when label budgets are small. Other methods have focused on the low-budget regime, but do poorly as label budgets increase. As the line between "low" and "high" budgets varies by problem, this is a serious issue in practice. We propose uncertainty coverage, an objective which generalizes a variety of low- and high-budget objectives, as well as natural, hyperparameter-light methods to smoothly interpolate between low- and high-budget regimes. We call greedy optimization of the estimate Uncertainty Herding; this simple method is computationally fast, and we prove that it nearly optimizes the distribution-level coverage. In experimental validation across a variety of active learning tasks, our proposal matches or beats state-of-the-art performance in essentially all cases; it is the only method of which we are aware that reliably works well in both low- and high-budget settings.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,hybrid,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\MFRVVG7X\\Bae et al. - 2024 - Uncertainty Herding One Active Learning Method for All Label Budgets.pdf;C\:\\Users\\E097600\\Zotero\\storage\\A5M7KSV5\\2412.html}
}

@incollection{balasubramanianConformalPredictionReliable2014,
  title = {Conformal {{Prediction}} for {{Reliable Machine Learning}}},
  booktitle = {Conformal {{Prediction}} for {{Reliable Machine Learning}}},
  editor = {Balasubramanian, Vineeth N. and Ho, Shen-Shyang and Vovk, Vladimir},
  year = 2014,
  month = jan,
  pages = {iii},
  publisher = {Morgan Kaufmann},
  address = {Boston},
  doi = {10.1016/B978-0-12-398537-8.00015-8},
  urldate = {2025-01-14},
  isbn = {978-0-12-398537-8},
  keywords = {basics,book,Conformal prediction},
  file = {C:\Users\E097600\Zotero\storage\MW5CWVLA\Conformal Prediction for Reliable Machine Learning. Theory, Adaptations and Applications (Vineeth Balasubramanian etc.) (Z-Library).pdf}
}

@misc{balestrieroCookbookSelfSupervisedLearning2023,
  title = {A {{Cookbook}} of {{Self-Supervised Learning}}},
  author = {Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann and Goldblum, Micah},
  year = 2023,
  month = jun,
  number = {arXiv:2304.12210},
  eprint = {2304.12210},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.12210},
  urldate = {2025-08-25},
  abstract = {Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,review,ssl},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\66BFNZH2\\Balestriero et al. - 2023 - A Cookbook of Self-Supervised Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\EM3G9Y7B\\2304.html}
}

@misc{balestrieroLeJEPAProvableScalable2025,
  title = {{{LeJEPA}}: {{Provable}} and {{Scalable Self-Supervised Learning Without}} the {{Heuristics}}},
  shorttitle = {{{LeJEPA}}},
  author = {Balestriero, Randall and LeCun, Yann},
  year = 2025,
  month = nov,
  number = {arXiv:2511.08544},
  eprint = {2511.08544},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2511.08544},
  urldate = {2025-11-27},
  abstract = {Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R\&D. We present a comprehensive theory of JEPAs and instantiate it in \textbraceleft\textbackslash bf LeJEPA\textbraceright, a lean, scalable, and theoretically grounded training objective. First, we identify the isotropic Gaussian as the optimal distribution that JEPAs' embeddings should follow to minimize downstream prediction risk. Second, we introduce a novel objective--\textbraceleft\textbackslash bf Sketched Isotropic Gaussian Regularization\textbraceright{} (SIGReg)--to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher-student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only \$\textbackslash approx\$50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79\textbackslash\% with a ViT-H/14. We hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (\textbackslash href\textbraceleft https://github.com/rbalestr-lab/lejepa\textbraceright\textbraceleft GitHub repo\textbraceright ).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\G69W7QKZ\\Balestriero et LeCun - 2025 - LeJEPA Provable and Scalable Self-Supervised Learning Without the Heuristics.pdf;C\:\\Users\\E097600\\Zotero\\storage\\E67IXBW7\\2511.html}
}

@misc{barberConformalPredictionExchangeability2023,
  title = {Conformal Prediction beyond Exchangeability},
  author = {Barber, Rina Foygel and Candes, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
  year = 2023,
  month = mar,
  number = {arXiv:2202.13415},
  eprint = {2202.13415},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.13415},
  urldate = {2025-01-07},
  abstract = {Conformal prediction is a popular, modern technique for providing valid predictive inference for arbitrary machine learning models. Its validity relies on the assumptions of exchangeability of the data, and symmetry of the given model fitting algorithm as a function of the data. However, exchangeability is often violated when predictive models are deployed in practice. For example, if the data distribution drifts over time, then the data points are no longer exchangeable; moreover, in such settings, we might want to use a nonsymmetric algorithm that treats recent observations as more relevant. This paper generalizes conformal prediction to deal with both aspects: we employ weighted quantiles to introduce robustness against distribution drift, and design a new randomization technique to allow for algorithms that do not treat data points symmetrically. Our new methods are provably robust, with substantially less loss of coverage when exchangeability is violated due to distribution drift or other challenging features of real data, while also achieving the same coverage guarantees as existing conformal prediction methods if the data points are in fact exchangeable. We demonstrate the practical utility of these new tools with simulations and real-data experiments on electricity and election forecasting.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\W3RN37Q2\\Barber et al. - 2023 - Conformal prediction beyond exchangeability.pdf;C\:\\Users\\E097600\\Zotero\\storage\\DZBISTYV\\2202.html}
}

@misc{barberLimitsDistributionfreeConditional2020,
  title = {The Limits of Distribution-Free Conditional Predictive Inference},
  author = {Barber, Rina Foygel and Cand{\`e}s, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
  year = 2020,
  month = apr,
  number = {arXiv:1903.04684},
  eprint = {1903.04684},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.04684},
  urldate = {2025-11-05},
  abstract = {We consider the problem of distribution-free predictive inference, with the goal of producing predictive coverage guarantees that hold conditionally rather than marginally. Existing methods such as conformal prediction offer marginal coverage guarantees, where predictive coverage holds on average over all possible test points, but this is not sufficient for many practical applications where we would like to know that our predictions are valid for a given individual, not merely on average over a population. On the other hand, exact conditional inference guarantees are known to be impossible without imposing assumptions on the underlying distribution. In this work we aim to explore the space in between these two, and examine what types of relaxations of the conditional coverage property would alleviate some of the practical concerns with marginal coverage guarantees while still being possible to achieve in a distribution-free setting.},
  archiveprefix = {arXiv},
  keywords = {Conformal prediction,domain adaptation,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\DHJ2G6AU\\Barber et al. - 2020 - The limits of distribution-free conditional predictive inference.pdf;C\:\\Users\\E097600\\Zotero\\storage\\YHYFG83D\\1903.html}
}

@misc{barberPredictiveInferenceJackknife2020,
  title = {Predictive Inference with the Jackknife+},
  author = {Barber, Rina Foygel and Candes, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
  year = 2020,
  month = may,
  number = {arXiv:1905.02928},
  eprint = {1905.02928},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.02928},
  urldate = {2025-11-26},
  abstract = {This paper introduces the jackknife+, which is a novel method for constructing predictive confidence intervals. Whereas the jackknife outputs an interval centered at the predicted response of a test point, with the width of the interval determined by the quantiles of leave-one-out residuals, the jackknife+ also uses the leave-one-out predictions at the test point to account for the variability in the fitted regression function. Assuming exchangeable training samples, we prove that this crucial modification permits rigorous coverage guarantees regardless of the distribution of the data points, for any algorithm that treats the training points symmetrically. Such guarantees are not possible for the original jackknife and we demonstrate examples where the coverage rate may actually vanish. Our theoretical and empirical analysis reveals that the jackknife and the jackknife+ intervals achieve nearly exact coverage and have similar lengths whenever the fitting algorithm obeys some form of stability. Further, we extend the jackknife+ to K-fold cross validation and similarly establish rigorous coverage properties. Our methods are related to cross-conformal prediction proposed by Vovk [2015] and we discuss connections.},
  archiveprefix = {arXiv},
  keywords = {Conformal prediction,Statistics - Methodology},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\T8ILVZIQ\\Barber et al. - 2020 - Predictive inference with the jackknife+.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ZRWSYSNP\\1905.html}
}

@article{batesTestingOutliersConformal2023,
  title = {Testing for {{Outliers}} with {{Conformal}} P-Values},
  author = {Bates, Stephen and Cand{\`e}s, Emmanuel and Lei, Lihua and Romano, Yaniv and Sesia, Matteo},
  year = 2023,
  month = feb,
  journal = {The Annals of Statistics},
  volume = {51},
  number = {1},
  eprint = {2104.08279},
  primaryclass = {math, stat},
  issn = {0090-5364},
  doi = {10.1214/22-AOS2244},
  urldate = {2024-06-28},
  abstract = {This paper studies the construction of p-values for nonparametric outlier detection, taking a multiple-testing perspective. The goal is to test whether new independent samples belong to the same distribution as a reference data set or are outliers. We propose a solution based on conformal inference, a broadly applicable framework which yields p-values that are marginally valid but mutually dependent for different test points. We prove these p-values are positively dependent and enable exact false discovery rate control, although in a relatively weak marginal sense. We then introduce a new method to compute p-values that are both valid conditionally on the training data and independent of each other for different test points; this paves the way to stronger type-I error guarantees. Our results depart from classical conformal inference as we leverage concentration inequalities rather than combinatorial arguments to establish our finite-sample guarantees. Furthermore, our techniques also yield a uniform confidence bound for the false positive rate of any outlier detection algorithm, as a function of the threshold applied to its raw statistics. Finally, the relevance of our results is demonstrated by numerical experiments on real and simulated data.},
  archiveprefix = {arXiv},
  keywords = {Conformal prediction,domain adaptation,Mathematics - Statistics Theory,Out-of-Distribution Detection,Statistics - Machine Learning,Statistics - Methodology,Statistics - Statistics Theory},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\CABTWXMQ\\Bates et al. - 2023 - Testing for Outliers with Conformal p-values.pdf;C\:\\Users\\E097600\\Zotero\\storage\\TPZSMY3I\\2104.html;C\:\\Users\\E097600\\Zotero\\storage\\W37SKUZA\\2104.html}
}

@misc{beliasPerformanceSmellsML2025,
  title = {Performance {{Smells}} in {{ML}} and {{Non-ML Python Projects}}: {{A Comparative Study}}},
  shorttitle = {Performance {{Smells}} in {{ML}} and {{Non-ML Python Projects}}},
  author = {Belias, Fran{\c c}ois and Silva, Leuson Da and Khomh, Foutse and Zid, Cyrine},
  year = 2025,
  month = apr,
  number = {arXiv:2504.20224},
  eprint = {2504.20224},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.20224},
  urldate = {2025-04-30},
  abstract = {Python is widely adopted across various domains, especially in Machine Learning (ML) and traditional software projects. Despite its versatility, Python is susceptible to performance smells, i.e., suboptimal coding practices that can reduce application efficiency. This study provides a comparative analysis of performance smells between ML and non-ML projects, aiming to assess the occurrence of these inefficiencies while exploring their distribution across stages in the ML pipeline. For that, we conducted an empirical study analyzing 300 Python-based GitHub projects, distributed across ML and non-ML projects, categorizing performance smells based on the RIdiom tool. Our results indicate that ML projects are more susceptible to performance smells likely due to the computational and data-intensive nature of ML workflows. We also observed that performance smells in the ML pipeline predominantly affect the Data Processing stage. However, their presence in the Model Deployment stage indicates that such smells are not limited to the early stages of the pipeline. Our findings offer actionable insights for developers, emphasizing the importance of targeted optimizations for smells prevalent in ML projects. Furthermore, our study underscores the need to tailor performance optimization strategies to the unique characteristics of ML projects, with particular attention to the pipeline stages most affected by performance smells.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\8F5DV4G9\\Belias et al. - 2025 - Performance Smells in ML and Non-ML Python Projects A Comparative Study.pdf;C\:\\Users\\E097600\\Zotero\\storage\\T5R8B759\\2504.html}
}

@book{bellmanDynamicProgramming2003,
  title = {Dynamic {{Programming}}},
  author = {Bellman, Richard Ernest},
  year = 2003,
  month = jan,
  publisher = {Courier Corporation},
  abstract = {An introduction to the mathematical theory of multistage decision processes, this text takes a \&quot;functional equation\&quot; approach to the discovery of optimum policies. Written by a leading developer of such policies, it presents a series of methods, uniqueness and existence theorems, and examples for solving the relevant equations. The text examines existence and uniqueness theorems, the optimal inventory equation, bottleneck problems in multistage production processes, a new formalism in the calculus of variation, strategies behind multistage games, and Markovian decision processes. Each chapter concludes with a problem set that Eric V. Denardo of Yale University, in his informative new introduction, calls \&quot;a rich lode of applications and research topics.\&quot; 1957 edition. 37 figures.},
  googlebooks = {fyVtp3EMxasC},
  isbn = {978-0-486-42809-3},
  langid = {english},
  keywords = {book,Computers / Programming / General,Mathematics / Linear & Nonlinear Programming}
}

@misc{bengarClassBalancedActiveLearning2021,
  title = {Class-{{Balanced Active Learning}} for {{Image Classification}}},
  author = {Bengar, Javad Zolfaghari and van de Weijer, Joost and Fuentes, Laura Lopez and Raducanu, Bogdan},
  year = 2021,
  month = oct,
  number = {arXiv:2110.04543},
  eprint = {2110.04543},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.04543},
  urldate = {2025-09-18},
  abstract = {Active learning aims to reduce the labeling effort that is required to train algorithms by learning an acquisition function selecting the most relevant data for which a label should be requested from a large unlabeled data pool. Active learning is generally studied on balanced datasets where an equal amount of images per class is available. However, real-world datasets suffer from severe imbalanced classes, the so called long-tail distribution. We argue that this further complicates the active learning process, since the imbalanced data pool can result in suboptimal classifiers. To address this problem in the context of active learning, we proposed a general optimization framework that explicitly takes class-balancing into account. Results on three datasets showed that the method is general (it can be combined with most existing active learning algorithms) and can be effectively applied to boost the performance of both informative and representative-based active learning methods. In addition, we showed that also on balanced datasets our method generally results in a performance gain.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Class imbalance,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\RF3W7L6W\\Bengar et al. - 2021 - Class-Balanced Active Learning for Image Classification.pdf;C\:\\Users\\E097600\\Zotero\\storage\\KZXPFUP4\\2110.html}
}

@inproceedings{berjawiGeneralizableFusionArchitecture2025,
  title = {Towards a {{Generalizable Fusion Architecture}} for {{Multimodal Object Detection}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Berjawi, Jad and Dupas, Yoann and C{\'e}rin, Christophe},
  year = 2025,
  pages = {2192--2200},
  urldate = {2025-11-27},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\JW4RHELV\Berjawi et al. - 2025 - Towards a Generalizable Fusion Architecture for Multimodal Object Detection.pdf}
}

@misc{biyikBatchActiveLearning2019,
  title = {Batch {{Active Learning Using Determinantal Point Processes}}},
  author = {B{\i}y{\i}k, Erdem and Wang, Kenneth and Anari, Nima and Sadigh, Dorsa},
  year = 2019,
  month = jun,
  number = {arXiv:1906.07975},
  eprint = {1906.07975},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.07975},
  urldate = {2024-03-05},
  abstract = {Data collection and labeling is one of the main challenges in employing machine learning algorithms in a variety of real-world applications with limited data. While active learning methods attempt to tackle this issue by labeling only the data samples that give high information, they generally suffer from large computational costs and are impractical in settings where data can be collected in parallel. Batch active learning methods attempt to overcome this computational burden by querying batches of samples at a time. To avoid redundancy between samples, previous works rely on some ad hoc combination of sample quality and diversity. In this paper, we present a new principled batch active learning method using Determinantal Point Processes, a repulsive point process that enables generating diverse batches of samples. We develop tractable algorithms to approximate the mode of a DPP distribution, and provide theoretical guarantees on the degree of approximation. We further demonstrate that an iterative greedy method for DPP maximization, which has lower computational costs but worse theoretical guarantees, still gives competitive results for batch active learning. Our experiments show the value of our methods on several datasets against state-of-the-art baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\HA8MXFFJ\\Bıyık et al. - 2019 - Batch Active Learning Using Determinantal Point Pr.pdf;C\:\\Users\\E097600\\Zotero\\storage\\4Y8UZHIJ\\1906.html}
}

@misc{biyikBatchActiveLearning2024,
  title = {Batch {{Active Learning}} of {{Reward Functions}} from {{Human Preferences}}},
  author = {B{\i}y{\i}k, Erdem and Anari, Nima and Sadigh, Dorsa},
  year = 2024,
  month = feb,
  number = {arXiv:2402.15757},
  eprint = {2402.15757},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.15757},
  urldate = {2024-03-05},
  abstract = {Data generation and labeling are often expensive in robot learning. Preference-based learning is a concept that enables reliable labeling by querying users with preference questions. Active querying methods are commonly employed in preference-based learning to generate more informative data at the expense of parallelization and computation time. In this paper, we develop a set of novel algorithms, batch active preference-based learning methods, that enable efficient learning of reward functions using as few data samples as possible while still having short query generation times and also retaining parallelizability. We introduce a method based on determinantal point processes (DPP) for active batch generation and several heuristic-based alternatives. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. We showcase one of our algorithms in a study to learn human users' preferences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\TFYYN9YE\\Bıyık et al. - 2024 - Batch Active Learning of Reward Functions from Hum.pdf;C\:\\Users\\E097600\\Zotero\\storage\\RB7XFN2I\\2402.html}
}

@misc{blasiokUnifyingTheoryDistance2023,
  title = {A {{Unifying Theory}} of {{Distance}} from {{Calibration}}},
  author = {B{\l}asiok, Jaros{\l}aw and Gopalan, Parikshit and Hu, Lunjia and Nakkiran, Preetum},
  year = 2023,
  month = mar,
  number = {arXiv:2211.16886},
  eprint = {2211.16886},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.16886},
  urldate = {2025-02-07},
  abstract = {We study the fundamental question of how to define and measure the distance from calibration for probabilistic predictors. While the notion of perfect calibration is well-understood, there is no consensus on how to quantify the distance from perfect calibration. Numerous calibration measures have been proposed in the literature, but it is unclear how they compare to each other, and many popular measures such as Expected Calibration Error (ECE) fail to satisfy basic properties like continuity. We present a rigorous framework for analyzing calibration measures, inspired by the literature on property testing. We propose a ground-truth notion of distance from calibration: the \$\textbackslash ell\_1\$ distance to the nearest perfectly calibrated predictor. We define a consistent calibration measure as one that is polynomially related to this distance. Applying our framework, we identify three calibration measures that are consistent and can be estimated efficiently: smooth calibration, interval calibration, and Laplace kernel calibration. The former two give quadratic approximations to the ground truth distance, which we show is information-theoretically optimal in a natural model for measuring calibration which we term the prediction-only access model. Our work thus establishes fundamental lower and upper bounds on measuring the distance to calibration, and also provides theoretical justification for preferring certain metrics (like Laplace kernel calibration) in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\GHABQ4LC\\Błasiok et al. - 2023 - A Unifying Theory of Distance from Calibration.pdf;C\:\\Users\\E097600\\Zotero\\storage\\PEAR6N4J\\2211.html}
}

@misc{blasiokWhenDoesOptimizing2023,
  title = {When {{Does Optimizing}} a {{Proper Loss Yield Calibration}}?},
  author = {B{\l}asiok, Jaros{\l}aw and Gopalan, Parikshit and Hu, Lunjia and Nakkiran, Preetum},
  year = 2023,
  month = dec,
  number = {arXiv:2305.18764},
  eprint = {2305.18764},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.18764},
  urldate = {2025-06-06},
  abstract = {Optimizing proper loss functions is popularly believed to yield predictors with good calibration properties; the intuition being that for such losses, the global optimum is to predict the ground-truth probabilities, which is indeed calibrated. However, typical machine learning models are trained to approximately minimize loss over restricted families of predictors, that are unlikely to contain the ground truth. Under what circumstances does optimizing proper loss over a restricted family yield calibrated models? What precise calibration guarantees does it give? In this work, we provide a rigorous answer to these questions. We replace the global optimality with a local optimality condition stipulating that the (proper) loss of the predictor cannot be reduced much by post-processing its predictions with a certain family of Lipschitz functions. We show that any predictor with this local optimality satisfies smooth calibration as defined in Kakade-Foster (2008), B\textbraceleft\textbackslash l\textbraceright asiok et al. (2023). Local optimality is plausibly satisfied by well-trained DNNs, which suggests an explanation for why they are calibrated from proper loss minimization alone. Finally, we show that the connection between local optimality and calibration error goes both ways: nearly calibrated predictors are also nearly locally optimal.},
  archiveprefix = {arXiv},
  keywords = {Calibration,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\XWK8ZX7M\\Błasiok et al. - 2023 - When Does Optimizing a Proper Loss Yield Calibration.pdf;C\:\\Users\\E097600\\Zotero\\storage\\4JXTLPIA\\2305.html}
}

@article{blokActiveLearningMaskAL2022,
  title = {Active Learning with {{MaskAL}} Reduces Annotation Effort for Training {{Mask R-CNN}}},
  author = {Blok, Pieter M. and Kootstra, Gert and Elghor, Hakim Elchaoui and Diallo, Boubacar and van Evert, Frits K. and van Henten, Eldert J.},
  year = 2022,
  month = jun,
  journal = {Computers and Electronics in Agriculture},
  volume = {197},
  eprint = {2112.06586},
  primaryclass = {cs},
  pages = {106917},
  issn = {01681699},
  doi = {10.1016/j.compag.2022.106917},
  urldate = {2025-09-12},
  abstract = {The generalisation performance of a convolutional neural network (CNN) is influenced by the quantity, quality, and variety of the training images. Training images must be annotated, and this is time consuming and expensive. The goal of our work was to reduce the number of annotated images needed to train a CNN while maintaining its performance. We hypothesised that the performance of a CNN can be improved faster by ensuring that the set of training images contains a large fraction of hard-to-classify images. The objective of our study was to test this hypothesis with an active learning method that can automatically select the hard-to-classify images. We developed an active learning method for Mask Region-based CNN (Mask R-CNN) and named this method MaskAL. MaskAL involved the iterative training of Mask R-CNN, after which the trained model was used to select a set of unlabelled images about which the model was most uncertain. The selected images were then annotated and used to retrain Mask R-CNN, and this was repeated for a number of sampling iterations. In our study, MaskAL was compared to a random sampling method on a broccoli dataset with five visually similar classes. MaskAL performed significantly better than the random sampling. In addition, MaskAL had the same performance after sampling 900 images as the random sampling had after 2300 images. Compared to a Mask R-CNN model that was trained on the entire training set (14,000 images), MaskAL achieved 93.9\% of that model's performance with 17.9\% of its training data. The random sampling achieved 81.9\% of that model's performance with 16.4\% of its training data. We conclude that by using MaskAL, the annotation effort can be reduced for training Mask R-CNN on a broccoli dataset with visually similar classes. Our software is available on https://github.com/pieterblok/maskal.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Computer Vision and Pattern Recognition,Instance segmentation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\YWDML82F\\Blok et al. - 2022 - Active learning with MaskAL reduces annotation effort for training Mask R-CNN.pdf;C\:\\Users\\E097600\\Zotero\\storage\\MMSMX8Q7\\2112.html}
}

@inproceedings{brabecModelEvaluationNonconstant2020,
  title = {On {{Model Evaluation Under Non-constant Class Imbalance}}},
  booktitle = {Computational {{Science}} -- {{ICCS}} 2020},
  author = {Brabec, Jan and Kom{\'a}rek, Tom{\'a}{\v s} and Franc, Vojt{\v e}ch and Machlica, Luk{\'a}{\v s}},
  editor = {Krzhizhanovskaya, Valeria V. and Z{\'a}vodszky, G{\'a}bor and Lees, Michael H. and Dongarra, Jack J. and Sloot, Peter M. A. and Brissos, S{\'e}rgio and Teixeira, Jo{\~a}o},
  year = 2020,
  pages = {74--87},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-50423-6_6},
  abstract = {Many real-world classification problems are significantly class-imbalanced to detriment of the class of interest. The standard set of proper evaluation metrics is well-known but the usual assumption is that the test dataset imbalance equals the real-world imbalance. In practice, this assumption is often broken for various reasons. The reported results are then often too optimistic and may lead to wrong conclusions about industrial impact and suitability of proposed techniques. We introduce methods (Supplementary code related to techniques described in this paper is available at: https://github.com/CiscoCTA/nci\_eval) focusing on evaluation under non-constant class imbalance. We show that not only the absolute values of commonly used metrics, but even the order of classifiers in relation to the evaluation metric used is affected by the change of the imbalance rate. Finally, we demonstrate that using subsampling in order to get a test dataset with class imbalance equal to the one observed in the wild is not necessary, and eventually can lead to significant errors in classifier's performance estimate.},
  isbn = {978-3-030-50423-6},
  langid = {english},
  keywords = {Class imbalance,Evaluation metrics,Imbalanced data,Precision,ROC},
  file = {C:\Users\E097600\Zotero\storage\H3JIZFJJ\Brabec et al. - 2020 - On Model Evaluation Under Non-constant Class Imbalance.pdf}
}

@misc{brunekreefKandinskyConformalPrediction2023,
  title = {Kandinsky {{Conformal Prediction}}: {{Efficient Calibration}} of {{Image Segmentation Algorithms}}},
  shorttitle = {Kandinsky {{Conformal Prediction}}},
  author = {Brunekreef, Joren and Marcus, Eric and Sheombarsing, Ray and Sonke, Jan-Jakob and Teuwen, Jonas},
  year = 2023,
  month = nov,
  number = {arXiv:2311.11837},
  eprint = {2311.11837},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.11837},
  urldate = {2025-01-28},
  abstract = {Image segmentation algorithms can be understood as a collection of pixel classifiers, for which the outcomes of nearby pixels are correlated. Classifier models can be calibrated using Inductive Conformal Prediction, but this requires holding back a sufficiently large calibration dataset for computing the distribution of non-conformity scores of the model's predictions. If one only requires only marginal calibration on the image level, this calibration set consists of all individual pixels in the images available for calibration. However, if the goal is to attain proper calibration for each individual pixel classifier, the calibration set consists of individual images. In a scenario where data are scarce (such as the medical domain), it may not always be possible to set aside sufficiently many images for this pixel-level calibration. The method we propose, dubbed ``Kandinsky calibration'', makes use of the spatial structure present in the distribution of natural images to simultaneously calibrate the classifiers of ``similar'' pixels. This can be seen as an intermediate approach between marginal (imagewise) and conditional (pixelwise) calibration, where non-conformity scores are aggregated over similar image regions, thereby making more efficient use of the images available for calibration. We run experiments on segmentation algorithms trained and calibrated on subsets of the public MS-COCO and Medical Decathlon datasets, demonstrating that Kandinsky calibration method can significantly improve the coverage. When compared to both pixelwise and imagewise calibration on little data, the Kandinsky method achieves much lower coverage errors, indicating the data efficiency of the Kandinsky calibration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Conformal prediction,semantic segmentation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\7KYKBHC6\\Brunekreef et al. - 2023 - Kandinsky Conformal Prediction Efficient Calibration of Image Segmentation Algorithms.pdf;C\:\\Users\\E097600\\Zotero\\storage\\3ZAEMILG\\2311.html}
}

@article{budaSystematicStudyClass2018,
  title = {A Systematic Study of the Class Imbalance Problem in Convolutional Neural Networks},
  author = {Buda, Mateusz and Maki, Atsuto and Mazurowski, Maciej A.},
  year = 2018,
  month = oct,
  journal = {Neural Networks},
  volume = {106},
  eprint = {1710.05381},
  primaryclass = {cs},
  pages = {249--259},
  issn = {08936080},
  doi = {10.1016/j.neunet.2018.07.011},
  urldate = {2025-02-11},
  abstract = {In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve (ROC AUC) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that completely eliminates the imbalance, whereas the optimal undersampling ratio depends on the extent of imbalance; (iv) as opposed to some classical machine learning models, oversampling does not cause overfitting of CNNs; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\SA3UCB4B\\Buda et al. - 2018 - A systematic study of the class imbalance problem in convolutional neural networks.pdf;C\:\\Users\\E097600\\Zotero\\storage\\EF6AZQH5\\1710.html}
}

@article{cacciarelliStreambasedActiveLearning2022,
  title = {Stream-Based Active Learning with Linear Models},
  author = {Cacciarelli, Davide and Kulahci, Murat and Tyssedal, John S{\o}lve},
  year = 2022,
  month = oct,
  journal = {Knowledge-Based Systems},
  volume = {254},
  eprint = {2207.09874},
  primaryclass = {stat},
  pages = {109664},
  issn = {09507051},
  doi = {10.1016/j.knosys.2022.109664},
  urldate = {2024-12-23},
  abstract = {The proliferation of automated data collection schemes and the advances in sensorics are increasing the amount of data we are able to monitor in real-time. However, given the high annotation costs and the time required by quality inspections, data is often available in an unlabeled form. This is fostering the use of active learning for the development of soft sensors and predictive models. In production, instead of performing random inspections to obtain product information, labels are collected by evaluating the information content of the unlabeled data. Several query strategy frameworks for regression have been proposed in the literature but most of the focus has been dedicated to the static pool-based scenario. In this work, we propose a new strategy for the stream-based scenario, where instances are sequentially offered to the learner, which must instantaneously decide whether to perform the quality check to obtain the label or discard the instance. The approach is inspired by the optimal experimental design theory and the iterative aspect of the decision-making process is tackled by setting a threshold on the informativeness of the unlabeled data points. The proposed approach is evaluated using numerical simulations and the Tennessee Eastman Process simulator. The results confirm that selecting the examples suggested by the proposed algorithm allows for a faster reduction in the prediction error.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,stream based AL},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\K8T84WHN\\Cacciarelli et al. - 2022 - Stream-based active learning with linear models.pdf;C\:\\Users\\E097600\\Zotero\\storage\\6TCHVEYZ\\2207.html}
}

@article{caiExploringSpatialDiversity2021,
  title = {Exploring {{Spatial Diversity}} for {{Region-based Active Learning}}},
  author = {Cai, Lile and Xu, Xun and Zhang, Lining and Foo, Chuan-Sheng},
  year = 2021,
  journal = {IEEE Transactions on Image Processing},
  volume = {30},
  eprint = {2507.17367},
  primaryclass = {cs},
  pages = {8702--8712},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2021.3120041},
  urldate = {2025-07-24},
  abstract = {State-of-the-art methods for semantic segmentation are based on deep neural networks trained on large-scale labeled datasets. Acquiring such datasets would incur large annotation costs, especially for dense pixel-level prediction tasks like semantic segmentation. We consider region-based active learning as a strategy to reduce annotation costs while maintaining high performance. In this setting, batches of informative image regions instead of entire images are selected for labeling. Importantly, we propose that enforcing local spatial diversity is beneficial for active learning in this case, and to incorporate spatial diversity along with the traditional active selection criterion, e.g., data sample uncertainty, in a unified optimization framework for region-based active learning. We apply this framework to the Cityscapes and PASCAL VOC datasets and demonstrate that the inclusion of spatial diversity effectively improves the performance of uncertainty-based and feature diversity-based active learning methods. Our framework achieves \$95\textbackslash\%\$ performance of fully supervised methods with only \$5-9\textbackslash\%\$ of the labeled pixels, outperforming all state-of-the-art region-based active learning methods for semantic segmentation.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Computer Vision and Pattern Recognition,semantic segmentation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\2R7PAIIQ\\Cai et al. - 2021 - Exploring Spatial Diversity for Region-based Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ZZZ7TWUI\\2507.html}
}

@misc{caoBayesianActiveLearning2021,
  title = {Bayesian {{Active Learning}} by {{Disagreements}}: {{A Geometric Perspective}}},
  shorttitle = {Bayesian {{Active Learning}} by {{Disagreements}}},
  author = {Cao, Xiaofeng and Tsang, Ivor W.},
  year = 2021,
  month = may,
  number = {arXiv:2105.02543},
  eprint = {2105.02543},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.02543},
  urldate = {2024-03-14},
  abstract = {We present geometric Bayesian active learning by disagreements (GBALD), a framework that performs BALD on its core-set construction interacting with model uncertainty estimation. Technically, GBALD constructs core-set on ellipsoid, not typical sphere, preventing low-representative elements from spherical boundaries. The improvements are twofold: 1) relieve uninformative prior and 2) reduce redundant estimations. Theoretically, geodesic search with ellipsoid can derive tighter lower bound on error and easier to achieve zero error than with sphere. Experiments show that GBALD has slight perturbations to noisy and repeated samples, and outperforms BALD, BatchBALD and other existing deep active learning approaches.},
  archiveprefix = {arXiv},
  keywords = {Active learning,bayesian,Computer Science - Machine Learning,Image classification},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\I8MDQEFB\\Cao et Tsang - 2021 - Bayesian Active Learning by Disagreements A Geome.pdf;C\:\\Users\\E097600\\Zotero\\storage\\T7QYHAGU\\2105.html}
}

@article{caoDigitalEddyCurrent2024,
  title = {Digital {{Eddy Current Detection Method Based}} on {{High-Speed Sampling}} with {{STM32}}},
  author = {Cao, Xiong and Li, Erlong and Yuan, Zilan and Zheng, Kaituo},
  year = 2024,
  month = jun,
  journal = {Micromachines},
  volume = {15},
  pages = {775},
  doi = {10.3390/mi15060775},
  abstract = {The electromagnetic eddy current non-destructive testing system enables the non-destructive analysis of surface defect information on tested materials. Based on the principles of eddy current detection, this paper presents a digital eddy current detection method using high-speed sampling based on STM32. A differential eddy current coil is used as the detection probe, and the combination of a differential bridge and a differential amplifier circuit helps to reduce common-mode noise interference. The detection signal is collected via an STM32-based acquisition circuit and transmitted to the host computer through Ethernet for digital demodulation processing. The host computer performs operations such as smoothing averaging, sinusoidal fitting, and outlier removal to extract the amplitude and phase of the detection signal. The system also visually displays the condition of the tested object's surface in real time through graphical visualization. Testing showed that this system can operate at frequencies up to 8.84 MHz and clearly identify defects as narrow as 1 mm on the surface of the tested steel plate.},
  file = {C:\Users\E097600\Zotero\storage\KVALC75X\Cao et al. - 2024 - Digital Eddy Current Detection Method Based on High-Speed Sampling with STM32.pdf}
}

@phdthesis{carassouInferringPhotometricSize2017,
  title = {Inferring the Photometric and Size Evolution of Galaxies from Image Simulations},
  author = {Carassou, S{\'e}bastien},
  year = 2017,
  month = oct,
  urldate = {2024-03-04},
  abstract = {Current constraints on the luminosity and size evolution of galaxies rely on catalogs extracted from multi-band surveys. However resulting catalogs are altered by selection effects difficult to model and that can lead to conflicting predictions if not taken into account properly. In this thesis we have developed a new approach to infer robust constraints on model parameters. We use an empirical model to generate a set of mock galaxies from physical parameters. These galaxies are passed through an image simulator emulating the instrumental characteristics of any survey and extracted in the same way as from observed data for direct comparison. The difference between mock and observed data is minimized via a sampling process based on adaptive Monte Carlo Markov Chain methods. Using mock data matching most of the properties of a Canada-France-Hawaii Telescope Legacy Survey Deep (CFHTLS Deep) field, we demonstrate the robustness and internal consistency of our approach by inferring the size and luminosity functions and their evolution parameters for realistic populations of galaxies. We compare our results with those obtained from the classical spectral energy distribution (SED) fitting method, and find that our pipeline infers the model parameters using only 3 filters and more accurately than SED fitting based on the same observables. We then apply our pipeline to a fraction of a real CFHTLS Deep field to constrain the same set of parameters in a way that is free from systematic biases. Finally, we highlight the potential of this technique in the context of future surveys and discuss its drawbacks.},
  langid = {english},
  school = {Universit\'e Pierre et Marie Curie - Paris VI}
}

@article{careyTenSimpleRules2020,
  title = {Ten Simple Rules for Reading a Scientific Paper},
  author = {Carey, Maureen A. and Steiner, Kevin L. and Petri, William A.},
  year = 2020,
  month = jul,
  journal = {PLoS Computational Biology},
  volume = {16},
  number = {7},
  pages = {e1008032},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1008032},
  urldate = {2025-04-17},
  pmcid = {PMC7392212},
  pmid = {32730251},
  keywords = {bibliography,method,research},
  file = {C:\Users\E097600\Zotero\storage\E5HFW3PW\Carey et al. - 2020 - Ten simple rules for reading a scientific paper.pdf}
}

@misc{caronEmergingPropertiesSelfSupervised2021,
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  year = 2021,
  month = may,
  number = {arXiv:2104.14294},
  eprint = {2104.14294},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.14294},
  urldate = {2024-03-11},
  abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\NARRZ234\\Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf;C\:\\Users\\E097600\\Zotero\\storage\\A848UGIA\\2104.html}
}

@misc{caronEmergingPropertiesSelfSupervised2021a,
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  year = 2021,
  month = apr,
  journal = {arXiv.org},
  urldate = {2025-03-12},
  abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  howpublished = {https://arxiv.org/abs/2104.14294v2},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\IUZCSK4R\Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Transformers.pdf}
}

@misc{caronGenerativeApproachWikipediaScale2024,
  title = {A {{Generative Approach}} for {{Wikipedia-Scale Visual Entity Recognition}}},
  author = {Caron, Mathilde and Iscen, Ahmet and Fathi, Alireza and Schmid, Cordelia},
  year = 2024,
  month = mar,
  number = {arXiv:2403.02041},
  eprint = {2403.02041},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.02041},
  urldate = {2025-09-09},
  abstract = {In this paper, we address web-scale visual entity recognition, specifically the task of mapping a given query image to one of the 6 million existing entities in Wikipedia. One way of approaching a problem of such scale is using dual-encoder models (eg CLIP), where all the entity names and query images are embedded into a unified space, paving the way for an approximate k-NN search. Alternatively, it is also possible to re-purpose a captioning model to directly generate the entity names for a given image. In contrast, we introduce a novel Generative Entity Recognition (GER) framework, which given an input image learns to auto-regressively decode a semantic and discriminative ``code'' identifying the target entity. Our experiments demonstrate the efficacy of this GER paradigm, showcasing state-of-the-art performance on the challenging OVEN benchmark. GER surpasses strong captioning, dual-encoder, visual matching and hierarchical classification baselines, affirming its advantage in tackling the complexities of web-scale recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\6TQZP4UJ\\Caron et al. - 2024 - A Generative Approach for Wikipedia-Scale Visual Entity Recognition.pdf;C\:\\Users\\E097600\\Zotero\\storage\\BL5UQ6WG\\2403.html}
}

@misc{caronUnsupervisedLearningVisual2020,
  title = {Unsupervised {{Learning}} of {{Visual Features}} by {{Contrasting Cluster Assignments}}},
  author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  year = 2020,
  month = jun,
  journal = {arXiv.org},
  urldate = {2025-03-12},
  abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
  howpublished = {https://arxiv.org/abs/2006.09882v5},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\5ZEIBMWF\Caron et al. - 2020 - Unsupervised Learning of Visual Features by Contrasting Cluster Assignments.pdf}
}

@misc{caronWebScaleVisualEntity2024,
  title = {Web-{{Scale Visual Entity Recognition}}: {{An LLM-Driven Data Approach}}},
  shorttitle = {Web-{{Scale Visual Entity Recognition}}},
  author = {Caron, Mathilde and Fathi, Alireza and Schmid, Cordelia and Iscen, Ahmet},
  year = 2024,
  month = oct,
  number = {arXiv:2410.23676},
  eprint = {2410.23676},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.23676},
  urldate = {2025-09-09},
  abstract = {Web-scale visual entity recognition, the task of associating images with their corresponding entities within vast knowledge bases like Wikipedia, presents significant challenges due to the lack of clean, large-scale training data. In this paper, we propose a novel methodology to curate such a dataset, leveraging a multimodal large language model (LLM) for label verification, metadata generation, and rationale explanation. Instead of relying on the multimodal LLM to directly annotate data, which we found to be suboptimal, we prompt it to reason about potential candidate entity labels by accessing additional contextually relevant information (such as Wikipedia), resulting in more accurate annotations. We further use the multimodal LLM to enrich the dataset by generating question-answer pairs and a grounded finegrained textual description (referred to as "rationale") that explains the connection between images and their assigned entities. Experiments demonstrate that models trained on this automatically curated data achieve state-of-the-art performance on web-scale visual entity recognition tasks (e.g. +6.9\% improvement in OVEN entity task), underscoring the importance of high-quality training data in this domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\SU7DJYIQ\\Caron et al. - 2024 - Web-Scale Visual Entity Recognition An LLM-Driven Data Approach.pdf;C\:\\Users\\E097600\\Zotero\\storage\\YAVPAQLA\\2410.html}
}

@misc{cauchoisKnowingWhatYou2020,
  title = {Knowing What You Know: Valid and Validated Confidence Sets in Multiclass and Multilabel Prediction},
  shorttitle = {Knowing What You Know},
  author = {Cauchois, Maxime and Gupta, Suyash and Duchi, John},
  year = 2020,
  month = jul,
  number = {arXiv:2004.10181},
  eprint = {2004.10181},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.10181},
  urldate = {2025-11-05},
  abstract = {We develop conformal prediction methods for constructing valid predictive confidence sets in multiclass and multilabel problems without assumptions on the data generating distribution. A challenge here is that typical conformal prediction methods---which give marginal validity (coverage) guarantees---provide uneven coverage, in that they address easy examples at the expense of essentially ignoring difficult examples. By leveraging ideas from quantile regression, we build methods that always guarantee correct coverage but additionally provide (asymptotically optimal) conditional coverage for both multiclass and multilabel prediction problems. To address the potential challenge of exponentially large confidence sets in multilabel prediction, we build tree-structured classifiers that efficiently account for interactions between labels. Our methods can be bolted on top of any classification model---neural network, random forest, boosted tree---to guarantee its validity. We also provide an empirical evaluation, simultaneously providing new validation methods, that suggests the more robust coverage of our confidence sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Conformal prediction,domain adaptation,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\47QDSQP9\\Cauchois et al. - 2020 - Knowing what you know valid and validated confidence sets in multiclass and multilabel prediction.pdf;C\:\\Users\\E097600\\Zotero\\storage\\2U78LCF3\\2004.html}
}

@misc{cenDeepMetricLearning2021,
  title = {Deep {{Metric Learning}} for {{Open World Semantic Segmentation}}},
  author = {Cen, Jun and Yun, Peng and Cai, Junhao and Wang, Michael Yu and Liu, Ming},
  year = 2021,
  month = aug,
  number = {arXiv:2108.04562},
  eprint = {2108.04562},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.04562},
  urldate = {2024-03-27},
  abstract = {Classical close-set semantic segmentation networks have limited ability to detect out-of-distribution (OOD) objects, which is important for safety-critical applications such as autonomous driving. Incrementally learning these OOD objects with few annotations is an ideal way to enlarge the knowledge base of the deep learning models. In this paper, we propose an open world semantic segmentation system that includes two modules: (1) an open-set semantic segmentation module to detect both in-distribution and OOD objects. (2) an incremental few-shot learning module to gradually incorporate those OOD objects into its existing knowledge base. This open world semantic segmentation system behaves like a human being, which is able to identify OOD objects and gradually learn them with corresponding supervision. We adopt the Deep Metric Learning Network (DMLNet) with contrastive clustering to implement open-set semantic segmentation. Compared to other open-set semantic segmentation methods, our DMLNet achieves state-of-the-art performance on three challenging open-set semantic segmentation datasets without using additional data or generative models. On this basis, two incremental few-shot learning methods are further proposed to progressively improve the DMLNet with the annotations of OOD objects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\JYV46NHW\\Cen et al. - 2021 - Deep Metric Learning for Open World Semantic Segme.pdf;C\:\\Users\\E097600\\Zotero\\storage\\J83LB87V\\2108.html}
}

@misc{chenAnyDoorZeroshotObjectlevel2024,
  title = {{{AnyDoor}}: {{Zero-shot Object-level Image Customization}}},
  shorttitle = {{{AnyDoor}}},
  author = {Chen, Xi and Huang, Lianghua and Liu, Yu and Shen, Yujun and Zhao, Deli and Zhao, Hengshuang},
  year = 2024,
  month = may,
  number = {arXiv:2307.09481},
  eprint = {2307.09481},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09481},
  urldate = {2025-03-26},
  abstract = {This work presents AnyDoor, a diffusion-based image generator with the power to teleport target objects to new scenes at user-specified locations in a harmonious way. Instead of tuning parameters for each object, our model is trained only once and effortlessly generalizes to diverse object-scene combinations at the inference stage. Such a challenging zero-shot setting requires an adequate characterization of a certain object. To this end, we complement the commonly used identity feature with detail features, which are carefully designed to maintain texture details yet allow versatile local variations (e.g., lighting, orientation, posture, etc.), supporting the object in favorably blending with different surroundings. We further propose to borrow knowledge from video datasets, where we can observe various forms (i.e., along the time axis) of a single object, leading to stronger model generalizability and robustness. Extensive experiments demonstrate the superiority of our approach over existing alternatives as well as its great potential in real-world applications, such as virtual try-on and object moving. Project page is https://damo-vilab.github.io/AnyDoor-Page/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\6YLDUQ7I\\Chen et al. - 2024 - AnyDoor Zero-shot Object-level Image Customization.pdf;C\:\\Users\\E097600\\Zotero\\storage\\QRLCH74B\\2307.html}
}

@misc{chengBoundaryIoUImproving2021,
  title = {Boundary {{IoU}}: {{Improving Object-Centric Image Segmentation Evaluation}}},
  shorttitle = {Boundary {{IoU}}},
  author = {Cheng, Bowen and Girshick, Ross and Doll{\'a}r, Piotr and Berg, Alexander C. and Kirillov, Alexander},
  year = 2021,
  month = mar,
  number = {arXiv:2103.16562},
  eprint = {2103.16562},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.16562},
  urldate = {2025-01-17},
  abstract = {We present Boundary IoU (Intersection-over-Union), a new segmentation evaluation measure focused on boundary quality. We perform an extensive analysis across different error types and object sizes and show that Boundary IoU is significantly more sensitive than the standard Mask IoU measure to boundary errors for large objects and does not over-penalize errors on smaller objects. The new quality measure displays several desirable characteristics like symmetry w.r.t. prediction/ground truth pairs and balanced responsiveness across scales, which makes it more suitable for segmentation evaluation than other boundary-focused measures like Trimap IoU and F-measure. Based on Boundary IoU, we update the standard evaluation protocols for instance and panoptic segmentation tasks by proposing the Boundary AP (Average Precision) and Boundary PQ (Panoptic Quality) metrics, respectively. Our experiments show that the new evaluation metrics track boundary quality improvements that are generally overlooked by current Mask IoU-based evaluation metrics. We hope that the adoption of the new boundary-sensitive evaluation metrics will lead to rapid progress in segmentation methods that improve boundary quality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\SVQXB76V\\Cheng et al. - 2021 - Boundary IoU Improving Object-Centric Image Segmentation Evaluation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\MY26GGWH\\2103.html}
}

@misc{chengMaskedattentionMaskTransformer2022,
  title = {Masked-Attention {{Mask Transformer}} for {{Universal Image Segmentation}}},
  author = {Cheng, Bowen and Misra, Ishan and Schwing, Alexander G. and Kirillov, Alexander and Girdhar, Rohit},
  year = 2022,
  month = jun,
  number = {arXiv:2112.01527},
  eprint = {2112.01527},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.01527},
  urldate = {2025-02-04},
  abstract = {Image segmentation is about grouping pixels with different semantics, e.g., category or instance membership, where each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\PYGQ8V5K\\Cheng et al. - 2022 - Masked-attention Mask Transformer for Universal Image Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\WCDQSIYD\\2112.html}
}

@misc{chenMakingYourFirst2022,
  title = {Making {{Your First Choice}}: {{To Address Cold Start Problem}} in {{Vision Active Learning}}},
  shorttitle = {Making {{Your First Choice}}},
  author = {Chen, Liangyu and Bai, Yutong and Huang, Siyu and Lu, Yongyi and Wen, Bihan and Yuille, Alan L. and Zhou, Zongwei},
  year = 2022,
  month = oct,
  journal = {arXiv.org},
  urldate = {2025-04-11},
  abstract = {Active learning promises to improve annotation efficiency by iteratively selecting the most important data to be annotated first. However, we uncover a striking contradiction to this promise: active learning fails to select data as efficiently as random selection at the first few choices. We identify this as the cold start problem in vision active learning, caused by a biased and outlier initial query. This paper seeks to address the cold start problem by exploiting the three advantages of contrastive learning: (1) no annotation is required; (2) label diversity is ensured by pseudo-labels to mitigate bias; (3) typical data is determined by contrastive features to reduce outliers. Experiments are conducted on CIFAR-10-LT and three medical imaging datasets (i.e. Colon Pathology, Abdominal CT, and Blood Cell Microscope). Our initial query not only significantly outperforms existing active querying strategies but also surpasses random selection by a large margin. We foresee our solution to the cold start problem as a simple yet strong baseline to choose the initial query for vision active learning. Code is available: https://github.com/c-liangyu/CSVAL},
  howpublished = {https://arxiv.org/abs/2210.02442v1},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\GULIAIBM\Chen et al. - 2022 - Making Your First Choice To Address Cold Start Problem in Vision Active Learning.pdf}
}

@misc{chenREALRepresentativeErrorDriven2023,
  title = {{{REAL}}: {{A Representative Error-Driven Approach}} for {{Active Learning}}},
  shorttitle = {{{REAL}}},
  author = {Chen, Cheng and Wang, Yong and Liao, Lizi and Chen, Yueguo and Du, Xiaoyong},
  year = 2023,
  month = jul,
  number = {arXiv:2307.00968},
  eprint = {2307.00968},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.00968},
  urldate = {2024-03-08},
  abstract = {Given a limited labeling budget, active learning (AL) aims to sample the most informative instances from an unlabeled pool to acquire labels for subsequent model training. To achieve this, AL typically measures the informativeness of unlabeled instances based on uncertainty and diversity. However, it does not consider erroneous instances with their neighborhood error density, which have great potential to improve the model performance. To address this limitation, we propose \$REAL\$, a novel approach to select data instances with \$\textbackslash underline\textbraceleft R\textbraceright\$epresentative \$\textbackslash underline\textbraceleft E\textbraceright\$rrors for \$\textbackslash underline\textbraceleft A\textbraceright\$ctive \$\textbackslash underline\textbraceleft L\textbraceright\$earning. It identifies minority predictions as \textbackslash emph\textbraceleft pseudo errors\textbraceright{} within a cluster and allocates an adaptive sampling budget for the cluster based on estimated error density. Extensive experiments on five text classification datasets demonstrate that \$REAL\$ consistently outperforms all best-performing baselines regarding accuracy and F1-macro scores across a wide range of hyperparameter settings. Our analysis also shows that \$REAL\$ selects the most representative pseudo errors that match the distribution of ground-truth errors along the decision boundary. Our code is publicly available at https://github.com/withchencheng/ECML\_PKDD\_23\_Real.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\E2XZHXZL\\Chen et al. - 2023 - REAL A Representative Error-Driven Approach for A.pdf;C\:\\Users\\E097600\\Zotero\\storage\\V89CBIZ8\\2307.html}
}

@misc{chenRethinkingAtrousConvolution2017,
  title = {Rethinking {{Atrous Convolution}} for {{Semantic Image Segmentation}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = 2017,
  month = dec,
  number = {arXiv:1706.05587},
  eprint = {1706.05587},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.05587},
  urldate = {2025-03-05},
  abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,modele},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\VKEMWWKD\\Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\H77IM24I\\1706.html}
}

@misc{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = 2020,
  month = jun,
  number = {arXiv:2002.05709},
  eprint = {2002.05709},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.05709},
  urldate = {2024-03-05},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\UD3FPIE3\\Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf;C\:\\Users\\E097600\\Zotero\\storage\\IQCXXAUL\\2002.html}
}

@misc{chenSortedAPRethinkingEvaluation2023,
  title = {{{SortedAP}}: {{Rethinking}} Evaluation Metrics for Instance Segmentation},
  shorttitle = {{{SortedAP}}},
  author = {Chen, Long and Wu, Yuli and Stegmaier, Johannes and Merhof, Dorit},
  year = 2023,
  month = sep,
  number = {arXiv:2309.04887},
  eprint = {2309.04887},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.04887},
  urldate = {2025-03-27},
  abstract = {Designing metrics for evaluating instance segmentation revolves around comprehensively considering object detection and segmentation accuracy. However, other important properties, such as sensitivity, continuity, and equality, are overlooked in the current study. In this paper, we reveal that most existing metrics have a limited resolution of segmentation quality. They are only conditionally sensitive to the change of masks or false predictions. For certain metrics, the score can change drastically in a narrow range which could provide a misleading indication of the quality gap between results. Therefore, we propose a new metric called sortedAP, which strictly decreases with both object- and pixel-level imperfections and has an uninterrupted penalization scale over the entire domain. We provide the evaluation toolkit and experiment code at https://www.github.com/looooongChen/sortedAP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,evaluation of learning methods,Instance segmentation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\RIIQCBQ2\\Chen et al. - 2023 - SortedAP Rethinking evaluation metrics for instance segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\BT42VFJ3\\2309.html}
}

@misc{chienActiveLearningGeometric2019,
  title = {Active Learning in the Geometric Block Model},
  author = {Chien, Eli and Tulino, Antonia Maria and Llorca, Jaime},
  year = 2019,
  month = nov,
  journal = {arXiv.org},
  urldate = {2025-06-11},
  abstract = {The geometric block model is a recently proposed generative model for random graphs that is able to capture the inherent geometric properties of many community detection problems, providing more accurate characterizations of practical community structures compared with the popular stochastic block model. Galhotra et al. recently proposed a motif-counting algorithm for unsupervised community detection in the geometric block model that is proved to be near-optimal. They also characterized the regimes of the model parameters for which the proposed algorithm can achieve exact recovery. In this work, we initiate the study of active learning in the geometric block model. That is, we are interested in the problem of exactly recovering the community structure of random graphs following the geometric block model under arbitrary model parameters, by possibly querying the labels of a limited number of chosen nodes. We propose two active learning algorithms that combine the idea of motif-counting with two different label query policies. Our main contribution is to show that sampling the labels of a vanishingly small fraction of nodes (sub-linear in the total number of nodes) is sufficient to achieve exact recovery in the regimes under which the state-of-the-art unsupervised method fails. We validate the superior performance of our algorithms via numerical simulations on both real and synthetic datasets.},
  howpublished = {https://arxiv.org/abs/1912.06570v1},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\7Z4XK8W8\Chien et al. - 2019 - Active learning in the geometric block model.pdf}
}

@misc{choiVaBALIncorporatingClass2020,
  title = {{{VaB-AL}}: {{Incorporating Class Imbalance}} and {{Difficulty}} with {{Variational Bayes}} for {{Active Learning}}},
  shorttitle = {{{VaB-AL}}},
  author = {Choi, Jongwon and Yi, Kwang Moo and Kim, Jihoon and Choo, Jinho and Kim, Byoungjip and Chang, Jin-Yeop and Gwon, Youngjune and Chang, Hyung Jin},
  year = 2020,
  month = dec,
  number = {arXiv:2003.11249},
  eprint = {2003.11249},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.11249},
  urldate = {2025-09-18},
  abstract = {Active Learning for discriminative models has largely been studied with the focus on individual samples, with less emphasis on how classes are distributed or which classes are hard to deal with. In this work, we show that this is harmful. We propose a method based on the Bayes' rule, that can naturally incorporate class imbalance into the Active Learning framework. We derive that three terms should be considered together when estimating the probability of a classifier making a mistake for a given sample; i) probability of mislabelling a class, ii) likelihood of the data given a predicted class, and iii) the prior probability on the abundance of a predicted class. Implementing these terms requires a generative model and an intractable likelihood estimation. Therefore, we train a Variational Auto Encoder (VAE) for this purpose. To further tie the VAE with the classifier and facilitate VAE training, we use the classifiers' deep feature representations as input to the VAE. By considering all three probabilities, among them especially the data imbalance, we can substantially improve the potential of existing methods under limited data budget. We show that our method can be applied to classification tasks on multiple different datasets -- including one that is a real-world dataset with heavy data imbalance -- significantly outperforming the state of the art.},
  archiveprefix = {arXiv},
  keywords = {Active learning,bayesian,cifar10,cifar100,Class imbalance,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Image classification,model:VAE,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\XLUQUCFA\\Choi et al. - 2020 - VaB-AL Incorporating Class Imbalance and Difficulty with Variational Bayes for Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\YIUS3UZN\\2003.html}
}

@misc{chornomazAgnosticLearningTargeted2025,
  title = {Agnostic {{Learning}} under {{Targeted Poisoning}}: {{Optimal Rates}} and the {{Role}} of {{Randomness}}},
  shorttitle = {Agnostic {{Learning}} under {{Targeted Poisoning}}},
  author = {Chornomaz, Bogdan and Koren, Yonatan and Moran, Shay and Waknine, Tom},
  year = 2025,
  month = jun,
  journal = {arXiv.org},
  urldate = {2025-06-10},
  abstract = {We study the problem of learning in the presence of an adversary that can corrupt an \$\textbackslash eta\$ fraction of the training examples with the goal of causing failure on a specific test point. In the realizable setting, prior work established that the optimal error under such instance-targeted poisoning attacks scales as \$\textbackslash Theta(d\textbackslash eta)\$, where \$d\$ is the VC dimension of the hypothesis class arXiv:2210.02713. In this work, we resolve the corresponding question in the agnostic setting. We show that the optimal excess error is \$\textbackslash tilde\textbraceleft\textbackslash Theta\textbraceright (\textbackslash sqrt\textbraceleft d\textbackslash eta\textbraceright )\$, answering one of the main open problems left by Hanneke et al. To achieve this rate, it is necessary to use randomized learners: Hanneke et al. showed that deterministic learners can be forced to suffer error close to 1, even under small amounts of poisoning. Perhaps surprisingly, our upper bound remains valid even when the learner's random bits are fully visible to the adversary . In the other direction, our lower bound is stronger than standard PAC-style bounds: instead of tailoring a hard distribution separately for each sample size, we exhibit a single fixed distribution under which the adversary can enforce an excess error of \$\textbackslash Omega(\textbackslash sqrt\textbraceleft d\textbackslash eta\textbraceright )\$ infinitely often.},
  howpublished = {https://arxiv.org/abs/2506.03075v1},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\ZWU6FQ53\Chornomaz et al. - 2025 - Agnostic Learning under Targeted Poisoning Optimal Rates and the Role of Randomness.pdf}
}

@inproceedings{chowdhuryActiveLearningStrategy2024,
  title = {Active {{Learning Strategy Using Contrastive Learning}} and {{K-Means}} for {{Aquatic Invasive Species Recognition}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Chowdhury, Shaif and Hamerly, Greg and McGarrity, Monica},
  year = 2024,
  pages = {848--858},
  urldate = {2024-03-06},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\SL79ST7Z\Chowdhury et al. - 2024 - Active Learning Strategy Using Contrastive Learnin.pdf}
}

@misc{coeurjollyNormalizedInformationbasedDivergences2006,
  title = {Normalized Information-Based Divergences},
  author = {Coeurjolly, Jean-Fran{\c c}ois and Drouilhet, R{\'e}my and Robineau, Jean-Fran{\c c}ois},
  year = 2006,
  month = nov,
  number = {arXiv:math/0604246},
  eprint = {math/0604246},
  publisher = {arXiv},
  doi = {10.48550/arXiv.math/0604246},
  urldate = {2025-06-25},
  abstract = {This paper is devoted to the mathematical study of some divergences based on the mutual information well-suited to categorical random vectors. These divergences are generalizations of the "entropy distance" and "information distance". Their main characteristic is that they combine a complexity term and the mutual information. We then introduce the notion of (normalized) information-based divergence, propose several examples and discuss their mathematical properties in particular in some prediction framework.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\ZIDUF6TS\\Coeurjolly et al. - 2006 - Normalized information-based divergences.pdf;C\:\\Users\\E097600\\Zotero\\storage\\9UBRNTWK\\0604246.html}
}

@article{cohnActiveLearningStatistical1996,
  title = {Active Learning with Statistical Models},
  author = {Cohn, David A. and Ghahramani, Zoubin and Jordan, Michael I.},
  year = 1996,
  month = mar,
  journal = {J. Artif. Int. Res.},
  volume = {4},
  number = {1},
  pages = {129--145},
  issn = {1076-9757},
  abstract = {For many types of machine learning algorithms, one can compute the statistically "optimal" way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.},
  keywords = {Active learning,basics,Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\YWGYFIEG\\Cohn et al. - 1996 - Active Learning with Statistical Models.pdf;C\:\\Users\\E097600\\Zotero\\storage\\SBB8Q4CP\\9603104.html}
}

@misc{cozmaDefectDetectionTire2024,
  title = {Defect {{Detection}} in {{Tire X-Ray Images}}: {{Conventional Methods Meet Deep Structures}}},
  shorttitle = {Defect {{Detection}} in {{Tire X-Ray Images}}},
  author = {Cozma, Andrei and Harris, Landon and Qi, Hairong and Ji, Ping and Guo, Wenpeng and Yuan, Song},
  year = 2024,
  month = feb,
  number = {arXiv:2402.18527},
  eprint = {2402.18527},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.18527},
  urldate = {2024-03-04},
  abstract = {This paper introduces a robust approach for automated defect detection in tire X-ray images by harnessing traditional feature extraction methods such as Local Binary Pattern (LBP) and Gray Level Co-Occurrence Matrix (GLCM) features, as well as Fourier and Wavelet-based features, complemented by advanced machine learning techniques. Recognizing the challenges inherent in the complex patterns and textures of tire X-ray images, the study emphasizes the significance of feature engineering to enhance the performance of defect detection systems. By meticulously integrating combinations of these features with a Random Forest (RF) classifier and comparing them against advanced models like YOLOv8, the research not only benchmarks the performance of traditional features in defect detection but also explores the synergy between classical and modern approaches. The experimental results demonstrate that these traditional features, when fine-tuned and combined with machine learning models, can significantly improve the accuracy and reliability of tire defect detection, aiming to set a new standard in automated quality assurance in tire manufacturing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,I.4.0,I.4.7,I.4.9},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\QRJVWPQP\\Cozma et al. - 2024 - Defect Detection in Tire X-Ray Images Conventiona.pdf;C\:\\Users\\E097600\\Zotero\\storage\\JI6MXQKC\\2402.html}
}

@misc{csurkaDomainAdaptationVisual2017,
  title = {Domain {{Adaptation}} for {{Visual Applications}}: {{A Comprehensive Survey}}},
  shorttitle = {Domain {{Adaptation}} for {{Visual Applications}}},
  author = {Csurka, Gabriela},
  year = 2017,
  month = mar,
  number = {arXiv:1702.05374},
  eprint = {1702.05374},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.05374},
  urldate = {2025-10-15},
  abstract = {The aim of this paper is to give an overview of domain adaptation and transfer learning with a specific view on visual applications. After a general motivation, we first position domain adaptation in the larger transfer learning problem. Second, we try to address and analyze briefly the state-of-the-art methods for different types of scenarios, first describing the historical shallow methods, addressing both the homogeneous and the heterogeneous domain adaptation methods. Third, we discuss the effect of the success of deep convolutional architectures which led to new type of domain adaptation methods that integrate the adaptation within the deep architecture. Fourth, we overview the methods that go beyond image categorization, such as object detection or image segmentation, video analyses or learning visual attributes. Finally, we conclude the paper with a section where we relate domain adaptation to other machine learning solutions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer vision,domain adaptation,review},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\F7YZWN7Q\\Csurka - 2017 - Domain Adaptation for Visual Applications A Comprehensive Survey.pdf;C\:\\Users\\E097600\\Zotero\\storage\\SPYSZ9S3\\1702.html}
}

@inproceedings{culottaReducingLabelingEffort2005,
  title = {Reducing Labeling Effort for Structured Prediction Tasks},
  booktitle = {Proceedings of the 20th National Conference on {{Artificial}} Intelligence - {{Volume}} 2},
  author = {Culotta, Aron and McCallum, Andrew},
  year = 2005,
  month = jul,
  series = {{{AAAI}}'05},
  pages = {746--751},
  publisher = {AAAI Press},
  address = {Pittsburgh, Pennsylvania},
  urldate = {2025-01-09},
  abstract = {A common obstacle preventing the rapid deployment of supervised machine learning algorithms is the lack of labeled training data. This is particularly expensive to obtain for structured prediction tasks, where each training instance may have multiple, interacting labels, all of which must be correctly annotated for the instance to be of use to the learner. Traditional active learning addresses this problem by optimizing the order in which the examples are labeled to increase learning efficiency. However, this approach does not consider the difficulty of labeling each example, which can vary widely in structured prediction tasks. For example, the labeling predicted by a partially trained system may be easier to correct for some instances than for others.We propose a new active learning paradigm which reduces not only how many instances the annotator must label, but also how difficult each instance is to annotate. The system also leverages information from partially correct predictions to efficiently solicit annotations from the user. We validate this active learning framework in an interactive information extraction system, reducing the total number of annotation actions by 22\%.},
  isbn = {978-1-57735-236-5},
  keywords = {Entropy}
}

@article{daigavaneUnderstandingConvolutionsGraphs2021,
  title = {Understanding {{Convolutions}} on {{Graphs}}},
  author = {Daigavane, Ameya and Ravindran, Balaraman and Aggarwal, Gaurav},
  year = 2021,
  month = sep,
  journal = {Distill},
  volume = {6},
  number = {9},
  pages = {e32},
  issn = {2476-0757},
  doi = {10.23915/distill.00032},
  urldate = {2024-08-26},
  abstract = {Understanding the building blocks and design choices of graph neural networks.},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\SDXVP5YA\understanding-gnns.html}
}

@misc{daiInstanceawareSemanticSegmentation2015,
  title = {Instance-Aware {{Semantic Segmentation}} via {{Multi-task Network Cascades}}},
  author = {Dai, Jifeng and He, Kaiming and Sun, Jian},
  year = 2015,
  month = dec,
  number = {arXiv:1512.04412},
  eprint = {1512.04412},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.04412},
  urldate = {2025-01-06},
  abstract = {Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multi-task Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\UK9MSA3I\\Dai et al. - 2015 - Instance-aware Semantic Segmentation via Multi-task Network Cascades.pdf;C\:\\Users\\E097600\\Zotero\\storage\\NFUTPH3T\\1512.html}
}

@inproceedings{dasguptaGeneralAgnosticActive2007,
  title = {A General Agnostic Active Learning Algorithm},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dasgupta, Sanjoy and Hsu, Daniel J and Monteleoni, Claire},
  year = 2007,
  volume = {20},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-06},
  abstract = {We present an agnostic active learning algorithm for any hypothesis class of bounded VC dimension under arbitrary data distributions. Most previ- ous work on active learning either makes strong distributional assumptions, or else is computationally prohibitive. Our algorithm extends the simple scheme of Cohn, Atlas, and Ladner [1] to the agnostic setting, using re- ductions to supervised learning that harness generalization bounds in a simple but subtle manner. We provide a fall-back guarantee that bounds the algorithm's label complexity by the agnostic PAC sample complexity. Our analysis yields asymptotic label complexity improvements for certain hypothesis classes and distributions. We also demonstrate improvements experimentally.},
  file = {C:\Users\E097600\Zotero\storage\7MNJR3LM\Dasgupta et al. - 2007 - A general agnostic active learning algorithm.pdf}
}

@misc{dasguptaUncertaintyawareActiveLearning2024,
  title = {Uncertainty-Aware {{Active Learning}} of {{NeRF-based Object Models}} for {{Robot Manipulators}} Using {{Visual}} and {{Re-orientation Actions}}},
  author = {Dasgupta, Saptarshi and Gupta, Akshat and Tuli, Shreshth and Paul, Rohan},
  year = 2024,
  month = apr,
  number = {arXiv:2404.01812},
  eprint = {2404.01812},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.01812},
  urldate = {2025-01-16},
  abstract = {Manipulating unseen objects is challenging without a 3D representation, as objects generally have occluded surfaces. This requires physical interaction with objects to build their internal representations. This paper presents an approach that enables a robot to rapidly learn the complete 3D model of a given object for manipulation in unfamiliar orientations. We use an ensemble of partially constructed NeRF models to quantify model uncertainty to determine the next action (a visual or re-orientation action) by optimizing informativeness and feasibility. Further, our approach determines when and how to grasp and re-orient an object given its partial NeRF model and re-estimates the object pose to rectify misalignments introduced during the interaction. Experiments with a simulated Franka Emika Robot Manipulator operating in a tabletop environment with benchmark objects demonstrate an improvement of (i) 14\% in visual reconstruction quality (PSNR), (ii) 20\% in the geometric/depth reconstruction of the object surface (F-score) and (iii) 71\% in the task success rate of manipulating objects a-priori unseen orientations/stable configurations in the scene; over current methods. The project page can be found here: https://actnerf.github.io.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\FGIBMIJL\\Dasgupta et al. - 2024 - Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual an.pdf;C\:\\Users\\E097600\\Zotero\\storage\\UF8IJKX5\\2404.html}
}

@inproceedings{deauPREFABGENADHOC2023,
  title = {{{PREFAB-GEN}} : {{AD HOC Image Generation}} for {{Pre-Manufacturing}} of {{Tires Using Image-To-Image Translation}}},
  shorttitle = {{{PREFAB-GEN}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {D{\'e}au, Guillaume and Bourdon, Pascal and Carr{\'e}, Philippe and M{\'e}rillou, St{\'e}phane and Dervill{\'e}, Alexandre and Mourougaya, Fran{\c c}ois},
  year = 2023,
  month = oct,
  pages = {1610--1614},
  doi = {10.1109/ICIP49359.2023.10222342},
  urldate = {2024-08-21},
  abstract = {In the pneumatic industry, quality control is an essential step in assessing tire compliance. Artificial neural networks are increasingly used to accomplish this task. Their training requires a large number of images of the controlled products. However, at the launch production of a new tire, the lack of images causes a performance loss for the network. To solve this problem, we propose to translate perfect tires computer-based images into ad hoc manufacturing context-realistic ones as pre-manufacturing step to improve robustness and ensure production quality. The challenging work is to extract features in real images and apply them to computer-based images while maintaining the original geometry. In the paper, we propose Prefab-GEN, a novel architecture based on Cycle-GAN. In the generator part, an Inception U-Net architecture is developed to enforce geometrical structure conversion and extract more detailed features. The qualitative and quantitative evaluation on tire dataset shows improvements compared with state-of-art.},
  keywords = {Adaptation models,Computational modeling,Computer architecture,CycleGAN,Feature extraction,Image-to-image translation,Manufacturing,Production,Training,Unpaired data,Visualization},
  file = {C:\Users\E097600\Zotero\storage\FWA8SL33\10222342.html}
}

@inproceedings{degranceyObjectDetectionProbabilistic2022,
  title = {Object {{Detection}} with~{{Probabilistic Guarantees}}: {{A Conformal Prediction Approach}}},
  shorttitle = {Object {{Detection}} with~{{Probabilistic Guarantees}}},
  booktitle = {Computer {{Safety}},  {{Reliability}}, and {{Security}}. {{SAFECOMP}} 2022 {{Workshops}}},
  author = {{de Grancey}, Florence and Adam, Jean-Luc and Alecu, Lucian and Gerchinovitz, S{\'e}bastien and Mamalet, Franck and Vigouroux, David},
  editor = {Trapp, Mario and Schoitsch, Erwin and Guiochet, J{\'e}r{\'e}mie and Bitsch, Friedemann},
  year = 2022,
  pages = {316--329},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-14862-0_23},
  abstract = {Providing reliable uncertainty quantification for complex visual tasks such as object detection is of utmost importance for safety-critical applications such as autonomous driving, tumor detection, etc. Conformal prediction methods offer simple yet practical means to build uncertainty estimations that come with probabilistic guarantees. In this paper we apply such methods to the task of object localization and illustrate our analysis on a pedestrian detection use-case. We highlight both theoretical and practical implications of our analysis.},
  isbn = {978-3-031-14862-0},
  langid = {english},
  keywords = {Conformal prediction,Object detection},
  file = {C:\Users\E097600\Zotero\storage\6VYXZF5D\de Grancey et al. - 2022 - Object Detection with Probabilistic Guarantees A Conformal Prediction Approach.pdf}
}

@misc{dekaComprehensiveReviewDeep2025,
  title = {Comprehensive {{Review}} of {{Deep Unfolding Techniques}} for {{Next-Generation Wireless Communication Systems}}},
  author = {Deka, Sukanya and Deka, Kuntal and Nguyen, Nhan Thanh and Sharma, Sanjeev and Bhatia, Vimal and Rajatheva, Nandana},
  year = 2025,
  month = feb,
  number = {arXiv:2502.05952},
  eprint = {2502.05952},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.05952},
  urldate = {2025-11-06},
  abstract = {The application of machine learning in wireless communications has been extensively explored, with deep unfolding emerging as a powerful model-based technique. Deep unfolding enhances interpretability by transforming complex iterative algorithms into structured layers of deep neural networks (DNNs). This approach seamlessly integrates domain knowledge with deep learning (DL), leveraging the strengths of both methods to simplify complex signal processing tasks in communication systems. To provide a solid foundation, we first present a brief overview of DL and deep unfolding. We then explore the applications of deep unfolding in key areas, including signal detection, channel estimation, beamforming design, decoding for error-correcting codes, sensing and communication, power allocation, and security. Each section focuses on a specific task, highlighting its significance in emerging 6G technologies and reviewing recent advancements in deep unfolding-based solutions. Finally, we discuss the challenges associated with developing deep unfolding techniques and propose potential improvements to enhance their applicability across diverse wireless communication scenarios.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Signal Processing},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\I58LWHHA\\Deka et al. - 2025 - Comprehensive Review of Deep Unfolding Techniques for Next-Generation Wireless Communication Systems.pdf;C\:\\Users\\E097600\\Zotero\\storage\\TWUDD3QF\\2502.html}
}

@phdthesis{demathelindepapignyReliableMachineLearning2024,
  type = {Theses},
  title = {Towards Reliable Machine Learning under Domain Shift and Costly Labeling, with Applications to Engineering Design},
  author = {{de Mathelin de Papigny}, Antoine},
  year = 2024,
  month = oct,
  number = {2024UPASM025},
  urldate = {2025-11-26},
  abstract = {In engineering design, using machine learning models to find innovative products poses major challenges. The effectiveness of machine learning models has been demonstrated when trained and used on large datasets of independently identically distributed observations. However, in the engineering design context, models are often deployed on shifted distributions, with few labeled data available. Moreover, the reliability of the model is strongly required as trusting wrong predictions can lead to dramatic consequences. This thesis tackles the challenge of providing a reliable machine learning model under the main engineering design constraints: domain shift and costly labeling. By leveraging novel contributions from domain adaptation, active learning and uncertainty quantification techniques, we propose a generic approach towards this goal. Moreover, the contributions of this thesis to the three aforementioned thematics are impactful beyond the context of engineering design. They allow for achieving similar or better performances with less data and reduced computation time compared to standard approaches. Additionally, the thesis delivers accessible and user-friendly tools through a domain adaptation and transfer learning library called Adapt.},
  school = {Universit\'e Paris-Saclay},
  keywords = {Active learning,Adaptation de domaine,Apprentissage actif,Conception produit,Conformal prediction,Domain adaptation,Engineering design,Quantification d'incertitude,Uncertainty quantification},
  file = {C:\Users\E097600\Zotero\storage\NE6TSRJ3\de Mathelin de Papigny - 2024 - Towards reliable machine learning under domain shift and costly labeling, with applications to engin.pdf}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = 2009,
  month = jun,
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  urldate = {2025-02-11},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500--1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {C:\Users\E097600\Zotero\storage\TK8LSB9B\5206848.html}
}

@phdthesis{dervilleDeveloppementDalgorithmesMetrologie2018,
  type = {These de Doctorat},
  title = {D\'eveloppement d'algorithmes de M\'etrologie D\'edi\'es \`a La Caract\'erisation de Nano-Objets \`a Partir d'informations H\'et\'erog\`enes},
  author = {Derville, Alexandre},
  year = 2018,
  month = dec,
  urldate = {2025-11-17},
  abstract = {Ces travaux de th\`ese s'inscrivent dans le contexte technico/\'economique des nanomat\'eriaux notamment les nanoparticules et les copolym\`eres. Aujourd'hui, une r\'evolution technologique est en cours avec l'introduction de ces mat\'eriaux dans des matrices plus ou moins complexes pr\'esentes dans notre quotidien (sant\'e, cosm\'etique, b\^atiment, agroalimentaire...). Ces mat\'eriaux conf\`erent \`a ces produits des propri\'et\'es uniques (m\'ecanique, \'electrique, chimique, thermique, ...). Cette omnipr\'esence associ\'ee aux enjeux \'economiques engendre deux probl\'ematiques li\'ees au contr\^ole des proc\'ed\'es de fabrication et \`a la m\'etrologie associ\'ee. La premi\`ere est de garantir une tra\c cabilit\'e de ces nanomat\'eriaux afin de pr\'evenir tout risque sanitaire et environnemental et la seconde est d'optimiser le d\'eveloppement des proc\'ed\'es afin de p\'erenniser des fili\`eres \'economiques rentables. Pour cela, les deux techniques les plus courantes de m\'etrologie utilis\'ees sont : la microscopie \'electronique \`a balayage (MEB) et la microscopie \`a force atomique (AFM).Le premier volet des travaux est consacr\'e au d\'eveloppement d'une m\'ethodologie de fusion de donn\'ees permettant d'analyser automatiquement les donn\'ees en provenance de chaque microscope et d'utiliser leurs points forts respectifs afin de r\'eduire les incertitudes de mesure en trois dimensions. Une premi\`ere partie a \'et\'e consacr\'ee \`a la correction d'un d\'efaut majeur d'asservissement de l'AFM qui g\'en\`ere des d\'erives et/ou sauts dans les signaux. Nous pr\'esentons une technique dirig\'ee par les donn\'ees permettant une correction de ces signaux. La m\'ethode pr\'esent\'ee a l'avantage de ne pas faire d'hypoth\`eses sur les objets et leurs positions. Elle peut \^etre utilis\'ee en routine automatique pour l'am\'elioration du signal avant l'analyse des objets.La deuxi\`eme partie est consacr\'ee au d\'eveloppement d'une m\'ethode d'analyse automatique des images de nanoparticules sph\'eriques en provenance d'un AFM ou d'un MEB. Dans le but de d\'evelopper une tra\c cabilit\'e en 3D, il est n\'ecessaire d'identifier et de mesurer les nanoparticules identiques qui ont \'et\'e mesur\'ees \`a la fois sur l'AFM et sur le MEB. Afin d'obtenir deux estimations du diam\`etre sur la m\^eme particule physique, nous avons d\'evelopp\'e une technique qui permet de mettre en correspondance les particules. Partant des estimations pour les deux types de microscopie, avec des particules pr\'esentes dans les deux types d'images ou non, nous pr\'esentons une technique qui permet l'agr\'egation d'estimateurs sur les populations de diam\`etres afin d'obtenir une valeur plus fiable des propri\'et\'es du diam\`etre des particules.Le second volet de cette th\`ese est d\'edi\'e \`a l'optimisation d'un proc\'ed\'e de fabrication de copolym\`eres \`a blocs (structures lamellaires) afin d'exploiter toutes les grandeurs caract\'eristiques utilis\'ees pour la validation du proc\'ed\'e (largeur de ligne, p\'eriode, rugosit\'e, taux de d\'efauts) notamment \`a partir d'images MEB afin de les mettre en correspondance avec un ensemble de param\`etres de proc\'ed\'e. En effet, lors du d\'eveloppement d'un nouveau proc\'ed\'e, un plan d'exp\'eriences est effectu\'e. L'analyse de ce dernier permet d'estimer manuellement une fen\^etre de proc\'ed\'e plus ou moins pr\'ecise (estimation li\'ee \`a l'expertise de l'ing\'enieur mat\'eriaux). L'\'etape est r\'eit\'er\'ee jusqu'\`a l'obtention des caract\'eristiques souhait\'ees. Afin d'acc\'el\'erer le d\'eveloppement, nous avons \'etudi\'e une fa\c con de pr\'edire le r\'esultat du proc\'ed\'e de fabrication sur l'espace des param\`etres. Pour cela, nous avons \'etudi\'e diff\'erentes techniques de r\'egression que nous pr\'esentons afin de proposer une m\'ethodologie automatique d'optimisation des param\`etres d'un proc\'ed\'e aliment\'ee par les caract\'eristiques d'images AFM et/ou MEB.Ces travaux d'agr\'egations d'estimateurs et d'optimisation de fen\^etre de proc\'ed\'es permettent d'envisager le d\'eveloppement d'une standardisation d'analyse automatique de donn\'ees issues de MEB et d'AFM en vue du d\'eveloppement d'une norme de tra\c cabilit\'e des nanomat\'eriaux.},
  collaborator = {Coeurjolly, Jean-Fran{\c c}ois and Clausel, Marianne},
  copyright = {Licence Etalab},
  school = {Universit\'e Grenoble Alpes (ComUE)},
  keywords = {004,510,Data fusion,Fusion de donnees,Glatiramer,Microscopie electronique a balayage,Nano-Object,Nanoparticules,Statistiques,Tracabilite},
  file = {C:\Users\E097600\Zotero\storage\UWM8Z4JT\Derville - 2018 - Développement d'algorithmes de métrologie dédiés à la caractérisation de nano-objets à partir d'info.pdf}
}

@phdthesis{deschampsApprentissageActifProfond2023,
  title = {{Apprentissage actif profond pour la reconnaissance visuelle \`a partir de peu d'exemples}},
  author = {Deschamps, S{\'e}bastien},
  year = 2023,
  month = jun,
  urldate = {2024-07-11},
  abstract = {L'analyse automatique d'images a permis d'am\'eliorer l'exploitation des capteurs d'image, avec des donn\'ees qui proviennent de diff\'erents capteurs tels que des cam\'eras de t\'el\'ephone, des cam\'eras de surveillance, des imageurs satellites ou encore des drones. L'apprentissage profond obtient d'excellents r\'esultats dans les applications d'analyse d'images o\`u de grandes quantit\'es de donn\'ees annot\'ees sont disponibles, mais apprendre un nouveau classifieur d'images \`a partir de z\'ero est une t\^ache difficile. La plupart des m\'ethodes de classification d'images sont supervis\'ees, n\'ecessitant des annotations, ce qui repr\'esente un investissement important. Diff\'erentes solutions d'apprentissage frugal (avec peu d'exemples annot\'es) existent, notamment l'apprentissage par transfert, l'apprentissage actif, l'apprentissage semi-supervis\'e ou bien le m\'eta-apprentissage. L'objectif de cette th\`ese est d'\'etudier ces solutions d'apprentissage frugal pour des t\^aches de reconnaissance visuelle, notamment la classification d'images et la d\'etection des changements dans des images satellites. Ainsi, le classifieur est entra\^in\'e de fa\c con it\'erative en commen\c cant avec tr\`es peu de donn\'ees, et en demandant \`a l'utilisateur d'annoter le moins possible de donn\'ees pour obtenir des performances satisfaisantes. L'apprentissage actif profond a \'et\'e \'etudi\'e initialement avec d'autres m\'ethodes et nous a sembl\'e le plus adapt\'e \`a notre probl\'ematique m\'etier, nous avons donc privil\'egi\'e cette solution. Nous avons d\'evelopp\'e dans cette th\`ese une premi\`ere approche interactive, o\`u nous posons les questions les plus informatives sur la pertinence des donn\'ees \`a un oracle (annotateur). En fonction de ses r\'eponses, une fonction de d\'ecision est mise \`a jour it\'erativement. Nous mod\'elisons la probabilit\'e que les \'echantillons soient pertinents, en minimisant une fonction objectif capturant la repr\'esentativit\'e, la diversit\'e et l'ambigu\"it\'e des donn\'ees. Les donn\'ees avec une probabilit\'e \'elev\'ee sont ensuite s\'electionn\'ees pour annotation. Nous avons fait \'evoluer cette approche, en utilisant l'apprentissage par renforcement pour pond\'erer dynamiquement et pr\'ecis\'ement l'importance de la repr\'esentativit\'e, l'ambigu\"it\'e et la diversit\'e des donn\'ees \`a chaque cycle d'apprentissage actif. Finalement, notre derni\`ere approche consiste en un mod\`ele d'affichage qui s\'electionne des exemples virtuels les plus repr\'esentatifs et divers, qui remettent en question le mod\`ele appris, de sorte \`a obtenir un mod\`ele tr\`es discriminatoire dans les it\'erations suivantes de l'apprentissage actif. Les bons r\'esultats obtenus face aux diff\'erentes baselines et l'\'etat de l'art, en d\'etection de changements dans des images satellites et en classification d'images, ont permis de d\'emontrer la pertinence des mod\`eles d'apprentissage frugal propos\'es, et ont donn\'e lieu \`a diverses publications (Sahbi et al. 2021 ; Deschamps et Sahbi 2022b ; Deschamps et Sahbi 2022a ; Sahbi et Deschamps 2022).},
  langid = {french},
  school = {Sorbonne Universit\'e},
  keywords = {Active learning,thesis},
  file = {C:\Users\E097600\Zotero\storage\8NDP2TNS\Deschamps - 2023 - Apprentissage actif profond pour la reconnaissance.pdf}
}

@incollection{devarakondaDataDrivenApproachesSelecting2023,
  title = {Data-{{Driven Approaches}} to {{Selecting Samples}} for {{Training Neural Networks}}},
  booktitle = {System {{Dependability}} and {{Analytics}}: {{Approaching System Dependability}} from {{Data}}, {{System}} and {{Analytics Perspectives}}},
  author = {Devarakonda, Murthy V.},
  editor = {Wang, Long and Pattabiraman, Karthik and Di Martino, Catello and Athreya, Arjun and Bagchi, Saurabh},
  year = 2023,
  series = {Springer {{Series}} in {{Reliability Engineering}}},
  pages = {327--345},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-02063-6_18},
  urldate = {2024-03-05},
  abstract = {Modern neural networks, that are now commonly used for most natural language processing (NLP) tasks, contain many hidden units and parameters. There is a~considerable interest in developing strategies for selecting an optimal set of samples to train such large models for biomedical tasks because  developing training data is expensive and time consuming in the biomedical space.  Lack of sufficient training data is exacerbated by the fact that~the ratio of negative samples to positive samples is also highly skewed, i.e., too many negative samples but too few positive samples. Therefore, an important~problem, especially for~the biomedical space, what~is~the~optimum set of negative samples to use in creating an effective and balanced~training data sample. Interestingly~though, the insights which may help to decide the most effective sample selection can be found in the data itself (i.e., in the samples themselves). This chapter briefly reviews traditional approaches to selecting training samples and then presents the latest data-driven approaches for selecting samples to effectively train modern neural networks.},
  isbn = {978-3-031-02063-6},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\BB3NDNTV\Devarakonda - 2023 - Data-Driven Approaches to Selecting Samples for Tr.pdf}
}

@inproceedings{diakonikolasDistributionIndependentPACLearning2019,
  title = {Distribution-{{Independent PAC Learning}} of {{Halfspaces}} with {{Massart Noise}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Diakonikolas, Ilias and Gouleakis, Themis and Tzamos, Christos},
  year = 2019,
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-12-23},
  file = {C:\Users\E097600\Zotero\storage\FRLXJNX3\Diakonikolas et al. - 2019 - Distribution-Independent PAC Learning of Halfspaces with Massart Noise.pdf}
}

@article{diceMeasuresAmountEcologic1945,
  title = {Measures of the {{Amount}} of {{Ecologic Association Between Species}}},
  author = {Dice, Lee R.},
  year = 1945,
  journal = {Ecology},
  volume = {26},
  number = {3},
  eprint = {1932409},
  eprinttype = {jstor},
  pages = {297--302},
  publisher = {[Wiley, Ecological Society of America]},
  issn = {0012-9658},
  doi = {10.2307/1932409},
  urldate = {2025-01-17},
  file = {C:\Users\E097600\Zotero\storage\WTG4VPZJ\Dice - 1945 - Measures of the Amount of Ecologic Association Between Species.pdf}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = 2021,
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.11929},
  urldate = {2025-04-05},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\S39CE6RC\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;C\:\\Users\\E097600\\Zotero\\storage\\CZZVBUHD\\2010.html}
}

@misc{doucetBridgingDiversityUncertainty2024,
  title = {Bridging {{Diversity}} and {{Uncertainty}} in {{Active}} Learning with {{Self-Supervised Pre-Training}}},
  author = {Doucet, Paul and Estermann, Benjamin and Aczel, Till and Wattenhofer, Roger},
  year = 2024,
  month = mar,
  number = {arXiv:2403.03728},
  eprint = {2403.03728},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.03728},
  urldate = {2024-03-08},
  abstract = {This study addresses the integration of diversity-based and uncertainty-based sampling strategies in active learning, particularly within the context of self-supervised pre-trained models. We introduce a straightforward heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels. By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies. Our experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\73B52JAS\\Doucet et al. - 2024 - Bridging Diversity and Uncertainty in Active learn.pdf;C\:\\Users\\E097600\\Zotero\\storage\\9D9A4VBE\\2403.html}
}

@misc{dumoulinGuideConvolutionArithmetic2018,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  year = 2018,
  month = jan,
  number = {arXiv:1603.07285},
  eprint = {1603.07285},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1603.07285},
  urldate = {2025-02-21},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\9MQUHLUC\\Dumoulin et Visin - 2018 - A guide to convolution arithmetic for deep learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\UR2PJYUF\\1603.html}
}

@inproceedings{dwyerDecisionTreeInstability2007,
  title = {Decision {{Tree Instability}} and {{Active Learning}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2007},
  author = {Dwyer, Kenneth and Holte, Robert},
  editor = {Kok, Joost N. and Koronacki, Jacek and de Mantaras, Raomon Lopez and Matwin, Stan and Mladeni{\v c}, Dunja and Skowron, Andrzej},
  year = 2007,
  pages = {128--139},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-74958-5_15},
  abstract = {Decision tree learning algorithms produce accurate models that can be interpreted by domain experts. However, these algorithms are known to be unstable -- they can produce drastically different hypotheses from training sets that differ just slightly. This instability undermines the objective of extracting knowledge from the trees. In this paper, we study the instability of the C4.5 decision tree learner in the context of active learning. We introduce a new measure of decision tree stability, and define three aspects of active learning stability. Several existing active learning methods that use C4.5 as a component are compared empirically; it is determined that query-by-bagging yields trees that are more stable and accurate than those produced by competing methods. Also, an alternative splitting criterion, DKM, is found to improve the stability and accuracy of C4.5 in the active learning setting.},
  isbn = {978-3-540-74958-5},
  langid = {english},
  keywords = {Active learning,Decision tree learning,ensemble methods,evaluation of learning methods,stability},
  file = {C:\Users\E097600\Zotero\storage\E8KW89NS\Dwyer et Holte - 2007 - Decision Tree Instability and Active Learning.pdf}
}

@article{ecarnotWritingScientificArticle2015,
  title = {Writing a Scientific Article: {{A}} Step-by-Step Guide for Beginners},
  shorttitle = {Writing a Scientific Article},
  author = {Ecarnot, F. and Seronde, M. -F. and Chopard, R. and Schiele, F. and Meneveau, N.},
  year = 2015,
  month = dec,
  journal = {European Geriatric Medicine},
  volume = {6},
  number = {6},
  pages = {573--579},
  issn = {1878-7649},
  doi = {10.1016/j.eurger.2015.08.005},
  urldate = {2025-09-18},
  abstract = {Many young researchers find it extremely difficult to write scientific articles, and few receive specific training in the art of presenting their research work in written format. Yet, publication is often vital for career advancement, to obtain funding, to obtain academic qualifications, or for all these reasons. We describe here the basic steps to follow in writing a scientific article. We outline the main sections that an average article should contain; the elements that should appear in these sections, and some pointers for making the overall result attractive and acceptable for publication.},
  keywords = {Article,Research,Scientific publications,Writing},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\FALTBNBS\\1-s2.0-S1878764915001606-main.pdf;C\:\\Users\\E097600\\Zotero\\storage\\YTDV9IJZ\\Ecarnot et al. - 2015 - Writing a scientific article A step-by-step guide for beginners.pdf;C\:\\Users\\E097600\\Zotero\\storage\\CZZV8YHT\\S1878764915001606.html}
}

@misc{eleziNotAllLabels2021,
  title = {Not {{All Labels Are Equal}}: {{Rationalizing The Labeling Costs}} for {{Training Object Detection}}},
  shorttitle = {Not {{All Labels Are Equal}}},
  author = {Elezi, Ismail and Yu, Zhiding and Anandkumar, Anima and {Leal-Taixe}, Laura and Alvarez, Jose M.},
  year = 2021,
  month = nov,
  number = {arXiv:2106.11921},
  eprint = {2106.11921},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.11921},
  urldate = {2025-05-06},
  abstract = {Deep neural networks have reached high accuracy on object detection but their success hinges on large amounts of labeled data. To reduce the labels dependency, various active learning strategies have been proposed, typically based on the confidence of the detector. However, these methods are biased towards high-performing classes and can lead to acquired datasets that are not good representatives of the testing set data. In this work, we propose a unified framework for active learning, that considers both the uncertainty and the robustness of the detector, ensuring that the network performs well in all classes. Furthermore, our method leverages auto-labeling to suppress a potential distribution drift while boosting the performance of the model. Experiments on PASCAL VOC07+12 and MS-COCO show that our method consistently outperforms a wide range of active learning methods, yielding up to a 7.7\% improvement in mAP, or up to 82\% reduction in labeling cost. Code will be released upon acceptance of the paper.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Object detection},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\JJSVVXPF\\Elezi et al. - 2021 - Not All Labels Are Equal Rationalizing The Labeling Costs for Training Object Detection.pdf;C\:\\Users\\E097600\\Zotero\\storage\\YDVWZV64\\2106.html}
}

@inproceedings{elkanFoundationsCostsensitiveLearning2001,
  title = {The Foundations of Cost-Sensitive Learning},
  booktitle = {Proceedings of the 17th International Joint Conference on {{Artificial}} Intelligence - {{Volume}} 2},
  author = {Elkan, Charles},
  year = 2001,
  month = aug,
  series = {{{IJCAI}}'01},
  pages = {973--978},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  urldate = {2025-02-11},
  abstract = {This paper revisits the problem of optimal learning and decision-making when different misclassification errors incur different penalties. We characterize precisely but intuitively when a cost matrix is reasonable, and we show how to avoid the mistake of defining a cost matrix that is economically incoherent. For the two-class case, we prove a theorem that shows how to change the proportion of negative examples in a training set in order to make optimal cost-sensitive classification decisions using a classifier learned by a standard non-cost-sensitive learning method. However, we then argue that changing the balance of negative and positive training examples has little effect on the classifiers produced by standard Bayesian and decision tree learning methods. Accordingly, the recommended way of applying one of these methods in a domain with differing misclassification costs is to learn a classifier from the training set as given, and then to compute optimal decisions explicitly using the probability estimates given by the classifier.},
  isbn = {978-1-55860-812-2}
}

@misc{fajriRobustActiveLearning2024,
  title = {Robust {{Active Learning}} ({{RoAL}}): {{Countering Dynamic Adversaries}} in {{Active Learning}} with {{Elastic Weight Consolidation}}},
  shorttitle = {Robust {{Active Learning}} ({{RoAL}})},
  author = {Fajri, Ricky Maulana and Pei, Yulong and Yin, Lu and Pechenizkiy, Mykola},
  year = 2024,
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-08-26},
  abstract = {Despite significant advancements in active learning and adversarial attacks, the intersection of these two fields remains underexplored, particularly in developing robust active learning frameworks against dynamic adversarial threats. The challenge of developing robust active learning frameworks under dynamic adversarial attacks is critical, as these attacks can lead to catastrophic forgetting within the active learning cycle. This paper introduces Robust Active Learning (RoAL), a novel approach designed to address this issue by integrating Elastic Weight Consolidation (EWC) into the active learning process. Our contributions are threefold: First, we propose a new dynamic adversarial attack that poses significant threats to active learning frameworks. Second, we introduce a novel method that combines EWC with active learning to mitigate catastrophic forgetting caused by dynamic adversarial attacks. Finally, we conduct extensive experimental evaluations to demonstrate the efficacy of our approach. The results show that RoAL not only effectively counters dynamic adversarial threats but also significantly reduces the impact of catastrophic forgetting, thereby enhancing the robustness and performance of active learning systems in adversarial environments.},
  howpublished = {https://arxiv.org/abs/2408.07364v2},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\A2JYSD33\Fajri et al. - 2024 - Robust Active Learning (RoAL) Countering Dynamic .pdf}
}

@misc{fangEVAExploringLimits2022,
  title = {{{EVA}}: {{Exploring}} the {{Limits}} of {{Masked Visual Representation Learning}} at {{Scale}}},
  shorttitle = {{{EVA}}},
  author = {Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  year = 2022,
  month = dec,
  number = {arXiv:2211.07636},
  eprint = {2211.07636},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.07636},
  urldate = {2025-05-14},
  abstract = {We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVISv1.0 dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models. To facilitate future research, we release all the code and models at https://github.com/baaivision/EVA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,modele,Object detection},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\7DA3J2PG\\Fang et al. - 2022 - EVA Exploring the Limits of Masked Visual Representation Learning at Scale.pdf;C\:\\Users\\E097600\\Zotero\\storage\\H2K7UP2D\\2211.html}
}

@misc{fangLearningHowActive2017,
  title = {Learning How to {{Active Learn}}: {{A Deep Reinforcement Learning Approach}}},
  shorttitle = {Learning How to {{Active Learn}}},
  author = {Fang, Meng and Li, Yuan and Cohn, Trevor},
  year = 2017,
  month = aug,
  number = {arXiv:1708.02383},
  eprint = {1708.02383},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.02383},
  urldate = {2024-05-17},
  abstract = {Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\6V5VTHAP\\Fang et al. - 2017 - Learning how to Active Learn A Deep Reinforcement.pdf;C\:\\Users\\E097600\\Zotero\\storage\\F9GRLJFD\\1708.html}
}

@misc{fangUnleashingVanillaVision2022,
  title = {Unleashing {{Vanilla Vision Transformer}} with {{Masked Image Modeling}} for {{Object Detection}}},
  author = {Fang, Yuxin and Yang, Shusheng and Wang, Shijie and Ge, Yixiao and Shan, Ying and Wang, Xinggang},
  year = 2022,
  month = may,
  number = {arXiv:2204.02964},
  eprint = {2204.02964},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.02964},
  urldate = {2025-07-24},
  abstract = {We present an approach to efficiently and effectively adapt a masked image modeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object detection, which is based on our two novel observations: (i) A MIM pre-trained vanilla ViT encoder can work surprisingly well in the challenging object-level recognition scenario even with randomly sampled partial observations, e.g., only 25\% \$\textbackslash sim\$ 50\% of the input embeddings. (ii) In order to construct multi-scale representations for object detection from single-scale ViT, a randomly initialized compact convolutional stem supplants the pre-trained large kernel patchify stem, and its intermediate features can naturally serve as the higher resolution inputs of a feature pyramid network without further upsampling or other manipulations. While the pre-trained ViT is only regarded as the 3\$\textasciicircum\textbraceleft rd\textbraceright\$-stage of our detector's backbone instead of the whole feature extractor. This results in a ConvNet-ViT hybrid feature extractor. The proposed detector, named MIMDet, enables a MIM pre-trained vanilla ViT to outperform hierarchical Swin Transformer by 2.5 box AP and 2.6 mask AP on COCO, and achieves better results compared with the previous best adapted vanilla ViT detector using a more modest fine-tuning recipe while converging 2.8\$\textbackslash times\$ faster. Code and pre-trained models are available at https://github.com/hustvl/MIMDet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\H27CHJ5M\\Fang et al. - 2022 - Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection.pdf;C\:\\Users\\E097600\\Zotero\\storage\\RCTBNWI6\\2204.html}
}

@misc{fangYouOnlyLook2021,
  title = {You {{Only Look}} at {{One Sequence}}: {{Rethinking Transformer}} in {{Vision}} through {{Object Detection}}},
  shorttitle = {You {{Only Look}} at {{One Sequence}}},
  author = {Fang, Yuxin and Liao, Bencheng and Wang, Xinggang and Fang, Jiemin and Qi, Jiyang and Wu, Rui and Niu, Jianwei and Liu, Wenyu},
  year = 2021,
  month = oct,
  number = {arXiv:2106.00666},
  eprint = {2106.00666},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.00666},
  urldate = {2024-03-04},
  abstract = {Can Transformer perform 2D object- and region-level recognition from a pure sequence-to-sequence perspective with minimal knowledge about the 2D spatial structure? To answer this question, we present You Only Look at One Sequence (YOLOS), a series of object detection models based on the vanilla Vision Transformer with the fewest possible modifications, region priors, as well as inductive biases of the target task. We find that YOLOS pre-trained on the mid-sized ImageNet-1k dataset only can already achieve quite competitive performance on the challenging COCO object detection benchmark, e.g., YOLOS-Base directly adopted from BERT-Base architecture can obtain 42.0 box AP on COCO val. We also discuss the impacts as well as limitations of current pre-train schemes and model scaling strategies for Transformer in vision through YOLOS. Code and pre-trained models are available at https://github.com/hustvl/YOLOS.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\E097600\Zotero\storage\G6HUULUU\2106.html}
}

@misc{fathiSemanticInstanceSegmentation2017,
  title = {Semantic {{Instance Segmentation}} via {{Deep Metric Learning}}},
  author = {Fathi, Alireza and Wojna, Zbigniew and Rathod, Vivek and Wang, Peng and Song, Hyun Oh and Guadarrama, Sergio and Murphy, Kevin P.},
  year = 2017,
  month = mar,
  number = {arXiv:1703.10277},
  eprint = {1703.10277},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.10277},
  urldate = {2024-03-13},
  abstract = {We propose a new method for semantic instance segmentation, by first computing how likely two pixels are to belong to the same object, and then by grouping similar pixels together. Our similarity metric is based on a deep, fully convolutional embedding model. Our grouping method is based on selecting all points that are sufficiently similar to a set of "seed points", chosen from a deep, fully convolutional scoring model. We show competitive results on the Pascal VOC instance segmentation benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\ZWPSX3MD\\Fathi et al. - 2017 - Semantic Instance Segmentation via Deep Metric Lea.pdf;C\:\\Users\\E097600\\Zotero\\storage\\C9ZUQ3I7\\1703.html}
}

@misc{fengALBenchFrameworkEvaluating2022,
  title = {{{ALBench}}: {{A Framework}} for {{Evaluating Active Learning}} in {{Object Detection}}},
  shorttitle = {{{ALBench}}},
  author = {Feng, Zhanpeng and Zhang, Shiliang and Takezoe, Rinyoichi and Hu, Wenze and Chandraker, Manmohan and Li, Li-Jia and Narayanan, Vijay K. and Wang, Xiaoyu},
  year = 2022,
  month = nov,
  number = {arXiv:2207.13339},
  eprint = {2207.13339},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.13339},
  urldate = {2025-09-18},
  abstract = {Active learning is an important technology for automated machine learning systems. In contrast to Neural Architecture Search (NAS) which aims at automating neural network architecture design, active learning aims at automating training data selection. It is especially critical for training a long-tailed task, in which positive samples are sparsely distributed. Active learning alleviates the expensive data annotation issue through incrementally training models powered with efficient data selection. Instead of annotating all unlabeled samples, it iteratively selects and annotates the most valuable samples. Active learning has been popular in image classification, but has not been fully explored in object detection. Most of current approaches on object detection are evaluated with different settings, making it difficult to fairly compare their performance. To facilitate the research in this field, this paper contributes an active learning benchmark framework named as ALBench for evaluating active learning in object detection. Developed on an automatic deep model training system, this ALBench framework is easy-to-use, compatible with different active learning algorithms, and ensures the same training and testing protocols. We hope this automated benchmark system help researchers to easily reproduce literature's performance and have objective comparisons with prior arts. The code will be release through Github.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Benchmark testing,Computer Science - Computer Vision and Pattern Recognition,Object detection},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\AU2AJ7X6\\Feng et al. - 2022 - ALBench A Framework for Evaluating Active Learning in Object Detection.pdf;C\:\\Users\\E097600\\Zotero\\storage\\8B4ISZME\\2207.html}
}

@article{filippiniStochasticApproachScheduling2024,
  title = {A {{Stochastic Approach}} for {{Scheduling AI Training Jobs}} in {{GPU-Based Systems}}},
  author = {Filippini, Federica and Anselmi, Jonatha and Ardagna, Danilo and Gaujal, Bruno},
  year = 2024,
  month = jan,
  journal = {IEEE Transactions on Cloud Computing},
  volume = {12},
  number = {1},
  pages = {53--69},
  issn = {2168-7161},
  doi = {10.1109/TCC.2023.3336540},
  urldate = {2025-11-27},
  abstract = {In this work, we optimize the scheduling of Deep Learning (DL) training jobs from the perspective of a Cloud Service Provider running a data center, which efficiently selects resources for the execution of each job to minimize the average energy consumption while satisfying time constraints. To model the problem, we first develop a Mixed-Integer Non-Linear Programming formulation. Unfortunately, the computation of an optimal solution is prohibitively expensive, and to overcome this difficulty, we design a heuristic STochastic Scheduler (STS). Exploiting the probability distribution of early termination, STS determines how to adapt the resource assignment during the execution of the jobs to minimize the expected energy cost while meeting the job due dates. The results of an extensive experimental evaluation show that STS guarantees significantly better results than other methods in the literature, effectively avoiding due date violations and yielding a percentage total cost reduction between 32\% and 80\% on average. We also prove the applicability of our method in real-world scenarios, as obtaining optimal schedules for systems of up to 100 nodes and 400 concurrent jobs requires less than 5 seconds. Finally, we evaluated the effectiveness of GPU sharing, i.e., running multiple jobs in a single GPU. The obtained results demonstrate that depending on the workload and GPU memory, this further reduces the energy cost by 17--29\% on average.},
  keywords = {Average energy consumption minimization,Cloud computing,Costs,deep learning,Energy consumption,GPU cluster,GPU sharing,Graphics processing units,job tardiness,Optimal scheduling,scheduling,Stochastic processes,Training},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\3S943SBF\\Filippini et al. - 2024 - A Stochastic Approach for Scheduling AI Training Jobs in GPU-Based Systems.pdf;C\:\\Users\\E097600\\Zotero\\storage\\FTS5LBXY\\10328678.html}
}

@book{fordBuildingEvolutionaryArchitectures2022,
  title = {Building {{Evolutionary Architectures}}: {{Automated Software Governance}}},
  shorttitle = {Building {{Evolutionary Architectures}}},
  author = {Ford, Neal and Parsons, Rebecca and Kua, Patrick and Sadalage, Pramod},
  year = 2022,
  month = dec,
  edition = {2nd edition},
  publisher = {O'Reilly Media},
  address = {Beijing Boston Farnham Sebastopol Tokyo},
  abstract = {The software development ecosystem is constantly changing, providing a constant stream of new tools, frameworks, techniques, and paradigms. Over the past few years, incremental developments in core engineering practices for software development have created the foundations for rethinking how architecture changes over time, along with ways to protect important architectural characteristics as it evolves. This practical guide ties those parts together with a new way to think about architecture and time.},
  isbn = {978-1-4920-9754-9},
  langid = {english},
  keywords = {book},
  file = {C:\Users\E097600\Zotero\storage\CIPQ6MTM\Ford et al. - 2022 - Building Evolutionary Architectures Automated Software Governance.pdf}
}

@misc{fuchsgruberUncertaintyActiveLearning2024,
  title = {Uncertainty for {{Active Learning}} on {{Graphs}}},
  author = {Fuchsgruber, Dominik and Wollschl{\"a}ger, Tom and Charpentier, Bertrand and Oroz, Antonio and G{\"u}nnemann, Stephan},
  year = 2024,
  month = aug,
  number = {arXiv:2405.01462},
  eprint = {2405.01462},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.01462},
  urldate = {2025-01-16},
  abstract = {Uncertainty Sampling is an Active Learning strategy that aims to improve the data efficiency of machine learning models by iteratively acquiring labels of data points with the highest uncertainty. While it has proven effective for independent data its applicability to graphs remains under-explored. We propose the first extensive study of Uncertainty Sampling for node classification: (1) We benchmark Uncertainty Sampling beyond predictive uncertainty and highlight a significant performance gap to other Active Learning strategies. (2) We develop ground-truth Bayesian uncertainty estimates in terms of the data generating process and prove their effectiveness in guiding Uncertainty Sampling toward optimal queries. We confirm our results on synthetic data and design an approximate approach that consistently outperforms other uncertainty estimators on real datasets. (3) Based on this analysis, we relate pitfalls in modeling uncertainty to existing methods. Our analysis enables and informs the development of principled uncertainty estimation on graphs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\P9S2Z2YA\\Fuchsgruber et al. - 2024 - Uncertainty for Active Learning on Graphs.pdf;C\:\\Users\\E097600\\Zotero\\storage\\8JJPQ225\\2405.html}
}

@article{fuTransferableQuerySelection2021,
  title = {Transferable {{Query Selection}} for {{Active Domain Adaptation}}},
  author = {Fu, Bo and Cao, Zhangjie and Wang, Jianmin and Long, Mingsheng},
  year = 2021,
  month = jun,
  journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {7268--7277},
  publisher = {IEEE},
  address = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.00719},
  urldate = {2025-09-26},
  abstract = {Unsupervised domain adaptation (UDA) enables transferring knowledge from a related source domain to a fully unlabeled target domain. Despite the significant advances in UDA, the performance gap remains quite large between UDA and supervised learning with fully labeled target data. Active domain adaptation (ADA) mitigates the gap under minimal annotation cost by selecting a small quota of target samples to annotate and incorporating them into training. Due to the domain shift, the query selection criteria of prior active learning methods may be ineffective to select the most informative target samples for annotation. In this paper, we propose Transferable Query Selection (TQS), which selects the most informative samples under domain shift by an ensemble of three new criteria: transferable committee, transferable uncertainty, and transferable domainness. We further develop a randomized selection algorithm to enhance the diversity of the selected samples. Experiments show that TQS remarkably outperforms previous UDA and ADA methods on several domain adaptation datasets. Deeper analyses demonstrate that TQS can select the most informative target samples under the domain shift.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {9781665445092},
  file = {C:\Users\E097600\Zotero\storage\VAC76K9R\Fu et al. - 2021 - Transferable Query Selection for Active Domain Adaptation.pdf}
}

@book{g.p.tolstovFourierSeries1962,
  title = {Fourier {{Series}}},
  author = {{G.P. Tolstov}},
  year = 1962,
  publisher = {Mir Publishers},
  urldate = {2025-01-06},
  abstract = {The present volume is the second in a new series oftranslations of outstanding Russian textbooks andmonographs in the fields of mathematics, physics andengineering, under my editorship. It is hoped thatProfessor Tolstov's book will constitute a valuable additionto the English-language literature on Fourier series.vi t r a n s l a t o r 's p r e f a c eThe following two changes, made with ProfessorTolstov's consent, are worth mentioning:1. To enhance the value of the English-languageedition, a large number of extra problems have beenadded by myself and Professor Allen L. Shields of theUniversity of Michigan. We have consulted a varietyof sources, in particular, A Collection o f Problems inMathematical Physics by N. N. Lebedev, I. P. Skalskaya,and Y. S. Uflyand (Moscow, 1957), from whichmost of the problems appearing at the end of Chapter9 have been taken.2. To keep the number of cross references to aminimum, four chapters (8 and 9, 10 and 11) of theRussian original have been combined to make twochapters (8 and 9) of the present edition.I have also added a Bibliography, containing suggestionsfor collateral and supplementary reading. Finally, itshould be noted that sections marked with asteriskscontain material of a more advanced nature, which can beomitted without loss of continuity.},
  langid = {english},
  keywords = {book,mathematics},
  file = {C:\Users\E097600\Zotero\storage\HMAKJTU6\G.P. Tolstov - 1962 - Fourier Series.pdf}
}

@misc{galDeepBayesianActive2017,
  title = {Deep {{Bayesian Active Learning}} with {{Image Data}}},
  author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
  year = 2017,
  month = mar,
  number = {arXiv:1703.02910},
  eprint = {1703.02910},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.02910},
  urldate = {2024-03-11},
  abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Image classification,mnist,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\KD5LU564\\Gal et al. - 2017 - Deep Bayesian Active Learning with Image Data.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ZDB9SBGF\\1703.html}
}

@inproceedings{galUncertaintyDeepLearning2016,
  title = {Uncertainty in {{Deep Learning}}},
  author = {Gal, Y.},
  year = 2016,
  urldate = {2025-09-17},
  abstract = {Deep learning has attracted tremendous attention from researchers in various fields of information engineering such as AI, computer vision, and language processing [Kalchbrenner and Blunsom, 2013; Krizhevsky et al., 2012; Mnih et al., 2013], but also from more traditional sciences such as physics, biology, and manufacturing [Anjos et al., 2015; Baldi et al., 2014; Bergmann et al., 2014]. Neural networks, image processing tools such as convolutional neural networks, sequence processing models such as recurrent neural networks, and regularisation tools such as dropout, are used extensively. However, fields such as physics, biology, and manufacturing are ones in which representing model uncertainty is of crucial importance [Ghahramani, 2015; Krzywinski and Altman, 2013]. With the recent shift in many of these fields towards the use of Bayesian uncertainty [Herzog and Ostwald, 2013; Nuzzo, 2014; Trafimow and Marks, 2015], new needs arise from deep learning. In this work we develop tools to obtain practical uncertainty estimates in deep learning, casting recent deep learning tools as Bayesian models without changing either the models or the optimisation. In the first part of this thesis we develop the theory for such tools, providing applications and illustrative examples. We tie approximate inference in Bayesian models to dropout and other stochastic regularisation techniques, and assess the approximations empirically. We give example applications arising from this connection between modern deep learning and Bayesian modelling such as active learning of image data and data efficient deep reinforcement learning. We further demonstrate the method's practicality through a survey of recent applications making use of the suggested tools in language applications, medical diagnostics, bioinformatics, image processing, and autonomous driving. In the second part of the thesis we explore its theoretical implications, and the insights stemming from the link between Bayesian modelling and deep learning. We discuss what determines model uncertainty properties, analyse the approximate inference analytically in the linear case, and theoretically examine various priors such as spike and slab priors.},
  file = {C:\Users\E097600\Zotero\storage\ZFBIF3FC\Gal - 2016 - Uncertainty in Deep Learning.pdf}
}

@inproceedings{gammermanLearningTransduction1998,
  title = {Learning by Transduction},
  booktitle = {Proceedings of the {{Fourteenth}} Conference on {{Uncertainty}} in Artificial Intelligence},
  author = {Gammerman, A. and Vovk, V. and Vapnik, V.},
  year = 1998,
  month = jul,
  series = {{{UAI}}'98},
  pages = {148--155},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  urldate = {2025-09-10},
  abstract = {We describe a method for predicting a classification of an object given classifications of the objects in the training set, assuming that the pairs object/classification are generated by an i.i.d. process from a continuous probability distribution. Our method is a modification of Vapnik's support-vector machine; its main novelty is that it gives not only the prediction itself but also a practicable measure of the evidence found in support of that prediction. We also describe a procedure for assigning degrees of confidence to predictions made by the support vector machine. Some experimental results are presented, and possible extensions of the algorithms are discussed.},
  isbn = {978-1-55860-555-8},
  keywords = {Conformal prediction},
  file = {C:\Users\E097600\Zotero\storage\MTF3P3BX\Gammerman et al. - 1998 - Learning by transduction.pdf}
}

@misc{gargDomainAdaptationOpen2022,
  title = {Domain {{Adaptation}} under {{Open Set Label Shift}}},
  author = {Garg, Saurabh and Balakrishnan, Sivaraman and Lipton, Zachary C.},
  year = 2022,
  month = oct,
  number = {arXiv:2207.13048},
  eprint = {2207.13048},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.13048},
  urldate = {2025-09-22},
  abstract = {We introduce the problem of domain adaptation under Open Set Label Shift (OSLS) where the label distribution can change arbitrarily and a new class may arrive during deployment, but the class-conditional distributions p(x\textbar y) are domain-invariant. OSLS subsumes domain adaptation under label shift and Positive-Unlabeled (PU) learning. The learner's goals here are two-fold: (a) estimate the target label distribution, including the novel class; and (b) learn a target classifier. First, we establish necessary and sufficient conditions for identifying these quantities. Second, motivated by advances in label shift and PU learning, we propose practical methods for both tasks that leverage black-box predictors. Unlike typical Open Set Domain Adaptation (OSDA) problems, which tend to be ill-posed and amenable only to heuristics, OSLS offers a well-posed problem amenable to more principled machinery. Experiments across numerous semi-synthetic benchmarks on vision, language, and medical datasets demonstrate that our methods consistently outperform OSDA baselines, achieving 10--25\% improvements in target domain accuracy. Finally, we analyze the proposed methods, establishing finite-sample convergence to the true label marginal and convergence to optimal classifier for linear models in a Gaussian setup. Code is available at https://github.com/acmi-lab/Open-Set-Label-Shift.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,domain adaptation,label shift,open set},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\P7QKTR5V\\Garg et al. - 2022 - Domain Adaptation under Open Set Label Shift.pdf;C\:\\Users\\E097600\\Zotero\\storage\\HNXC46CZ\\2207.html}
}

@misc{gashiDeepActiveLearning2024,
  title = {Deep {{Active Learning}}: {{A Reality Check}}},
  shorttitle = {Deep {{Active Learning}}},
  author = {Gashi, Edrina and Deng, Jiankang and Elezi, Ismail},
  year = 2024,
  month = mar,
  number = {arXiv:2403.14800},
  eprint = {2403.14800},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.14800},
  urldate = {2025-09-23},
  abstract = {We conduct a comprehensive evaluation of state-of-the-art deep active learning methods. Surprisingly, under general settings, no single-model method decisively outperforms entropy-based active learning, and some even fall short of random sampling. We delve into overlooked aspects like starting budget, budget step, and pretraining's impact, revealing their significance in achieving superior results. Additionally, we extend our evaluation to other tasks, exploring the active learning effectiveness in combination with semi-supervised learning, and object detection. Our experiments provide valuable insights and concrete recommendations for future active learning studies. By uncovering the limitations of current methods and understanding the impact of different experimental settings, we aim to inspire more efficient training of deep learning models in real-world scenarios with limited annotation budgets. This work contributes to advancing active learning's efficacy in deep learning and empowers researchers to make informed decisions when applying active learning to their tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\U3P2SADH\\Gashi et al. - 2024 - Deep Active Learning A Reality Check.pdf;C\:\\Users\\E097600\\Zotero\\storage\\5ZUH3LCV\\2403.html}
}

@misc{gauthierEValuesExpandScope2025,
  title = {E-{{Values Expand}} the {{Scope}} of {{Conformal Prediction}}},
  author = {Gauthier, Etienne and Bach, Francis and Jordan, Michael I.},
  year = 2025,
  month = mar,
  number = {arXiv:2503.13050},
  eprint = {2503.13050},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.13050},
  urldate = {2025-03-26},
  abstract = {Conformal prediction is a powerful framework for distribution-free uncertainty quantification. The standard approach to conformal prediction relies on comparing the ranks of prediction scores: under exchangeability, the rank of a future test point cannot be too extreme relative to a calibration set. This rank-based method can be reformulated in terms of p-values. In this paper, we explore an alternative approach based on e-values, known as conformal e-prediction. E-values offer key advantages that cannot be achieved with p-values, enabling new theoretical and practical capabilities. In particular, we present three applications that leverage the unique strengths of e-values: batch anytime-valid conformal prediction, fixed-size conformal sets with data-dependent coverage, and conformal prediction under ambiguous ground truth. Overall, these examples demonstrate that e-value-based constructions provide a flexible expansion of the toolbox of conformal prediction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\HC93HDA6\\Gauthier et al. - 2025 - E-Values Expand the Scope of Conformal Prediction.pdf;C\:\\Users\\E097600\\Zotero\\storage\\8Z2W3RUI\\2503.html}
}

@article{gawlikowskiSurveyUncertaintyDeep2023,
  title = {A Survey of Uncertainty in Deep Neural Networks},
  author = {Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and Shahzad, Muhammad and Yang, Wen and Bamler, Richard and Zhu, Xiao Xiang},
  year = 2023,
  month = oct,
  journal = {Artificial Intelligence Review},
  volume = {56},
  number = {1},
  pages = {1513--1589},
  issn = {1573-7462},
  doi = {10.1007/s10462-023-10562-9},
  urldate = {2025-01-09},
  abstract = {Over the last decade, neural networks have reached almost every field of science and become a crucial part of various real world applications. Due to the increasing spread, confidence in neural network predictions has become more and more important. However, basic neural networks do not deliver certainty estimates or suffer from over- or under-confidence, i.e. are badly calibrated. To overcome this, many researchers have been working on understanding and quantifying uncertainty in a neural network's prediction. As a result, different types and sources of uncertainty have been identified and various approaches to measure and quantify uncertainty in neural networks have been proposed. This work gives a comprehensive overview of uncertainty estimation in neural networks, reviews recent advances in the field, highlights current challenges, and identifies potential research opportunities. It is intended to give anyone interested in uncertainty estimation in neural networks a broad overview and introduction, without presupposing prior knowledge in this field. For that, a comprehensive introduction to the most crucial sources of uncertainty is given and their separation into reducible model uncertainty and irreducible data uncertainty is presented. The modeling of these uncertainties based on deterministic neural networks, Bayesian neural networks (BNNs), ensemble of neural networks, and test-time data augmentation approaches is introduced and different branches of these fields as well as the latest developments are discussed. For a practical application, we discuss different measures of uncertainty, approaches for calibrating neural networks, and give an overview of existing baselines and available implementations. Different examples from the wide spectrum of challenges in the fields of medical image analysis, robotics, and earth observation give an idea of the needs and challenges regarding uncertainties in the practical applications of neural networks. Additionally, the practical limitations of uncertainty quantification methods in neural networks for mission- and safety-critical real world applications are discussed and an outlook on the next steps towards a broader usage of such methods is given.},
  langid = {english},
  keywords = {Artificial Intelligence,Bayesian deep neural networks,Calibration,Ensembles,Test-time augmentation,Uncertainty},
  file = {C:\Users\E097600\Zotero\storage\ZL3JDT4T\Gawlikowski et al. - 2023 - A survey of uncertainty in deep neural networks.pdf}
}

@misc{gazinSelectingInformativeConformal2024,
  title = {Selecting Informative Conformal Prediction Sets with False Coverage Rate Control},
  author = {Gazin, Ulysse and Heller, Ruth and Marandon, Ariane and Roquain, Etienne},
  year = 2024,
  month = nov,
  number = {arXiv:2403.12295},
  eprint = {2403.12295},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.12295},
  urldate = {2025-01-27},
  abstract = {In supervised learning, including regression and classification, conformal methods provide prediction sets for the outcome/label with finite sample coverage for any machine learning predictor. We consider here the case where such prediction sets come after a selection process. The selection process requires that the selected prediction sets be `informative' in a well defined sense. We consider both the classification and regression settings where the analyst may consider as informative only the sample with prediction sets small enough, excluding null values, or obeying other appropriate `monotone' constraints. We develop a unified framework for building such informative conformal prediction sets while controlling the false coverage rate (FCR) on the selected sample. While conformal prediction sets after selection have been the focus of much recent literature in the field, the new introduced procedures, called InfoSP and InfoSCOP, are to our knowledge the first ones providing FCR control for informative prediction sets. We show the usefulness of our resulting procedures on real and simulated data.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\ARKS3T45\\Gazin et al. - 2024 - Selecting informative conformal prediction sets with false coverage rate control.pdf;C\:\\Users\\E097600\\Zotero\\storage\\AQVT7BKJ\\2403.html}
}

@misc{geifmanDeepActiveLearning2017,
  title = {Deep {{Active Learning}} over the {{Long Tail}}},
  author = {Geifman, Yonatan and {El-Yaniv}, Ran},
  year = 2017,
  month = nov,
  number = {arXiv:1711.00941},
  eprint = {1711.00941},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.00941},
  urldate = {2025-11-28},
  abstract = {This paper is concerned with pool-based active learning for deep neural networks. Motivated by coreset dataset compression ideas, we present a novel active learning algorithm that queries consecutive points from the pool using farthest-first traversals in the space of neural activation over a representation layer. We show consistent and overwhelming improvement in sample complexity over passive learning (random sampling) for three datasets: MNIST, CIFAR-10, and CIFAR-100. In addition, our algorithm outperforms the traditional uncertainty sampling technique (obtained using softmax activations), and we identify cases where uncertainty sampling is only slightly better than random sampling.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,Image classification},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\3X7VPIU3\\Geifman et El-Yaniv - 2017 - Deep Active Learning over the Long Tail.pdf;C\:\\Users\\E097600\\Zotero\\storage\\VJNQT4KN\\1711.html}
}

@misc{geifmanSelectiveClassificationDeep2017,
  title = {Selective {{Classification}} for {{Deep Neural Networks}}},
  author = {Geifman, Yonatan and {El-Yaniv}, Ran},
  year = 2017,
  month = jun,
  number = {arXiv:1705.08500},
  eprint = {1705.08500},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.08500},
  urldate = {2025-11-28},
  abstract = {Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2\% error in top-5 ImageNet classification can be guaranteed with probability 99.9\%, and almost 60\% test coverage.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\JVN3DVKW\\Geifman et El-Yaniv - 2017 - Selective Classification for Deep Neural Networks.pdf;C\:\\Users\\E097600\\Zotero\\storage\\CKCT3FMG\\1705.html}
}

@misc{ghiasiSimpleCopyPasteStrong2021,
  title = {Simple {{Copy-Paste}} Is a {{Strong Data Augmentation Method}} for {{Instance Segmentation}}},
  author = {Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D. and Le, Quoc V. and Zoph, Barret},
  year = 2021,
  month = jun,
  number = {arXiv:2012.07177},
  eprint = {2012.07177},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.07177},
  urldate = {2024-03-06},
  abstract = {Building instance segmentation models that are data-efficient and can handle rare object categories is an important challenge in computer vision. Leveraging data augmentations is a promising direction towards addressing this challenge. Here, we perform a systematic study of the Copy-Paste augmentation ([13, 12]) for instance segmentation where we randomly paste objects onto an image. Prior studies on Copy-Paste relied on modeling the surrounding visual context for pasting the objects. However, we find that the simple mechanism of pasting objects randomly is good enough and can provide solid gains on top of strong baselines. Furthermore, we show Copy-Paste is additive with semi-supervised methods that leverage extra data through pseudo labeling (e.g. self-training). On COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an improvement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art. We further demonstrate that Copy-Paste can lead to significant improvements on the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge winning entry by +3.6 mask AP on rare categories.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\E097600\Zotero\storage\AUJWKXCB\2012.html}
}

@misc{girshickFastRCNN2015,
  title = {Fast {{R-CNN}}},
  author = {Girshick, Ross},
  year = 2015,
  month = sep,
  number = {arXiv:1504.08083},
  eprint = {1504.08083},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1504.08083},
  urldate = {2024-12-18},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,modele},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\XRY2W8KF\\Girshick - 2015 - Fast R-CNN.pdf;C\:\\Users\\E097600\\Zotero\\storage\\FYU76DD5\\1504.html}
}

@misc{girshickRichFeatureHierarchies2014,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = 2014,
  month = oct,
  number = {arXiv:1311.2524},
  eprint = {1311.2524},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1311.2524},
  urldate = {2024-12-19},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/\textasciitilde rbg/rcnn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\VPT4GMAM\\Girshick et al. - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\PDABNWB6\\1311.html}
}

@misc{gladstoneEnergyBasedTransformersAre2025,
  title = {Energy-{{Based Transformers}} Are {{Scalable Learners}} and {{Thinkers}}},
  author = {Gladstone, Alexi and Nanduru, Ganesh and Islam, Md Mofijul and Han, Peixuan and Ha, Hyeonjeong and Chadha, Aman and Du, Yilun and Ji, Heng and Li, Jundong and Iqbal, Tariq},
  year = 2025,
  month = jul,
  number = {arXiv:2507.02092},
  eprint = {2507.02092},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.02092},
  urldate = {2025-07-28},
  abstract = {Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable domains like math and coding), or require additional supervision/training on top of unsupervised pretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question "Is it possible to generalize these System 2 Thinking approaches, and develop models that learn to think solely from unsupervised learning?" Interestingly, we find the answer is yes, by learning to explicitly verify the compatibility between inputs and candidate-predictions, and then re-framing prediction problems as optimization with respect to this verifier. Specifically, we train Energy-Based Transformers (EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy value to every input and candidate-prediction pair, enabling predictions through gradient descent-based energy minimization until convergence. Across both discrete (text) and continuous (visual) modalities, we find EBTs scale faster than the dominant Transformer++ approach during training, achieving an up to 35\% higher scaling rate with respect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs improve performance with System 2 Thinking by 29\% more than the Transformer++ on language tasks, and EBTs outperform Diffusion Transformers on image denoising while using fewer forward passes. Further, we find that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, suggesting that EBTs generalize better than existing approaches. Consequently, EBTs are a promising new paradigm for scaling both the learning and thinking capabilities of models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\RBG2JMLZ\\Gladstone et al. - 2025 - Energy-Based Transformers are Scalable Learners and Thinkers.pdf;C\:\\Users\\E097600\\Zotero\\storage\\3DP6TZFS\\2507.html}
}

@misc{goelReliableActiveLearning2025,
  title = {Reliable {{Active Learning}} from {{Unreliable Labels}} via {{Neural Collapse Geometry}}},
  author = {Goel, Atharv and Agarwal, Sharat and Anand, Saket and Arora, Chetan},
  year = 2025,
  month = oct,
  number = {arXiv:2510.09740},
  eprint = {2510.09740},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.09740},
  urldate = {2025-10-14},
  abstract = {Active Learning (AL) promises to reduce annotation cost by prioritizing informative samples, yet its reliability is undermined when labels are noisy or when the data distribution shifts. In practice, annotators make mistakes, rare categories are ambiguous, and conventional AL heuristics (uncertainty, diversity) often amplify such errors by repeatedly selecting mislabeled or redundant samples. We propose Reliable Active Learning via Neural Collapse Geometry (NCAL-R), a framework that leverages the emergent geometric regularities of deep networks to counteract unreliable supervision. Our method introduces two complementary signals: (i) a Class-Mean Alignment Perturbation score, which quantifies how candidate samples structurally stabilize or distort inter-class geometry, and (ii) a Feature Fluctuation score, which captures temporal instability of representations across training checkpoints. By combining these signals, NCAL-R prioritizes samples that both preserve class separation and highlight ambiguous regions, mitigating the effect of noisy or redundant labels. Experiments on ImageNet-100 and CIFAR100 show that NCAL-R consistently outperforms standard AL baselines, achieving higher accuracy with fewer labels, improved robustness under synthetic label noise, and stronger generalization to out-of-distribution data. These results suggest that incorporating geometric reliability criteria into acquisition decisions can make Active Learning less brittle to annotation errors and distribution shifts, a key step toward trustworthy deployment in real-world labeling pipelines. Our code is available at https://github.com/Vision-IIITD/NCAL.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Image classification,label noise},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\HX5BKCVL\\Goel et al. - 2025 - Reliable Active Learning from Unreliable Labels via Neural Collapse Geometry.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ILMT2FQH\\2510.html}
}

@misc{gohActiveLabActiveLearning2023,
  title = {{{ActiveLab}}: {{Active Learning}} with {{Re-Labeling}} by {{Multiple Annotators}}},
  shorttitle = {{{ActiveLab}}},
  author = {Goh, Hui Wen and Mueller, Jonas},
  year = 2023,
  month = jan,
  number = {arXiv:2301.11856},
  eprint = {2301.11856},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.11856},
  urldate = {2025-02-13},
  abstract = {In real-world data labeling applications, annotators often provide imperfect labels. It is thus common to employ multiple annotators to label data with some overlap between their examples. We study active learning in such settings, aiming to train an accurate classifier by collecting a dataset with the fewest total annotations. Here we propose ActiveLab, a practical method to decide what to label next that works with any classifier model and can be used in pool-based batch active learning with one or multiple annotators. ActiveLab automatically estimates when it is more informative to re-label examples vs. labeling entirely new ones. This is a key aspect of producing high quality labels and trained models within a limited annotation budget. In experiments on image and tabular data, ActiveLab reliably trains more accurate classifiers with far fewer annotations than a wide variety of popular active learning methods.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,label noise,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\8BUA3QBJ\\Goh et Mueller - 2023 - ActiveLab Active Learning with Re-Labeling by Multiple Annotators.pdf;C\:\\Users\\E097600\\Zotero\\storage\\UXVPXJ85\\2301.html}
}

@misc{golestanehImportanceSelfConsistencyActive2020,
  title = {Importance of {{Self-Consistency}} in {{Active Learning}} for {{Semantic Segmentation}}},
  author = {Golestaneh, S. Alireza and Kitani, Kris M.},
  year = 2020,
  month = aug,
  number = {arXiv:2008.01860},
  eprint = {2008.01860},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.01860},
  urldate = {2025-10-28},
  abstract = {We address the task of active learning in the context of semantic segmentation and show that self-consistency can be a powerful source of self-supervision to greatly improve the performance of a data-driven model with access to only a small amount of labeled data. Self-consistency uses the simple observation that the results of semantic segmentation for a specific image should not change under transformations like horizontal flipping (i.e., the results should only be flipped). In other words, the output of a model should be consistent under equivariant transformations. The self-supervisory signal of self-consistency is particularly helpful during active learning since the model is prone to overfitting when there is only a small amount of labeled training data. In our proposed active learning framework, we iteratively extract small image patches that need to be labeled, by selecting image patches that have high uncertainty (high entropy) under equivariant transformations. We enforce pixel-wise self-consistency between the outputs of segmentation network for each image and its transformation (horizontally flipped) to utilize the rich self-supervisory information and reduce the uncertainty of the network. In this way, we are able to find the image patches over which the current model struggles the most to classify. By iteratively training over these difficult image patches, our experiments show that our active learning approach reaches \$\textbackslash sim96\textbackslash\%\$ of the top performance of a model trained on all data, by using only \$12\textbackslash\%\$ of the total data on benchmark semantic segmentation datasets (e.g., CamVid and Cityscapes).},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Computer Vision and Pattern Recognition,semantic segmentation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\8E2MVXLY\\Golestaneh et Kitani - 2020 - Importance of Self-Consistency in Active Learning for Semantic Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\XKUVXW26\\2008.html}
}

@misc{gordon-rodriguezUsesAbusesCrossEntropy2020,
  title = {Uses and {{Abuses}} of the {{Cross-Entropy Loss}}: {{Case Studies}} in {{Modern Deep Learning}}},
  shorttitle = {Uses and {{Abuses}} of the {{Cross-Entropy Loss}}},
  author = {{Gordon-Rodriguez}, Elliott and {Loaiza-Ganem}, Gabriel and Pleiss, Geoff and Cunningham, John P.},
  year = 2020,
  month = nov,
  number = {arXiv:2011.05231},
  eprint = {2011.05231},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.05231},
  urldate = {2025-02-07},
  abstract = {Modern deep learning is primarily an experimental science, in which empirical advances occasionally come at the expense of probabilistic rigor. Here we focus on one such example; namely the use of the categorical cross-entropy loss to model data that is not strictly categorical, but rather takes values on the simplex. This practice is standard in neural network architectures with label smoothing and actor-mimic reinforcement learning, amongst others. Drawing on the recently discovered continuous-categorical distribution, we propose probabilistically-inspired alternatives to these models, providing an approach that is more principled and theoretically appealing. Through careful experimentation, including an ablation study, we identify the potential for outperformance in these models, thereby highlighting the importance of a proper probabilistic treatment, as well as illustrating some of the failure modes thereof.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\X49PD2DR\\Gordon-Rodriguez et al. - 2020 - Uses and Abuses of the Cross-Entropy Loss Case Studies in Modern Deep Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\LGM2KFSR\\2011.html}
}

@misc{greerWhyWhenHow2024,
  title = {The {{Why}}, {{When}}, and {{How}} to {{Use Active Learning}} in {{Large-Data-Driven 3D Object Detection}} for {{Safe Autonomous Driving}}: {{An Empirical Exploration}}},
  shorttitle = {The {{Why}}, {{When}}, and {{How}} to {{Use Active Learning}} in {{Large-Data-Driven 3D Object Detection}} for {{Safe Autonomous Driving}}},
  author = {Greer, Ross and Antoniussen, Bj{\o}rk and Andersen, Mathias V. and M{\o}gelmose, Andreas and Trivedi, Mohan M.},
  year = 2024,
  month = jan,
  number = {arXiv:2401.16634},
  eprint = {2401.16634},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.16634},
  urldate = {2024-03-08},
  abstract = {Active learning strategies for 3D object detection in autonomous driving datasets may help to address challenges of data imbalance, redundancy, and high-dimensional data. We demonstrate the effectiveness of entropy querying to select informative samples, aiming to reduce annotation costs and improve model performance. We experiment using the BEVFusion model for 3D object detection on the nuScenes dataset, comparing active learning to random sampling and demonstrating that entropy querying outperforms in most cases. The method is particularly effective in reducing the performance gap between majority and minority classes. Class-specific analysis reveals efficient allocation of annotated resources for limited data budgets, emphasizing the importance of selecting diverse and informative data for model training. Our findings suggest that entropy querying is a promising strategy for selecting data that enhances model learning in resource-constrained environments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\6WFKGESM\\Greer et al. - 2024 - The Why, When, and How to Use Active Learning in L.pdf;C\:\\Users\\E097600\\Zotero\\storage\\5G9PRVTZ\\2401.html}
}

@misc{griffinAutoLabelingDataObject2025,
  title = {Auto-{{Labeling Data}} for {{Object Detection}}},
  author = {Griffin, Brent A. and Gangwar, Manushree and Sela, Jacob and Corso, Jason J.},
  year = 2025,
  month = jun,
  journal = {arXiv.org},
  urldate = {2025-06-11},
  abstract = {Great labels make great models. However, traditional labeling approaches for tasks like object detection have substantial costs at scale. Furthermore, alternatives to fully-supervised object detection either lose functionality or require larger models with prohibitive computational costs for inference at scale. To that end, this paper addresses the problem of training standard object detection models without any ground truth labels. Instead, we configure previously-trained vision-language foundation models to generate application-specific pseudo "ground truth" labels. These auto-generated labels directly integrate with existing model training frameworks, and we subsequently train lightweight detection models that are computationally efficient. In this way, we avoid the costs of traditional labeling, leverage the knowledge of vision-language models, and keep the efficiency of lightweight models for practical application. We perform exhaustive experiments across multiple labeling configurations, downstream inference models, and datasets to establish best practices and set an extensive auto-labeling benchmark. From our results, we find that our approach is a viable alternative to standard labeling in that it maintains competitive performance on multiple datasets and substantially reduces labeling time and costs.},
  howpublished = {https://arxiv.org/abs/2506.02359v1},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\HNS89XSS\Griffin et al. - 2025 - Auto-Labeling Data for Object Detection.pdf}
}

@article{grimbergEddyCurrentSensor2000,
  title = {Eddy Current Sensor for Non-Destructive Evaluation of Metallic Wires, Bars and Pipes},
  author = {Grimberg, R. and Savin, A. and Radu, E. and Chifan, S. M.},
  year = 2000,
  month = apr,
  journal = {Sensors and Actuators A: Physical},
  volume = {81},
  number = {1},
  pages = {224--226},
  issn = {0924-4247},
  doi = {10.1016/S0924-4247(99)00128-4},
  urldate = {2025-03-05},
  abstract = {This work presents a new type of eddy-current sensor used in non-destructive evaluation, being able to detect with equal sensitivity both long and short flaws, in any orientation. The proposed sensor is of send--receive encircling type, using for the control operation a rotating magnetic field.},
  keywords = {Discontinuity,Eddy current,Non-destructive evaluation,Probability of detection,Reliability,Rotating magnetic field},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\22ETD6VY\\1-s2.0-S0924424799001284-main.pdf;C\:\\Users\\E097600\\Zotero\\storage\\U9QKVKBT\\Grimberg et al. - 2000 - Eddy current sensor for non-destructive evaluation of metallic wires, bars and pipes.pdf;C\:\\Users\\E097600\\Zotero\\storage\\KR9JFCVA\\S0924424799001284.html}
}

@article{guanPredictionOutlierDetection2022,
  title = {Prediction and Outlier Detection in Classification Problems},
  author = {Guan, Leying and Tibshirani, Robert},
  year = 2022,
  month = apr,
  journal = {Journal of the Royal Statistical Society. Series B, Statistical Methodology},
  volume = {84},
  number = {2},
  pages = {524--546},
  issn = {1369-7412},
  doi = {10.1111/rssb.12443},
  urldate = {2025-11-05},
  abstract = {We consider the multi-class classification problem when the training data and the out-of-sample test data may have different distributions and propose a method called BCOPS (balanced and conformal optimized prediction sets). BCOPS constructs a prediction set C(x) as a subset of class labels, possibly empty. It tries to optimize the out-of-sample performance, aiming to include the correct class and to detect outliers x as often as possible. BCOPS returns no prediction (corresponding to C(x) equal to the empty set) if it infers x to be an outlier. The proposed method combines supervised learning algorithms with conformal prediction to minimize a misclassification loss averaged over the out-of-sample distribution. The constructed prediction sets have a finite sample coverage guarantee without distributional assumptions. We also propose a method to estimate the outlier detection rate of a given procedure. We prove asymptotic consistency and optimality of our proposals under suitable assumptions and illustrate our methods on real data examples.},
  pmcid = {PMC9305480},
  pmid = {35910400},
  keywords = {Conformal prediction,label shift,Out-of-Distribution Detection},
  file = {C:\Users\E097600\Zotero\storage\G8YA5HE6\Guan et Tibshirani - 2022 - Prediction and outlier detection in classification problems.pdf}
}

@misc{guidezERDEEntropyRegularizedDistillation2025,
  title = {{{ERDE}}: {{Entropy-Regularized Distillation}} for {{Early-exit}}},
  shorttitle = {{{ERDE}}},
  author = {Guidez, Martial and Duffner, Stefan and Alpou, Yannick and R{\"o}th, Oscar and Garcia, Christophe},
  year = 2025,
  month = oct,
  number = {arXiv:2510.04856},
  eprint = {2510.04856},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.04856},
  urldate = {2025-11-27},
  abstract = {Although deep neural networks and in particular Convolutional Neural Networks have demonstrated state-of-the-art performance in image classification with relatively high efficiency, they still exhibit high computational costs, often rendering them impractical for real-time and edge applications. Therefore, a multitude of compression techniques have been developed to reduce these costs while maintaining accuracy. In addition, dynamic architectures have been introduced to modulate the level of compression at execution time, which is a desirable property in many resource-limited application scenarios. The proposed method effectively integrates two well-established optimization techniques: early exits and knowledge distillation, where a reduced student early-exit model is trained from a more complex teacher early-exit model. The primary contribution of this research lies in the approach for training the student early-exit model. In comparison to the conventional Knowledge Distillation loss, our approach incorporates a new entropy-based loss for images where the teacher's classification was incorrect. The proposed method optimizes the trade-off between accuracy and efficiency, thereby achieving significant reductions in computational complexity without compromising classification performance. The validity of this approach is substantiated by experimental results on image classification datasets CIFAR10, CIFAR100 and SVHN, which further opens new research perspectives for Knowledge Distillation in other contexts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\YWQMHTPA\\Guidez et al. - 2025 - ERDE Entropy-Regularized Distillation for Early-exit.pdf;C\:\\Users\\E097600\\Zotero\\storage\\7WWIQTAR\\2510.html}
}

@article{gunnarTopologyData2009,
  title = {Topology and Data},
  author = {{gunnar}, Carlsson},
  year = 2009,
  month = apr,
  journal = {BULLETIN (New Series) OF THE AMERICAN MATHEMATICAL SOCIETY},
  volume = {46},
  number = {2},
  pages = {255--308},
  doi = {S 0273-0979(09)01249-X},
  urldate = {2024-07-22},
  langid = {english},
  keywords = {topology},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\AP6VGS35\\[PDF] Topology and data  Semantic Scholar.pdf;C\:\\Users\\E097600\\Zotero\\storage\\E45RYLR8\\a4b603ca6aaaa18968e08ac1b0ee093db8a99a6b.html}
}

@inproceedings{guoCalibrationModernNeural2017,
  title = {On {{Calibration}} of {{Modern Neural Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  year = 2017,
  month = jul,
  pages = {1321--1330},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-10-02},
  abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
  langid = {english},
  keywords = {Calibration,Deep learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\GTR8V8W3\\Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ZGJM89PL\\Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf}
}

@misc{gupteRevisitingActiveLearning2024,
  title = {Revisiting {{Active Learning}} in the {{Era}} of {{Vision Foundation Models}}},
  author = {Gupte, Sanket Rajan and Aklilu, Josiah and Nirschl, Jeffrey J. and {Yeung-Levy}, Serena},
  year = 2024,
  month = jan,
  number = {arXiv:2401.14555},
  eprint = {2401.14555},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.14555},
  urldate = {2024-03-08},
  abstract = {Foundation vision or vision-language models are trained on large unlabeled or noisy data and learn robust representations that can achieve impressive zero- or few-shot performance on diverse tasks. Given these properties, they are a natural fit for active learning (AL), which aims to maximize labeling efficiency, but the full potential of foundation models has not been explored in the context of AL, specifically in the low-budget regime. In this work, we evaluate how foundation models influence three critical components of effective AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling, and 3) the trade-off between representative and uncertainty sampling. We systematically study how the robust representations of foundation models (DINOv2, OpenCLIP) challenge existing findings in active learning. Our observations inform the principled construction of a new simple and elegant AL strategy that balances uncertainty estimated via dropout with sample diversity. We extensively test our strategy on many challenging image classification benchmarks, including natural images as well as out-of-domain biomedical images that are relatively understudied in the AL literature. Source code will be made available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\KKJ3UZA3\\Gupte et al. - 2024 - Revisiting Active Learning in the Era of Vision Fo.pdf;C\:\\Users\\E097600\\Zotero\\storage\\94WIPZKX\\2401.html}
}

@misc{hacohenActiveLearningBudget2022,
  title = {Active {{Learning}} on a {{Budget}}: {{Opposite Strategies Suit High}} and {{Low Budgets}}},
  shorttitle = {Active {{Learning}} on a {{Budget}}},
  author = {Hacohen, Guy and Dekel, Avihu and Weinshall, Daphna},
  year = 2022,
  month = jun,
  number = {arXiv:2202.02794},
  eprint = {2202.02794},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.02794},
  urldate = {2024-03-11},
  abstract = {Investigating active learning, we focus on the relation between the number of labeled examples (budget size), and suitable querying strategies. Our theoretical analysis shows a behavior reminiscent of phase transition: typical examples are best queried when the budget is low, while unrepresentative examples are best queried when the budget is large. Combined evidence shows that a similar phenomenon occurs in common classification models. Accordingly, we propose TypiClust -- a deep active learning strategy suited for low budgets. In a comparative empirical investigation of supervised learning, using a variety of architectures and image datasets, TypiClust outperforms all other active learning strategies in the low-budget regime. Using TypiClust in the semi-supervised framework, performance gets an even more significant boost. In particular, state-of-the-art semi-supervised methods trained on CIFAR-10 with 10 labeled examples selected by TypiClust, reach 93.2\% accuracy -- an improvement of 39.4\% over random selection. Code is available at https://github.com/avihu111/TypiClust.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,Image classification,typiclust},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\X5NFQAZP\\Hacohen et al. - 2022 - Active Learning on a Budget Opposite Strategies S.pdf;C\:\\Users\\E097600\\Zotero\\storage\\E75IKHD9\\2202.html}
}

@misc{hacohenHowSelectWhich2023,
  title = {How to {{Select Which Active Learning Strategy}} Is {{Best Suited}} for {{Your Specific Problem}} and {{Budget}}},
  author = {Hacohen, Guy and Weinshall, Daphna},
  year = 2023,
  month = oct,
  number = {arXiv:2306.03543},
  eprint = {2306.03543},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.03543},
  urldate = {2024-03-26},
  abstract = {In the domain of Active Learning (AL), a learner actively selects which unlabeled examples to seek labels from an oracle, while operating within predefined budget constraints. Importantly, it has been recently shown that distinct query strategies are better suited for different conditions and budgetary constraints. In practice, the determination of the most appropriate AL strategy for a given situation remains an open problem. To tackle this challenge, we propose a practical derivative-based method that dynamically identifies the best strategy for a given budget. Intuitive motivation for our approach is provided by the theoretical analysis of a simplified scenario. We then introduce a method to dynamically select an AL strategy, which takes into account the unique characteristics of the problem and the available budget. Empirical results showcase the effectiveness of our approach across diverse budgets and computer vision tasks.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,Image classification},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\TAI9NETV\\Hacohen et Weinshall - 2023 - How to Select Which Active Learning Strategy is Be.pdf;C\:\\Users\\E097600\\Zotero\\storage\\KZ65WSVU\\2306.html;C\:\\Users\\E097600\\Zotero\\storage\\SGFXM3TV\\2306.html}
}

@misc{hadjadjPoolBasedActiveLearning2023,
  title = {Pool-{{Based Active Learning}} with {{Proper Topological Regions}}},
  author = {Hadjadj, Lies and Devijver, Emilie and Molinier, Remi and Amini, Massih-Reza},
  year = 2023,
  month = oct,
  journal = {arXiv.org},
  urldate = {2025-04-11},
  abstract = {Machine learning methods usually rely on large sample size to have good performance, while it is difficult to provide labeled set in many applications. Pool-based active learning methods are there to detect, among a set of unlabeled data, the ones that are the most relevant for the training. We propose in this paper a meta-approach for pool-based active learning strategies in the context of multi-class classification tasks based on Proper Topological Regions. PTR, based on topological data analysis (TDA), are relevant regions used to sample cold-start points or within the active learning scheme. The proposed method is illustrated empirically on various benchmark datasets, being competitive to the classical methods from the literature.},
  howpublished = {https://arxiv.org/abs/2310.01597v1},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\SBZ85NV8\Hadjadj et al. - 2023 - Pool-Based Active Learning with Proper Topological Regions.pdf}
}

@misc{haimovichConvergenceLossUncertaintybased2023,
  title = {On the Convergence of Loss and Uncertainty-Based Active Learning Algorithms},
  author = {Haimovich, Daniel and Karamshuk, Dima and Linder, Fridolin and Tax, Niek and Vojnovic, Milan},
  year = 2023,
  month = dec,
  number = {arXiv:2312.13927},
  eprint = {2312.13927},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.13927},
  urldate = {2024-03-06},
  abstract = {We study convergence rates of loss and uncertainty-based active learning algorithms under various assumptions. First, we provide a set of conditions under which a convergence rate guarantee holds, and use this for linear classifiers and linearly separable datasets to show convergence rate guarantees for loss-based sampling and different loss functions. Second, we provide a framework that allows us to derive convergence rate bounds for loss-based sampling by deploying known convergence rate bounds for stochastic gradient descent algorithms. Third, and last, we propose an active learning algorithm that combines sampling of points and stochastic Polyak's step size. We show a condition on the sampling that ensures a convergence rate guarantee for this algorithm for smooth convex loss functions. Our numerical results demonstrate efficiency of our proposed algorithm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\RIQ7ETA9\\Haimovich et al. - 2024 - On the Convergence of Loss and Uncertainty-based Active Learning Algorithms.pdf;C\:\\Users\\E097600\\Zotero\\storage\\S68DF36B\\Haimovich et al. - 2023 - On the convergence of loss and uncertainty-based a.pdf;C\:\\Users\\E097600\\Zotero\\storage\\NYKVBBWR\\2312.html;C\:\\Users\\E097600\\Zotero\\storage\\WMGIZNUH\\2312.html}
}

@misc{hallProbabilitybasedDetectionQuality2018,
  title = {Probability-Based {{Detection Quality}} ({{PDQ}}): {{A Probabilistic Approach}} to {{Detection Evaluation}}},
  shorttitle = {Probability-Based {{Detection Quality}} ({{PDQ}})},
  author = {Hall, David and Dayoub, Feras and Skinner, John and Corke, Peter and Carneiro, Gustavo and S{\"u}nderhauf, Niko},
  year = 2018,
  month = nov,
  number = {arXiv:1811.10800},
  eprint = {1811.10800},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.10800},
  urldate = {2025-09-17},
  abstract = {We propose a new visual object detector evaluation measure which not only assesses detection quality, but also accounts for the spatial and label uncertainties produced by object detection systems. Current evaluation measures such as mean average precision (mAP) do not take these two aspects into account, accepting detections with no spatial uncertainty and using only the label with the winning score instead of a full class probability distribution to rank detections. To overcome these limitations, we propose the probability-based detection quality (PDQ) measure which evaluates both spatial and label probabilities, requires no thresholds to be predefined, and optimally assigns ground-truth objects to detections. Our experimental evaluation shows that PDQ rewards detections with accurate spatial probabilities and explicitly evaluates label probability to determine detection quality. PDQ aims to encourage the development of new object detection approaches that provide meaningful spatial and label uncertainty measures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Instance segmentation,metric,Object detection},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\5YF8DJVN\\Hall et al. - 2018 - Probability-based Detection Quality (PDQ) A Probabilistic Approach to Detection Evaluation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\M8FQUWTW\\1811.html}
}

@misc{haoAddressingImbalanceClass2024,
  title = {Addressing {{Imbalance}} for {{Class Incremental Learning}} in {{Medical Image Classification}}},
  author = {Hao, Xuze and Ni, Wenqian and Jiang, Xuhao and Tan, Weimin and Yan, Bo},
  year = 2024,
  month = jul,
  number = {arXiv:2407.13768},
  eprint = {2407.13768},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.13768},
  urldate = {2025-02-11},
  abstract = {Deep convolutional neural networks have made significant breakthroughs in medical image classification, under the assumption that training samples from all classes are simultaneously available. However, in real-world medical scenarios, there's a common need to continuously learn about new diseases, leading to the emerging field of class incremental learning (CIL) in the medical domain. Typically, CIL suffers from catastrophic forgetting when trained on new classes. This phenomenon is mainly caused by the imbalance between old and new classes, and it becomes even more challenging with imbalanced medical datasets. In this work, we introduce two simple yet effective plug-in methods to mitigate the adverse effects of the imbalance. First, we propose a CIL-balanced classification loss to mitigate the classifier bias toward majority classes via logit adjustment. Second, we propose a distribution margin loss that not only alleviates the inter-class overlap in embedding space but also enforces the intra-class compactness. We evaluate the effectiveness of our method with extensive experiments on three benchmark datasets (CCH5000, HAM10000, and EyePACS). The results demonstrate that our approach outperforms state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\6PPMHHBH\\Hao et al. - 2024 - Addressing Imbalance for Class Incremental Learning in Medical Image Classification.pdf;C\:\\Users\\E097600\\Zotero\\storage\\99XU6IJQ\\2407.html}
}

@phdthesis{hautotRepresentationBaseRadiale,
  title = {{Repr\'esentation \`a base radiale pour l'apprentissage par renforcement visuel}},
  author = {Hautot, Juien},
  address = {Clemront-Ferrand},
  abstract = {Ces travaux sont essentiellement bas\'es sur l'apprentissage par renforcement (Renforcement Learning - RL) visuel. Contrairement \`a l'apprentissage supervis\'e qui permet d'effectuer diff\'erentes t\^aches telles que la classification, la r\'egression ou encore la segmentation \`a partir d'une base de donn\'ees annot\'ee, le RL permet d'apprendre, sans base de donn\'ees, via des interaction avec un environnement. En effet, dans ces m\'ethodes, un agent tel qu'un robot va effectuer diff\'erentes actions afin d'explorer son environnement et de r\'ecup\'erer les donn\'ees d'entra\^inement. L'entra\^inement de ce type d'agent s'effectue par essais et erreurs ; lorsque l'agent \'echoue dans sa t\^ache, il est p\'enalis\'e, tandis que lorsqu'il r\'eussit, il est r\'ecompens\'e. Le but pour l'agent est d'am\'eliorer son comportement pour obtenir le plus de r\'ecompenses \`a long terme. Dans ces travaux, nous nous int\'eressons aux environnements visuels \`a la premi\`ere personne. L'utilisation de donn\'ees visuelles fait souvent appel \`a des r\'eseaux de convolution lecun1989, ces r\'eseaux permettant de travailler directement avec des images. Cependant, ces r\'eseaux sont limit\'es par leur complexit\'e de calcul, leur manque d'explicabilit\'e ainsi que leur instabilit\'e prononc\'ee dans les environnements visuels \`a la premi\`ere personne. Dans ces travaux, nous avons investigu\'e le d\'eveloppement d'un r\'eseau bas\'e sur les fonctions \`a base radiale Park1991, ces fonctions permettant des activations \'eparses et localis\'ees dans l'espace d'entr\'ee. Les r\'eseaux \`a base radiale ont connu leur apog\'ee dans les ann\'ees 90, puis ont \'et\'e supplant\'es par les r\'eseaux de convolution car ils \'etaient difficilement utilisables sur des images en raison de leur co\^ut en calcul. Dans cette th\`ese, nous avons d\'evelopp\'e un extracteur de caract\'eristiques visuelles inspir\'e des RBFN en simplifiant le co\^ut calculatoire sur les images. Nous avons utilis\'e notre r\'eseau pour la r\'esolution de probl\`emes visuels \`a la premi\`ere personne et nous avons compar\'e ses r\'esultats avec diff\'erentes m\'ethodes de l'\'etat de l'art, des m\'ethodes d'apprentissage de bout-en-bout, des m\'ethodes utilisant l'apprentissage de repr\'esentation d'\'etat et des m\'ethodes d'apprentissage machine extr\^eme. Diff\'erents sc\'enarios ont \'et\'e test\'es issus du simulateur VizDoom Kempka2016 du jeu vid\'eo Doom, ainsi que du simulateur physique de robotique Pybullet coumans2021. Outre la comparaison des r\'ecompenses obtenues apr\`es l'apprentissage, nous avons aussi effectu\'e diff\'erents tests sur la robustesse au bruit, la g\'en\'eration des param\`etres de notre r\'eseau et le transfert d'une t\^ache dans la r\'ealit\'e. Notre r\'eseau est simple d'utilisation, permet une certaine explicabilit\'e et obtient les meilleurs r\'esultats lors d'apprentissage par renforcement sur les sc\'enarios test\'es. De plus, notre r\'eseau est robuste face \`a diff\'erents bruits, ce qui a permis un transfert direct des connaissances acquises en simulation \`a la r\'ealit\'e.},
  langid = {french},
  school = {universit\'e Clermont-Auvergne},
  file = {C:\Users\E097600\Zotero\storage\GLYBZPMI\Hautot - Représentation à base radiale pour lapprentissage.pdf}
}

@article{hautotSolvingPartiallyObservable2023,
  title = {Solving {{Partially Observable 3D-Visual Tasks}} with {{Visual Radial Basis Function Network}} and {{Proximal Policy Optimization}}},
  author = {Hautot, Julien and Teuli{\`e}re, C{\'e}line and Azzaoui, Nourddine},
  year = 2023,
  month = dec,
  journal = {Machine Learning and Knowledge Extraction},
  volume = {5},
  number = {4},
  pages = {1888--1904},
  issn = {2504-4990},
  doi = {10.3390/make5040091},
  urldate = {2024-08-23},
  abstract = {Visual Reinforcement Learning (RL) has been largely investigated in recent decades. Existing approaches are often composed of multiple networks requiring massive computational power to solve partially observable tasks from high-dimensional data such as images. Using State Representation Learning (SRL) has been shown to improve the performance of visual RL by reducing the high-dimensional data into compact representation, but still often relies on deep networks and on the environment. In contrast, we propose a lighter, more generic method to extract sparse and localized features from raw images without training. We achieve this using a Visual Radial Basis Function Network (VRBFN), which offers significant practical advantages, including efficient and accurate training with minimal complexity due to its two linear layers. For real-world applications, its scalability and resilience to noise are essential, as real sensors are subject to change and noise. Unlike CNNs, which may require extensive retraining, this network might only need minor fine-tuning. We test the efficiency of the VRBFN representation to solve different RL tasks using Proximal Policy Optimization (PPO). We present a large study and comparison of our extraction methods with five classical visual RL and SRL approaches on five different first-person partially observable scenarios. We show that this approach presents appealing features such as sparsity and robustness to noise and that the obtained results when training RL agents are better than other tested methods on four of the five proposed scenarios.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\UCAI39PG\Hautot et al. - 2023 - Solving Partially Observable 3D-Visual Tasks with .pdf}
}

@misc{hautotVisualRadialBasis2022,
  title = {Visual {{Radial Basis Q-Network}}},
  author = {Hautot, Julien and Teuliere, C{\'e}line and Azzaoui, Nourddine},
  year = 2022,
  month = jun,
  eprint = {2206.06712},
  primaryclass = {cs},
  doi = {10.1007/978-3-031-09282-4_27},
  urldate = {2025-09-19},
  abstract = {While reinforcement learning (RL) from raw images has been largely investigated in the last decade, existing approaches still suffer from a number of constraints. The high input dimension is often handled using either expert knowledge to extract handcrafted features or environment encoding through convolutional networks. Both solutions require numerous parameters to be optimized. In contrast, we propose a generic method to extract sparse features from raw images with few trainable parameters. We achieved this using a Radial Basis Function Network (RBFN) directly on raw image. We evaluate the performance of the proposed approach for visual extraction in Q-learning tasks in the Vizdoom environment. Then, we compare our results with two Deep Q-Network, one trained directly on images and another one trained on feature extracted by a pretrained auto-encoder. We show that the proposed approach provides similar or, in some cases, even better performances with fewer trainable parameters while being conceptually simpler.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\S89T5NGA\\Hautot et al. - 2022 - Visual Radial Basis Q-Network.pdf;C\:\\Users\\E097600\\Zotero\\storage\\Y2Q7T55V\\2206.html}
}

@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = 2015,
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.03385},
  urldate = {2024-07-05},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\E097600\Zotero\storage\9QNGXUUL\1512.html}
}

@misc{heideckerSamplingbasedUncertaintyEstimation2023,
  title = {Sampling-Based {{Uncertainty Estimation}} for an {{Instance Segmentation Network}}},
  author = {Heidecker, Florian and {El-Khateeb}, Ahmad and Sick, Bernhard},
  year = 2023,
  month = may,
  number = {arXiv:2305.14977},
  eprint = {2305.14977},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14977},
  urldate = {2025-09-17},
  abstract = {The examination of uncertainty in the predictions of machine learning (ML) models is receiving increasing attention. One uncertainty modeling technique used for this purpose is Monte-Carlo (MC)-Dropout, where repeated predictions are generated for a single input. Therefore, clustering is required to describe the resulting uncertainty, but only through efficient clustering is it possible to describe the uncertainty from the model attached to each object. This article uses Bayesian Gaussian Mixture (BGM) to solve this problem. In addition, we investigate different values for the dropout rate and other techniques, such as focal loss and calibration, which we integrate into the Mask-RCNN model to obtain the most accurate uncertainty approximation of each instance and showcase it graphically.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Calibration,Computer Science - Computer Vision and Pattern Recognition,Instance segmentation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\FEP6QIJP\\Heidecker et al. - 2023 - Sampling-based Uncertainty Estimation for an Instance Segmentation Network.pdf;C\:\\Users\\E097600\\Zotero\\storage\\L3JXTXU4\\2305.html}
}

@misc{heimGuideFailureMachine2025,
  title = {A {{Guide}} to {{Failure}} in {{Machine Learning}}: {{Reliability}} and {{Robustness}} from {{Foundations}} to {{Practice}}},
  shorttitle = {A {{Guide}} to {{Failure}} in {{Machine Learning}}},
  author = {Heim, Eric and Wright, Oren and Shriver, David},
  year = 2025,
  month = mar,
  number = {arXiv:2503.00563},
  eprint = {2503.00563},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.00563},
  urldate = {2025-03-18},
  abstract = {One of the main barriers to adoption of Machine Learning (ML) is that ML models can fail unexpectedly. In this work, we aim to provide practitioners a guide to better understand why ML models fail and equip them with techniques they can use to reason about failure. Specifically, we discuss failure as either being caused by lack of reliability or lack of robustness. Differentiating the causes of failure in this way allows us to formally define why models fail from first principles and tie these definitions to engineering concepts and real-world deployment settings. Throughout the document we provide 1) a summary of important theoretic concepts in reliability and robustness, 2) a sampling current techniques that practitioners can utilize to reason about ML model reliability and robustness, and 3) examples that show how these concepts and techniques can apply to real-world settings.},
  archiveprefix = {arXiv},
  keywords = {basics,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,method,research},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\IE5CR8E2\\Heim et al. - 2025 - A Guide to Failure in Machine Learning Reliability and Robustness from Foundations to Practice.pdf;C\:\\Users\\E097600\\Zotero\\storage\\T6C2PJKY\\2503.html}
}

@misc{hekimogluActiveLearningObject2023,
  title = {Active {{Learning}} for {{Object Detection}} with {{Non-Redundant Informative Sampling}}},
  author = {Hekimoglu, Aral and Brucker, Adrian and Kayali, Alper Kagan and Schmidt, Michael and {Marcos-Ramiro}, Alvaro},
  year = 2023,
  month = jul,
  number = {arXiv:2307.08414},
  eprint = {2307.08414},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.08414},
  urldate = {2024-03-08},
  abstract = {Curating an informative and representative dataset is essential for enhancing the performance of 2D object detectors. We present a novel active learning sampling strategy that addresses both the informativeness and diversity of the selections. Our strategy integrates uncertainty and diversity-based selection principles into a joint selection objective by measuring the collective information score of the selected samples. Specifically, our proposed NORIS algorithm quantifies the impact of training with a sample on the informativeness of other similar samples. By exclusively selecting samples that are simultaneously informative and distant from other highly informative samples, we effectively avoid redundancy while maintaining a high level of informativeness. Moreover, instead of utilizing whole image features to calculate distances between samples, we leverage features extracted from detected object regions within images to define object features. This allows us to construct a dataset encompassing diverse object types, shapes, and angles. Extensive experiments on object detection and image classification tasks demonstrate the effectiveness of our strategy over the state-of-the-art baselines. Specifically, our selection strategy achieves a 20\% and 30\% reduction in labeling costs compared to random selection for PASCAL-VOC and KITTI, respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\2JHTFBPJ\\Hekimoglu et al. - 2023 - Active Learning for Object Detection with Non-Redu.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ACG3QXHI\\2307.html}
}

@article{helberEuroSATNovelDataset2019,
  title = {{{EuroSAT}}: {{A Novel Dataset}} and {{Deep Learning Benchmark}} for {{Land Use}} and {{Land Cover Classification}}},
  shorttitle = {{{EuroSAT}}},
  author = {Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},
  year = 2019,
  month = jul,
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume = {12},
  number = {7},
  pages = {2217--2226},
  issn = {2151-1535},
  doi = {10.1109/JSTARS.2019.2918242},
  urldate = {2025-01-16},
  abstract = {In this paper, we present a patch-based land use and land cover classification approach using Sentinel-2 satellite images. The Sentinel-2 satellite images are openly and freely accessible, and are provided in the earth observation program Copernicus. We present a novel dataset, based on these images that covers 13 spectral bands and is comprised of ten classes with a total of 27 000 labeled and geo-referenced images. Benchmarks are provided for this novel dataset with its spectral bands using state-of-the-art deep convolutional neural networks. An overall classification accuracy of 98.57\% was achieved with the proposed novel dataset. The resulting classification system opens a gate toward a number of earth observation applications. We demonstrate how this classification system can be used for detecting land use and land cover changes, and how it can assist in improving geographical maps. The geo-referenced dataset EuroSAT is made publicly available at https://github.com/phelber/eurosat.},
  keywords = {Benchmark testing,Dataset,Deep learning,Earth,earth observation,Feature extraction,land cover classification,land use classification,Machine Learning,remote sensing,Remote sensing,satellite image classification,satellite images,Satellites,Spatial resolution}
}

@misc{heMaskedAutoencodersAre2021,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  year = 2021,
  month = dec,
  number = {arXiv:2111.06377},
  eprint = {2111.06377},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.06377},
  urldate = {2025-05-27},
  abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,ssl},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\QLZAZRIW\\He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf;C\:\\Users\\E097600\\Zotero\\storage\\CEL9F2TJ\\2111.html}
}

@misc{heMaskRCNN2018,
  title = {Mask {{R-CNN}}},
  author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  year = 2018,
  month = jan,
  number = {arXiv:1703.06870},
  eprint = {1703.06870},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.06870},
  urldate = {2024-03-04},
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Instance segmentation,modele},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\IQAE69AV\\He et al. - 2018 - Mask R-CNN.pdf;C\:\\Users\\E097600\\Zotero\\storage\\5T5AYIYY\\1703.html}
}

@misc{heMomentumContrastUnsupervised2020,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  year = 2020,
  month = mar,
  number = {arXiv:1911.05722},
  eprint = {1911.05722},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.05722},
  urldate = {2024-12-19},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\YS5TAXS5\\He et al. - 2020 - Momentum Contrast for Unsupervised Visual Representation Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\APS2ER2M\\1911.html}
}

@misc{heuselGANsTrainedTwo2018,
  title = {{{GANs Trained}} by a {{Two Time-Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  year = 2018,
  month = jan,
  number = {arXiv:1706.08500},
  eprint = {1706.08500},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.08500},
  urldate = {2025-04-02},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\textbackslash 'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\M6ZEJJGW\\Heusel et al. - 2018 - GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.pdf;C\:\\Users\\E097600\\Zotero\\storage\\TZGWCVTD\\1706.html}
}

@misc{hoLearningLearnFewshot2024,
  title = {Learning to {{Learn}} for {{Few-shot Continual Active Learning}}},
  author = {Ho, Stella and Liu, Ming and Gao, Shang and Gao, Longxiang},
  year = 2024,
  month = may,
  number = {arXiv:2311.03732},
  eprint = {2311.03732},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.03732},
  urldate = {2024-06-28},
  abstract = {Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in continual learning are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We exploit meta-learning and propose a method, called Meta-Continual Active Learning. This method sequentially queries the most informative examples from a pool of unlabeled data for annotation to enhance task-specific performance and tackle continual learning problems through meta-objective. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to avoid memory over-fitting caused by experience replay and sample queries, thereby ensuring generalization. We conduct extensive experiments on benchmark text classification datasets from diverse domains to validate the feasibility and effectiveness of meta-continual active learning. We also analyze the impact of different active learning strategies on various meta continual learning models. The experimental results demonstrate that introducing randomness into sample selection is the best default strategy for maintaining generalization in meta-continual learning framework.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\DPNYV793\\Ho et al. - 2024 - Learning to Learn for Few-shot Continual Active Le.pdf;C\:\\Users\\E097600\\Zotero\\storage\\239GJMWH\\2311.html}
}

@article{hondruMaskedImageModeling2025,
  title = {Masked {{Image Modeling}}: {{A Survey}}},
  shorttitle = {Masked {{Image Modeling}}},
  author = {Hondru, Vlad and Croitoru, Florinel Alin and Minaee, Shervin and Ionescu, Radu Tudor and Sebe, Nicu},
  year = 2025,
  month = jul,
  journal = {International Journal of Computer Vision},
  issn = {1573-1405},
  doi = {10.1007/s11263-025-02524-1},
  urldate = {2025-09-17},
  abstract = {In this work, we survey recent studies on masked image modeling (MIM), an approach that emerged as a powerful self-supervised learning technique in computer vision. The MIM task involves masking some information, e.g.~pixels, patches, or even latent representations, and training a model, usually an autoencoder, to predicting the missing information by using the context available in the visible part of the input. We identify and formalize two categories of approaches on how to implement MIM as a pretext task, one based on reconstruction and one based on contrastive learning. Then, we construct a taxonomy and review the most prominent papers in recent years. We complement the manually constructed taxonomy with a dendrogram obtained by applying a hierarchical clustering algorithm. We further identify relevant clusters via manually inspecting the resulting dendrogram. Our review also includes datasets that are commonly used in MIM research. We aggregate the performance results of various masked image modeling methods on the most popular datasets, to facilitate the comparison of competing methods. Finally, we identify research gaps and propose several interesting directions of future work. We supplement our survey with the following public repository containing organized references: https://github.com/vladhondru25/MIM-Survey.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Masked autoencoders,Masked image modeling,review,Self-supervised learning,ssl},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\66VNY38F\\Hondru et al. - 2025 - Masked Image Modeling A Survey.pdf;C\:\\Users\\E097600\\Zotero\\storage\\T482WPL5\\Hondru et al. - 2025 - Masked Image Modeling A Survey.pdf;C\:\\Users\\E097600\\Zotero\\storage\\MEPEDAUG\\2408.html}
}

@misc{houlsbyBayesianActiveLearning2011,
  title = {Bayesian {{Active Learning}} for {{Classification}} and {{Preference Learning}}},
  author = {Houlsby, Neil and Husz{\'a}r, Ferenc and Ghahramani, Zoubin and Lengyel, M{\'a}t{\'e}},
  year = 2011,
  month = dec,
  number = {arXiv:1112.5745},
  eprint = {1112.5745},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1112.5745},
  urldate = {2024-03-08},
  abstract = {Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,Statistics - Machine Learning,tabular data},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\6SEV3L89\\Houlsby et al. - 2011 - Bayesian Active Learning for Classification and Pr.pdf;C\:\\Users\\E097600\\Zotero\\storage\\EZYA5G5H\\Houlsby et al. - 2011 - Bayesian Active Learning for Classification and Preference Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\2JYY3BKX\\1112.html;C\:\\Users\\E097600\\Zotero\\storage\\CQBM34TQ\\1112.html}
}

@misc{howladerPixelsSemiSupervisedSemantic2024,
  title = {Beyond {{Pixels}}: {{Semi-Supervised Semantic Segmentation}} with a {{Multi-scale Patch-based Multi-Label Classifier}}},
  shorttitle = {Beyond {{Pixels}}},
  author = {Howlader, Prantik and Das, Srijan and Le, Hieu and Samaras, Dimitris},
  year = 2024,
  month = jul,
  number = {arXiv:2407.04036},
  eprint = {2407.04036},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.04036},
  urldate = {2024-07-10},
  abstract = {Incorporating pixel contextual information is critical for accurate segmentation. In this paper, we show that an effective way to incorporate contextual information is through a patch-based classifier. This patch classifier is trained to identify classes present within an image region, which facilitates the elimination of distractors and enhances the classification of small object segments. Specifically, we introduce Multi-scale Patch-based Multi-label Classifier (MPMC), a novel plug-in module designed for existing semi-supervised segmentation (SSS) frameworks. MPMC offers patch-level supervision, enabling the discrimination of pixel regions of different classes within a patch. Furthermore, MPMC learns an adaptive pseudo-label weight, using patch-level classification to alleviate the impact of the teacher's noisy pseudo-label supervision the student. This lightweight module can be integrated into any SSS framework, significantly enhancing their performance. We demonstrate the efficacy of our proposed MPMC by integrating it into four SSS methodologies and improving them across two natural image and one medical segmentation dataset, notably improving the segmentation results of the baselines across all the three datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\U5WFCY5I\\Howlader et al. - 2024 - Beyond Pixels Semi-Supervised Semantic Segmentati.pdf;C\:\\Users\\E097600\\Zotero\\storage\\I8MGZ54Q\\2407.html}
}

@book{howseOpenCVComputerVision,
  title = {{{OpenCV}}: {{Computer Vision Projects}} with {{Python}} \textbar{} {{Data}} \textbar{} {{Subscription}}},
  shorttitle = {{{OpenCV}}},
  author = {{howse}, joseph},
  urldate = {2025-01-13},
  abstract = {Develop computer vision applications with OpenCV. Instant delivery. Top rated Data products.},
  langid = {english},
  keywords = {book},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\JISHLNWH\\howse - OpenCV Computer Vision Projects with Python  Data  Subscription.pdf;C\:\\Users\\E097600\\Zotero\\storage\\PAIB7GTG\\opencv-computer-vision-projects-with-python-9781787125490.html}
}

@misc{huangConformalPredictionDeep2024,
  title = {Conformal {{Prediction}} for {{Deep Classifier}} via {{Label Ranking}}},
  author = {Huang, Jianguo and Xi, Huajun and Zhang, Linjun and Yao, Huaxiu and Qiu, Yue and Wei, Hongxin},
  year = 2024,
  month = jun,
  number = {arXiv:2310.06430},
  eprint = {2310.06430},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06430},
  urldate = {2025-03-31},
  abstract = {Conformal prediction is a statistical framework that generates prediction sets containing ground-truth labels with a desired coverage guarantee. The predicted probabilities produced by machine learning models are generally miscalibrated, leading to large prediction sets in conformal prediction. To address this issue, we propose a novel algorithm named \$\textbackslash textit\textbraceleft Sorted Adaptive Prediction Sets\textbraceright\$ (SAPS), which discards all the probability values except for the maximum softmax probability. The key idea behind SAPS is to minimize the dependence of the non-conformity score on the probability values while retaining the uncertainty information. In this manner, SAPS can produce compact prediction sets and communicate instance-wise uncertainty. Extensive experiments validate that SAPS not only lessens the prediction sets but also broadly enhances the conditional coverage rate of prediction sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Conformal prediction,Image classification,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\BWTNSPUJ\\Huang et al. - 2024 - Conformal Prediction for Deep Classifier via Label Ranking.pdf;C\:\\Users\\E097600\\Zotero\\storage\\DED2RK3N\\2310.html}
}

@inproceedings{huangCosteffectiveActiveLearning2017,
  title = {Cost-Effective Active Learning from Diverse Labelers},
  booktitle = {Proceedings of the 26th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Huang, Sheng-Jun and Chen, Jia-Lve and Mu, Xin and Zhou, Zhi-Hua},
  year = 2017,
  month = aug,
  series = {{{IJCAI}}'17},
  pages = {1879--1885},
  publisher = {AAAI Press},
  address = {Melbourne, Australia},
  urldate = {2025-02-13},
  abstract = {In traditional active learning, there is only one labeler that always returns the ground truth of queried labels. However, in many applications, multiple labelers are available to offer diverse qualities of labeling with different costs. In this paper, we perform active selection on both instances and labelers, aiming to improve the classification model most with the lowest cost. While the cost of a labeler is proportional to its overall labeling quality, we also observe that different labelers usually have diverse expertise, and thus it is likely that labelers with a low overall quality can provide accurate labels on some specific instances. Based on this fact, we propose a novel active selection criterion to evaluate the cost-effectiveness of instance-labeler pairs, which ensures that the selected instance is helpful for improving the classification model, and meanwhile the selected labeler can provide an accurate label for the instance with a relative low cost. Experiments on both UCI and real crowdsourcing data sets demonstrate the superiority of our proposed approach on selecting cost-effective queries.},
  isbn = {978-0-9992411-0-3}
}

@misc{huangDataefficientEventCamera2024,
  title = {Data-Efficient {{Event Camera Pre-training}} via {{Disentangled Masked Modeling}}},
  author = {Huang, Zhenpeng and Li, Chao and Chen, Hao and Deng, Yongjian and Geng, Yifeng and Wang, Limin},
  year = 2024,
  month = mar,
  number = {arXiv:2403.00416},
  eprint = {2403.00416},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-04},
  abstract = {In this paper, we present a new data-efficient voxel-based self-supervised learning method for event cameras. Our pre-training overcomes the limitations of previous methods, which either sacrifice temporal information by converting event sequences into 2D images for utilizing pre-trained image models or directly employ paired image data for knowledge distillation to enhance the learning of event streams. In order to make our pre-training data-efficient, we first design a semantic-uniform masking method to address the learning imbalance caused by the varying reconstruction difficulties of different regions in non-uniform data when using random masking. In addition, we ease the traditional hybrid masked modeling process by explicitly decomposing it into two branches, namely local spatio-temporal reconstruction and global semantic reconstruction to encourage the encoder to capture local correlations and global semantics, respectively. This decomposition allows our selfsupervised learning method to converge faster with minimal pre-training data. Compared to previous approaches, our self-supervised learning method does not rely on paired RGB images, yet enables simultaneous exploration of spatial and temporal cues in multiple scales. It exhibits excellent generalization performance and demonstrates significant improvements across various tasks with fewer parameters and lower computational costs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\CWVCVYIY\\Huang et al. - 2024 - Data-efficient Event Camera Pre-training via Disen.pdf;C\:\\Users\\E097600\\Zotero\\storage\\3IMW23P7\\2403.html}
}

@article{huangFabricDefectSegmentation2021,
  title = {Fabric {{Defect Segmentation Method Based}} on {{Deep Learning}}},
  author = {Huang, Yanqing and Jing, Junfeng and Wang, Zhen},
  year = 2021,
  journal = {IEEE Transactions on Instrumentation and Measurement},
  volume = {70},
  pages = {1--15},
  issn = {0018-9456, 1557-9662},
  doi = {10.1109/TIM.2020.3047190},
  urldate = {2024-03-05}
}

@article{huangRicciCurvatureTensorBased2025,
  title = {Ricci {{Curvature Tensor-Based Volumetric Segmentation}}},
  author = {Huang, Jisui and Chen, Ke and Alpers, Andreas and Lei, Na},
  year = 2025,
  month = sep,
  journal = {International Journal of Computer Vision},
  volume = {133},
  number = {9},
  pages = {6491--6512},
  issn = {1573-1405},
  doi = {10.1007/s11263-025-02492-6},
  urldate = {2025-09-22},
  abstract = {Existing level set models employ regularization based only on gradient information, 1D curvature or 2D curvature. For 3D image segmentation, however, an appropriate curvature-based regularization should involve a well-defined 3D curvature energy. This is the first paper to introduce a regularization energy that incorporates 3D scalar curvature for 3D image segmentation, inspired by the Einstein-Hilbert functional. To derive its Euler-Lagrange equation, we employ a two-step gradient descent strategy, alternately updating the level set function and its gradient. The paper also establishes the existence and uniqueness of the viscosity solution for the proposed model. Experimental results demonstrate that our proposed model outperforms other state-of-the-art models in 3D image segmentation.},
  langid = {english},
  keywords = {Image segmentation,Ricci curvature tensor,Riemannian geometry,Variational model},
  file = {C:\Users\E097600\Zotero\storage\7RNUEEEP\Huang et al. - 2025 - Ricci Curvature Tensor-Based Volumetric Segmentation.pdf}
}

@misc{huangTorchCPPythonLibrary2024,
  title = {{{TorchCP}}: {{A Python Library}} for {{Conformal Prediction}}},
  shorttitle = {{{TorchCP}}},
  author = {Huang, Jianguo and Song, Jianqing and Zhou, Xuanning and Jing, Bingyi and Wei, Hongxin},
  year = 2024,
  month = dec,
  number = {arXiv:2402.12683},
  eprint = {2402.12683},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.12683},
  urldate = {2025-04-30},
  abstract = {Conformal Prediction (CP) has attracted great attention from the research community due to its strict theoretical guarantees. However, researchers and developers still face challenges of applicability and efficiency when applying CP algorithms to deep learning models. In this paper, we introduce \textbackslash torchcp, a comprehensive PyTorch-based toolkit to strengthen the usability of CP for deep learning models. \textbackslash torchcp implements a wide range of post-hoc and training methods of conformal prediction for various machine learning tasks, including classification, regression, GNN, and LLM. Moreover, we provide user-friendly interfaces and extensive evaluations to easily integrate CP algorithms into specific tasks. Our \textbackslash torchcp toolkit, built entirely with PyTorch, enables high-performance GPU acceleration for deep learning models and mini-batch computation on large-scale datasets. With the LGPL license, the code is open-sourced at \textbackslash url\textbraceleft https://github.com/ml-stat-Sustech/TorchCP\textbraceright{} and will be continuously updated.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\EL2U3FZN\\Huang et al. - 2024 - TorchCP A Python Library for Conformal Prediction.pdf;C\:\\Users\\E097600\\Zotero\\storage\\BEH8JAZK\\2402.html}
}

@misc{hulkundDataS3DatasetSubset2025,
  title = {{{DataS}}\textasciicircum 3: {{Dataset Subset Selection}} for {{Specialization}}},
  shorttitle = {{{DataS}}\textasciicircum 3},
  author = {Hulkund, Neha and Maalouf, Alaa and Cai, Levi and Yang, Daniel and Wang, Tsun-Hsuan and O'Neil, Abigail and Haucke, Timm and Mukherjee, Sandeep and Ramaswamy, Vikram and Shen, Judy Hansen and Tseng, Gabriel and Walmsley, Mike and Rus, Daniela and Goldberg, Ken and Kerner, Hannah and Chen, Irene and Girdhar, Yogesh and Beery, Sara},
  year = 2025,
  month = apr,
  number = {arXiv:2504.16277},
  eprint = {2504.16277},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.16277},
  urldate = {2025-04-28},
  abstract = {In many real-world machine learning (ML) applications (e.g. detecting broken bones in x-ray images, detecting species in camera traps), in practice models need to perform well on specific deployments (e.g. a specific hospital, a specific national park) rather than the domain broadly. However, deployments often have imbalanced, unique data distributions. Discrepancy between the training distribution and the deployment distribution can lead to suboptimal performance, highlighting the need to select deployment-specialized subsets from the available training data. We formalize dataset subset selection for specialization (DS3): given a training set drawn from a general distribution and a (potentially unlabeled) query set drawn from the desired deployment-specific distribution, the goal is to select a subset of the training data that optimizes deployment performance. We introduce DataS\textasciicircum 3; the first dataset and benchmark designed specifically for the DS3 problem. DataS\textasciicircum 3 encompasses diverse real-world application domains, each with a set of distinct deployments to specialize in. We conduct a comprehensive study evaluating algorithms from various families--including coresets, data filtering, and data curation--on DataS\textasciicircum 3, and find that general-distribution methods consistently fail on deployment-specific tasks. Additionally, we demonstrate the existence of manually curated (deployment-specific) expert subsets that outperform training on all available data with accuracy gains up to 51.3 percent. Our benchmark highlights the critical role of tailored dataset curation in enhancing performance and training efficiency on deployment-specific distributions, which we posit will only become more important as global, public datasets become available across domains and ML models are deployed in the real world.},
  archiveprefix = {arXiv},
  keywords = {Active sampling,Class imbalance,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Domain Generalisation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\G3M72Q34\\Hulkund et al. - 2025 - DataS^3 Dataset Subset Selection for Specialization.pdf;C\:\\Users\\E097600\\Zotero\\storage\\R8MKBI6X\\2504.html}
}

@misc{huMostInfluentialSubset2025,
  title = {Most {{Influential Subset Selection}}: {{Challenges}}, {{Promises}}, and {{Beyond}}},
  shorttitle = {Most {{Influential Subset Selection}}},
  author = {Hu, Yuzheng and Hu, Pingbang and Zhao, Han and Ma, Jiaqi W.},
  year = 2025,
  month = jan,
  number = {arXiv:2409.18153},
  eprint = {2409.18153},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.18153},
  urldate = {2025-05-06},
  abstract = {How can we attribute the behaviors of machine learning models to their training data? While the classic influence function sheds light on the impact of individual samples, it often fails to capture the more complex and pronounced collective influence of a set of samples. To tackle this challenge, we study the Most Influential Subset Selection (MISS) problem, which aims to identify a subset of training samples with the greatest collective influence. We conduct a comprehensive analysis of the prevailing approaches in MISS, elucidating their strengths and weaknesses. Our findings reveal that influence-based greedy heuristics, a dominant class of algorithms in MISS, can provably fail even in linear regression. We delineate the failure modes, including the errors of influence function and the non-additive structure of the collective influence. Conversely, we demonstrate that an adaptive version of these heuristics which applies them iteratively, can effectively capture the interactions among samples and thus partially address the issues. Experiments on real-world datasets corroborate these theoretical findings and further demonstrate that the merit of adaptivity can extend to more complex scenarios such as classification tasks and non-linear neural networks. We conclude our analysis by emphasizing the inherent trade-off between performance and computational efficiency, questioning the use of additive metrics such as the Linear Datamodeling Score, and offering a range of discussions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\9RJS5YPZ\\Hu et al. - 2025 - Most Influential Subset Selection Challenges, Promises, and Beyond.pdf;C\:\\Users\\E097600\\Zotero\\storage\\GBJWLFHM\\2409.html}
}

@misc{ignatAnnotationsBudgetLeveraging2024,
  title = {Annotations on a {{Budget}}: {{Leveraging Geo-Data Similarity}} to {{Balance Model Performance}} and {{Annotation Cost}}},
  shorttitle = {Annotations on a {{Budget}}},
  author = {Ignat, Oana and Bai, Longju and Nwatu, Joan and Mihalcea, Rada},
  year = 2024,
  month = mar,
  number = {arXiv:2403.07687},
  eprint = {2403.07687},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-14},
  abstract = {Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck. In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. Next, we identify countries with higher visual similarity for these topics and show that using data from these countries to supplement the training data improves model performance and reduces annotation costs. The resulting lists of countries and corresponding topics are made available at https://github.com/MichiganNLP/visual\_diversity\_budget.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\6BSMV9VU\\Ignat et al. - 2024 - Annotations on a Budget Leveraging Geo-Data Simil.pdf;C\:\\Users\\E097600\\Zotero\\storage\\IPNTZPIH\\2403.html}
}

@misc{iscenRetrievalEnhancedContrastiveVisionText2024,
  title = {Retrieval-{{Enhanced Contrastive Vision-Text Models}}},
  author = {Iscen, Ahmet and Caron, Mathilde and Fathi, Alireza and Schmid, Cordelia},
  year = 2024,
  month = feb,
  number = {arXiv:2306.07196},
  eprint = {2306.07196},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.07196},
  urldate = {2025-09-09},
  abstract = {Contrastive image-text models such as CLIP form the building blocks of many state-of-the-art systems. While they excel at recognizing common generic concepts, they still struggle on fine-grained entities which are rare, or even absent from the pre-training dataset. Hence, a key ingredient to their success has been the use of large-scale curated pre-training data aiming at expanding the set of concepts that they can memorize during the pre-training stage. In this work, we explore an alternative to encoding fine-grained knowledge directly into the model's parameters: we instead train the model to retrieve this knowledge from an external memory. Specifically, we propose to equip existing vision-text models with the ability to refine their embedding with cross-modal retrieved information from a memory at inference time, which greatly improves their zero-shot predictions. Remarkably, we show that this can be done with a light-weight, single-layer, fusion transformer on top of a frozen CLIP. Our experiments validate that our retrieval-enhanced contrastive (RECO) training improves CLIP performance substantially on several challenging fine-grained tasks: for example +10.9 on Stanford Cars, +10.2 on CUB-2011 and +7.3 on the recent OVEN benchmark, where we even outperform the fine-tuned models on unseen classes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\E3NXPJB6\\Iscen et al. - 2024 - Retrieval-Enhanced Contrastive Vision-Text Models.pdf;C\:\\Users\\E097600\\Zotero\\storage\\H3ZFK69H\\2306.html}
}

@misc{ishibashiStoppingCriterionActive2021,
  title = {Stopping {{Criterion}} for {{Active Learning Based}} on {{Error Stability}}},
  author = {Ishibashi, Hideaki and Hino, Hideitsu},
  year = 2021,
  month = apr,
  number = {arXiv:2104.01836},
  eprint = {2104.01836},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.01836},
  urldate = {2025-02-03},
  abstract = {Active learning is a framework for supervised learning to improve the predictive performance by adaptively annotating a small number of samples. To realize efficient active learning, both an acquisition function that determines the next datum and a stopping criterion that determines when to stop learning should be considered. In this study, we propose a stopping criterion based on error stability, which guarantees that the change in generalization error upon adding a new sample is bounded by the annotation cost and can be applied to any Bayesian active learning. We demonstrate that the proposed criterion stops active learning at the appropriate timing for various learning models and real datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\CWHZWWYP\\Ishibashi et Hino - 2021 - Stopping Criterion for Active Learning Based on Error Stability.pdf;C\:\\Users\\E097600\\Zotero\\storage\\2ZN5ZURL\\2104.html}
}

@inproceedings{izbickiFlexibleDistributionfreeConditional2020,
  title = {Flexible Distribution-Free Conditional Predictive Bands Using Density Estimators},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Izbicki, Rafael and Shimizu, Gilson and Stern, Rafael},
  year = 2020,
  month = jun,
  pages = {3068--3077},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-11-05},
  abstract = {Conformal methods create prediction bands that control average coverage assuming solely i.i.d. data. Besides average coverage, one might also desire to control conditional coverage, that is, coverage for every new testing point. However, without strong assumptions, conditional coverage is unachievable. Given this limitation, the literature has focused on methods with asymptotical conditional coverage. In order to obtain this property, these methods require strong conditions on the dependence between the target variable and the features. We introduce two conformal methods based on conditional density estimators that do not depend on this type of assumption to obtain asymptotic conditional coverage: Dist-split and CD-split. While Dist-split asymptotically obtains optimal intervals, which are easier to interpret than general regions, CD-split obtains optimal size regions, which are smaller than intervals. CD-split also obtains local coverage by creating prediction bands locally on a partition of the features space. This partition is data-driven and scales to high-dimensional settings. In a wide variety of simulated scenarios, our methods have a better control of conditional coverage and have smaller length than previously proposed methods.},
  langid = {english},
  keywords = {Conformal prediction,domain adaptation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\FPA7DFPY\\Izbicki et al. - 2020 - Flexible distribution-free conditional predictive bands using density estimators.pdf;C\:\\Users\\E097600\\Zotero\\storage\\NSQLE5J9\\Izbicki et al. - 2020 - Flexible distribution-free conditional predictive bands using density estimators.pdf}
}

@article{jaccardPDFEtudeDistribution1901,
  title = {{(PDF) Etude de la distribution florale dans une portion des Alpes et du Jura}},
  author = {Jaccard, Paul},
  year = 1901,
  month = jan,
  journal = {ResearchGate},
  doi = {10.5169/seals-266450},
  urldate = {2025-01-17},
  abstract = {PDF \textbar{} Common error in bibliographies: "\'Etude comparative de la distribution florale dans une portion des Alpes et des Jura". \textbar{} Find, read and cite all the research you need on ResearchGate},
  langid = {french},
  file = {C:\Users\E097600\Zotero\storage\TG94DJ57\2024 - (PDF) Etude de la distribution florale dans une portion des Alpes et du Jura.pdf}
}

@inproceedings{jadonSurveyLossFunctions2020,
  title = {A Survey of Loss Functions for Semantic Segmentation},
  booktitle = {2020 {{IEEE Conference}} on {{Computational Intelligence}} in {{Bioinformatics}} and {{Computational Biology}} ({{CIBCB}})},
  author = {Jadon, Shruti},
  year = 2020,
  month = oct,
  eprint = {2006.14822},
  primaryclass = {eess},
  pages = {1--7},
  doi = {10.1109/CIBCB48159.2020.9277638},
  urldate = {2025-01-07},
  abstract = {Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self-driving cars. In the past five years, various papers came up with different objective loss functions used in different cases such as biased data, sparse segmentation, etc. In this paper, we have summarized some of the well-known loss functions widely used for Image Segmentation and listed out the cases where their usage can help in fast and better convergence of a model. Furthermore, we have also introduced a new log-cosh dice loss function and compared its performance on the NBFS skull-segmentation open-source data-set with widely used loss functions. We also showcased that certain loss functions perform well across all data-sets and can be taken as a good baseline choice in unknown data distribution scenarios. Our code is available at Github: https://github.com/shruti-jadon/Semantic-Segmentation-Loss-Functions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\82USDDKS\\Jadon - 2020 - A survey of loss functions for semantic segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\27L9YNCJ\\2006.html}
}

@misc{jausGoodEnoughIt2025,
  title = {Good {{Enough}}: {{Is}} It {{Worth Improving}} Your {{Label Quality}}?},
  shorttitle = {Good {{Enough}}},
  author = {Jaus, Alexander and Marinov, Zdravko and Seibold, Constantin and Rei{\ss}, Simon and Kleesiek, Jens and Stiefelhagen, Rainer},
  year = 2025,
  month = may,
  number = {arXiv:2505.20928},
  eprint = {2505.20928},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.20928},
  urldate = {2025-06-02},
  abstract = {Improving label quality in medical image segmentation is costly, but its benefits remain unclear. We systematically evaluate its impact using multiple pseudo-labeled versions of CT datasets, generated by models like nnU-Net, TotalSegmentator, and MedSAM. Our results show that while higher-quality labels improve in-domain performance, gains remain unclear if below a small threshold. For pre-training, label quality has minimal impact, suggesting that models rather transfer general concepts than detailed annotations. These findings provide guidance on when improving label quality is worth the effort.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,noisy labels},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\RW6PZIEV\\Jaus et al. - 2025 - Good Enough Is it Worth Improving your Label Quality.pdf;C\:\\Users\\E097600\\Zotero\\storage\\QGJLEEZG\\2505.html}
}

@misc{jimenezWhyMachineLearning2025,
  title = {Why {{Machine Learning Models Fail}} to {{Fully Capture Epistemic Uncertainty}}},
  author = {Jim{\'e}nez, Sebasti{\'a}n and J{\"u}rgens, Mira and Waegeman, Willem},
  year = 2025,
  month = may,
  journal = {arXiv.org},
  urldate = {2025-06-02},
  abstract = {In recent years various supervised learning methods that disentangle aleatoric and epistemic uncertainty based on second-order distributions have been proposed. We argue that these methods fail to capture critical components of epistemic uncertainty, particularly due to the often-neglected component of model bias. To show this, we make use of a more fine-grained taxonomy of epistemic uncertainty sources in machine learning models, and analyse how the classical bias-variance decomposition of the expected prediction error can be decomposed into different parts reflecting these uncertainties. By using a simulation-based evaluation protocol which encompasses epistemic uncertainty due to both procedural- and data-driven uncertainty components, we illustrate that current methods rarely capture the full spectrum of epistemic uncertainty. Through theoretical insights and synthetic experiments, we show that high model bias can lead to misleadingly low estimates of epistemic uncertainty, and common second-order uncertainty quantification methods systematically blur bias-induced errors into aleatoric estimates, thereby underrepresenting epistemic uncertainty. Our findings underscore that meaningful aleatoric estimates are feasible only if all relevant sources of epistemic uncertainty are properly represented.},
  howpublished = {https://arxiv.org/abs/2505.23506v1},
  langid = {english},
  keywords = {Uncertainty},
  file = {C:\Users\E097600\Zotero\storage\F37PUBUP\Jiménez et al. - 2025 - Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty.pdf}
}

@article{jinOneshotActiveLearning2022,
  title = {One-Shot Active Learning for Image Segmentation via Contrastive Learning and Diversity-Based Sampling},
  author = {Jin, Qiuye and Yuan, Mingzhi and Qiao, Qin and Song, Zhijian},
  year = 2022,
  month = apr,
  journal = {Knowledge-Based Systems},
  volume = {241},
  pages = {108278},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2022.108278},
  urldate = {2024-03-13},
  abstract = {Image segmentation tasks based on deep learning usually require a large number of labeled samples to obtain great performance of Convolutional Neural Networks (CNNs). However, even if samples are abundant, a major issue remains that labeling samples is usually time-consuming and costly. Active learning can select valuable samples for annotation, so as to reduce the annotation cost as much as possible while maintaining the performance of CNNs. Most existing active learning approaches work in an iterative way. However, this iterative scheme needs more interaction with experts, more labor and more computing resources. In this paper, we propose a one-shot active learning framework, i.e. Contrastive Annotation (CA) based on contrastive self-supervised learning and diversity-based query strategy, aiming to select valuable samples in one-shot. Extensive experiments on three segmentation datasets, i.e., skin lesion segmentation, remote sensing image segmentation and chest X-ray segmentation, show that our proposed CA framework outperforms state-of-the-art methods by large margins.},
  keywords = {Active learning,Contrastive learning,semantic segmentation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\M8B6L8IM\\Jin et al. - 2022 - One-shot active learning for image segmentation vi.pdf;C\:\\Users\\E097600\\Zotero\\storage\\HMKLV69T\\S0950705122000909.html}
}

@article{jiSemisupervisedBatchActive2024,
  title = {Semi-Supervised Batch Active Learning Based on Mutual Information},
  author = {Ji, Xia and Wang, LingZhu and Fang, XiaoHao},
  year = 2024,
  month = dec,
  journal = {Applied Intelligence},
  volume = {55},
  number = {2},
  pages = {117},
  issn = {1573-7497},
  doi = {10.1007/s10489-024-05962-5},
  urldate = {2025-06-26},
  abstract = {Active learning reduces the annotation cost of machine learning by selecting and querying informative unlabeled samples. Semi-supervised active learning methods can considerably utilize the regional information of unlabeled samples, and thus, more effectively select valuable samples. Existing semi-supervised batch active learning algorithms frequently exhibit poor robustness due to their high computational complexity, making handling large-scale datasets a difficult task. However, existing active learning algorithms based on high-performance semi-supervised learners adopt a single-sample selection mode, under which the model requires multiple rounds of iterative processes, significantly reducing the overall efficiency of the algorithm and affecting its practicality. To address these issues, we propose a new semi-supervised batch active learning algorithm called approximate error reduction based on mutual information (MIAER). First, we use hierarchical anchor graph regularization (HAGR) as the semi-supervised learner. HAGR exhibits good robustness and only involves a small-scale reduced Laplacian matrix in its optimization process, enabling rapid processing of large-scale datasets. Second, we propose a batch sampling strategy based on mutual information and error reduction in the sample selection stage. This strategy, which is based on hierarchical anchor graphs, first measures the uncertainty of samples by using approximate error reduction, considerably reducing computational overhead. Then, it uses mutual information to measure the diversity of samples in category space while removing redundant batch samples, preserving samples with high uncertainty as much as possible. Comparative experiments with several advanced active learning methods on a large number of datasets fully demonstrate the effectiveness and stability of MIAER.},
  langid = {english},
  keywords = {Active learning,Batch mode,Continuous Optimization,Information theory,Learning algorithms,Machine Learning,Marker Assisted Selection,Mutual information,Semi-supervised,Statistical Learning},
  file = {C:\Users\E097600\Zotero\storage\LDWQBJ6F\Ji et al. - 2024 - Semi-supervised batch active learning based on mutual information.pdf}
}

@inproceedings{johanssonModelagnosticNonconformityFunctions2017,
  title = {Model-Agnostic Nonconformity Functions for Conformal Classification},
  booktitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Johansson, Ulf and Linusson, Henrik and L{\"o}fstr{\"o}m, Tuve and Bostr{\"o}m, Henrik},
  year = 2017,
  month = may,
  pages = {2072--2079},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2017.7966105},
  urldate = {2025-11-28},
  abstract = {A conformal predictor outputs prediction regions, for classification label sets. The key property of all conformal predictors is that they are valid, i.e., their error rate on novel data is bounded by a preset significance level. Thus, the key performance metric for evaluating conformal predictors is the size of the output prediction regions, where smaller (more informative) prediction regions are said to be more efficient. All conformal predictions rely on nonconformity functions, measuring the strangeness of an input-output pair, and the efficiency depends critically on the quality of the chosen nonconformity function. In this paper, three model-agnostic nonconformity functions, based on well-known loss functions, are evaluated with regard to how they affect efficiency. In the experimentation on 21 publicly available multi-class data sets, both single neural networks and ensembles of neural networks are used as underlying models for conformal classifiers. The results show that the choice of nonconformity function has a major impact on the efficiency, but also that different nonconformity functions should be used depending on the exact efficiency metric. For a high fraction of single-label predictions, a margin-based nonconformity function is the best option, while a nonconformity function based on the hinge loss obtained the smallest label sets on average.},
  keywords = {Calibration,Classification,Conformal prediction,Data models,Iterative closest point algorithm,Neural networks,Predictive models,Training,Training data},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\HAQAWXAX\\Johansson et al. - 2017 - Model-agnostic nonconformity functions for conformal classification.pdf;C\:\\Users\\E097600\\Zotero\\storage\\36WY35DC\\7966105.html}
}

@article{johnsonSurveyDeepLearning2019,
  title = {Survey on Deep Learning with Class Imbalance},
  author = {Johnson, Justin M. and Khoshgoftaar, Taghi M.},
  year = 2019,
  month = mar,
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {27},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0192-5},
  urldate = {2025-05-27},
  abstract = {The purpose of this study is to examine existing deep learning techniques for addressing class imbalanced data. Effective classification with imbalanced data is an important area of research, as high class imbalance is naturally inherent in many real-world applications, e.g., fraud detection and cancer detection. Moreover, highly imbalanced data poses added difficulty, as most learners will exhibit bias towards the majority class, and in extreme cases, may ignore the minority class altogether. Class imbalance has been studied thoroughly over the last two decades using traditional machine learning models, i.e. non-deep learning. Despite recent advances in deep learning, along with its increasing popularity, very little empirical work in the area of deep learning with class imbalance exists. Having achieved record-breaking performance results in several complex domains, investigating the use of deep neural networks for problems containing high levels of class imbalance is of great interest. Available studies regarding class imbalance and deep learning are surveyed in order to better understand the efficacy of deep learning when applied to class imbalanced data. This survey discusses the implementation details and experimental results for each study, and offers additional insight into their strengths and weaknesses. Several areas of focus include: data complexity, architectures tested, performance interpretation, ease of use, big data application, and generalization to other domains. We have found that research in this area is very limited, that most existing work focuses on computer vision tasks with convolutional neural networks, and that the effects of big data are rarely considered. Several traditional methods for class imbalance, e.g. data sampling and cost-sensitive learning, prove to be applicable in deep learning, while more advanced methods that exploit neural network feature learning abilities show promising results. The survey concludes with a discussion that highlights various gaps in deep learning from class imbalanced data for the purpose of guiding future research.},
  keywords = {Big data,Class imbalance,Deep learning,Deep neural networks},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\9M96ANXS\\Johnson et Khoshgoftaar - 2019 - Survey on deep learning with class imbalance.pdf;C\:\\Users\\E097600\\Zotero\\storage\\YJ9YHPXL\\s40537-019-0192-5.html}
}

@misc{jonasOpenALEvaluationInterpretation2023,
  title = {{{OpenAL}}: {{Evaluation}} and {{Interpretation}} of {{Active Learning Strategies}}},
  shorttitle = {{{OpenAL}}},
  author = {Jonas, W. and Abraham, A. and {Dreyfus-Schmidt}, L.},
  year = 2023,
  month = apr,
  number = {arXiv:2304.05246},
  eprint = {2304.05246},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.05246},
  urldate = {2025-09-18},
  abstract = {Despite the vast body of literature on Active Learning (AL), there is no comprehensive and open benchmark allowing for efficient and simple comparison of proposed samplers. Additionally, the variability in experimental settings across the literature makes it difficult to choose a sampling strategy, which is critical due to the one-off nature of AL experiments. To address those limitations, we introduce OpenAL, a flexible and open-source framework to easily run and compare sampling AL strategies on a collection of realistic tasks. The proposed benchmark is augmented with interpretability metrics and statistical analysis methods to understand when and why some samplers outperform others. Last but not least, practitioners can easily extend the benchmark by submitting their own AL samplers.},
  archiveprefix = {arXiv},
  keywords = {Benchmark testing,Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Image classification},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\96J8TDYX\\Jonas et al. - 2023 - OpenAL Evaluation and Interpretation of Active Learning Strategies.pdf;C\:\\Users\\E097600\\Zotero\\storage\\VVPUTTN5\\2304.html}
}

@misc{joshiClassifyingRadioLoudRadioQuiet2025,
  title = {Classifying {{Radio-Loud}} and {{Radio-Quiet Quasars With Novel PCA Based Regression Classifier}}},
  author = {Joshi, Ramkrishna and Shinde, Vivek},
  year = 2025,
  month = may,
  number = {arXiv:2505.01335},
  eprint = {2505.01335},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.01335},
  urldate = {2025-05-13},
  abstract = {The problem of quasar classification comes in the class of highly imbalanced classification problems since Radio-loud (RL) quasars are rare and make up only about 10\% of the quasar population. In this work, we use the Sloan Digital Sky Survey-DR3 dataset and introduce a PCA-based regression pipeline designed to maximize recall for rare classes in class-imbalanced astronomical data. We demonstrate an effective methodology to identify the key features of the dataset and apply Principal Component Analysis (PCA) for dimensionality reduction. For the PCA transformed SDSS-DR3 dataset, first two components account for the 97\% of the observed variance. We perform classification of Radio-Loud (RL) and Radio-Quiet (RQ) quasars with Random Forest Classifier (RFC), novel PCA based balanced linear regression classifier (PBC), Random forest integrated with SMOTE classifier and XGBoost classifier with threshold tuning. RFC achieves an overall accuracy of 92\% while PBC achieves an overall accuracy of 62\%. XGBoost achieves an overall accuracy of 72\% and SMOTE integrated RFC achieves an accuracy of 85\%. Higher precision is obtained for RQ quasars in all classification methods. For the RL class, RFC achieves a recall of 0.04, XGBoost achieves a recall of 0.39, SMOTE integrated RFC achieves a recall of 0.25 and PBC achieves a recall of 0.52 attributed to the balanced logistic regression. RFC and PBC achieve F1 score of 0.08 and 0.19 respectively for RL while XGBoost achieves an improved F1 score of 0.22 but at the cost of reduced recall of the RL class. SMOTE integrated RFC achieves a better F1 score of 0.21 over RFC and PBC. Overall results of classifiers point to extreme class imbalance between RQ and RL classes in the data set.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Astrophysics of Galaxies},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\JEIIMQAJ\\Joshi et Shinde - 2025 - Classifying Radio-Loud and Radio-Quiet Quasars With Novel PCA Based Regression Classifier.pdf;C\:\\Users\\E097600\\Zotero\\storage\\59VMNPTH\\2505.html}
}

@inproceedings{joshiMulticlassActiveLearning2009,
  title = {Multi-Class Active Learning for Image Classification},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Joshi, Ajay J. and Porikli, Fatih and Papanikolopoulos, Nikolaos},
  year = 2009,
  month = jun,
  pages = {2372--2379},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206627},
  urldate = {2024-03-08},
  abstract = {One of the principal bottlenecks in applying learning techniques to classification problems is the large amount of labeled training data required. Especially for images and video, providing training data is very expensive in terms of human time and effort. In this paper we propose an active learning approach to tackle the problem. Instead of passively accepting random training examples, the active learning algorithm iteratively selects unlabeled examples for the user to label, so that human effort is focused on labeling the most ``useful'' examples. Our method relies on the idea of uncertainty sampling, in which the algorithm selects unlabeled examples that it finds hardest to classify. Specifically, we propose an uncertainty measure that generalizes margin-based uncertainty to the multi-class case and is easy to compute, so that active learning can handle a large number of classes and large data sizes efficiently. We demonstrate results for letter and digit recognition on datasets from the UCI repository, object recognition results on the Caltech-101 dataset, and scene categorization results on a dataset of 13 natural scene categories. The proposed method gives large reductions in the number of training examples required over random selection to achieve similar classification accuracy, with little computational overhead.},
  keywords = {Humans,Image classification,Iterative algorithms,Labeling,Layout,Measurement uncertainty,Object recognition,Sampling methods,Size measurement,Training data},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\FFG8H7RB\\5206627.html;C\:\\Users\\E097600\\Zotero\\storage\\VED34945\\5206627.html;C\:\\Users\\E097600\\Zotero\\storage\\WAI9QRJ2\\5206627.html}
}

@unpublished{kambaleAutoscalingServerlessPlatforms2025,
  title = {Autoscaling in {{Serverless Platforms}} via {{Online Learning}} with {{Convergence Guarantees}}},
  author = {Kambale, Abednego Wamuhindo and ANSELMI, Jonatha and Ardagna, Danilo and Gaujal, Bruno},
  year = 2025,
  urldate = {2025-11-27},
  abstract = {As the adoption of serverless computing platforms continue to grow, designing autoscaling policies that strike the right balance between energy efficiency and user-perceived performance has become a central challenge. In this work, we propose an online learning algorithm with theoretical convergence guarantees that dynamically tunes control parameters in a serverless autoscaling environment. The proposed algorithm, grounded in stochastic gradient descent, learns online-during the actual operation of the platform-the optimal values of three key control parameters: (i) the target stock size of prewarmed (idle) functions, (ii) the threshold triggering provisioning actions, and (iii) the expiration rate of idle resources. We prove that, under Markovian dynamics, the algorithm converges to the parameter set that minimizes a cost function capturing the tradeoff between energy consumption and response latency. In addition, we demonstrate that its structure naturally supports parallelization, significantly accelerating convergence.Extensive numerical experiments show that our method outperforms existing baselines, including recent deep learning-based approaches, even under non-Markovian settings-highlighting both its robustness and practical viability for next-generation serverless infrastructures.},
  keywords = {Autoscaling,online learning,serverless functions,stochastic gradient descent},
  file = {C:\Users\E097600\Zotero\storage\C5QAM4WB\Kambale et al. - 2025 - Autoscaling in Serverless Platforms via Online Learning with Convergence Guarantees.pdf}
}

@article{kangExploitingLightweightHierarchical2025,
  title = {Exploiting {{Lightweight Hierarchical ViT}} and {{Dynamic Framework}} for {{Efficient Visual Tracking}}},
  author = {Kang, Ben and Chen, Xin and Zhao, Jie and Bo, Chunjuan and Wang, Dong and Lu, Huchuan},
  year = 2025,
  month = jun,
  journal = {International Journal of Computer Vision},
  issn = {1573-1405},
  doi = {10.1007/s11263-025-02500-9},
  urldate = {2025-09-22},
  abstract = {Transformer-based visual trackers have demonstrated significant advancements due to their powerful modeling capabilities. However, their practicality is limited on resource-constrained devices because of their slow processing speeds. To address this challenge, we present HiT, a novel family of efficient tracking models that achieve high performance while maintaining fast operation across various devices. The core innovation of HiT lies in its Bridge Module, which connects lightweight transformers to the tracking framework, enhancing feature representation quality. Additionally, we introduce a dual-image position encoding approach to effectively encode spatial information. HiT achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson AGX platform, alongside a competitive AUC of 64.6\% on the LaSOT benchmark, outperforming all previous efficient trackers. Building on HiT, we propose DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by selecting routes with varying computational requirements. DyHiT uses search area features extracted by the backbone network and inputs them into an efficient dynamic router to classify tracking scenarios. Based on the classification, DyHiT applies a divide-and-conquer strategy, selecting appropriate routes to achieve a superior trade-off between accuracy and speed. The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while maintaining an AUC of 62.4\% on LaSOT. Furthermore, we introduce a training-free acceleration method based on the dynamic routing architecture of DyHiT. This method significantly improves the execution speed of various high-performance trackers without sacrificing accuracy. For instance, our acceleration method enables the state-of-the-art tracker SeqTrack-B256 to achieve a \$\$2.68\textbackslash times \$\$speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of 69.9\% on the LaSOT. Codes, models, and results are available at https://github.com/kangben258/HiT.},
  langid = {english},
  keywords = {Divide-and-Conquer Strategy,Dynamic Routing,Efficient Tracking,Hierarchical Transformer,Object Tracking,vit},
  file = {C:\Users\E097600\Zotero\storage\BRRDIYQG\Kang et al. - 2025 - Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking.pdf}
}

@misc{keskarLargeBatchTrainingDeep2017,
  title = {On {{Large-Batch Training}} for {{Deep Learning}}: {{Generalization Gap}} and {{Sharp Minima}}},
  shorttitle = {On {{Large-Batch Training}} for {{Deep Learning}}},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  year = 2017,
  month = feb,
  number = {arXiv:1609.04836},
  eprint = {1609.04836},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1609.04836},
  urldate = {2024-12-17},
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\TC8II4GY\\Keskar et al. - 2017 - On Large-Batch Training for Deep Learning Generalization Gap and Sharp Minima.pdf;C\:\\Users\\E097600\\Zotero\\storage\\45HEZQUH\\1609.html}
}

@inproceedings{kharazianCoPALConformalPrediction2024,
  title = {{{CoPAL}}: {{Conformal Prediction}} in {{Active Learning An Algorithm}} for {{Enhancing Remaining Useful Life Estimation}} in {{Predictive Maintenance}}},
  shorttitle = {{{CoPAL}}},
  booktitle = {Proceedings of the {{Thirteenth Symposium}} on {{Conformal}} and {{Probabilistic Prediction}} with {{Applications}}},
  author = {Kharazian, Zahra and Lindgren, Tony and Magnusson, Sindri and Bostr{\"o}m, Henrik},
  year = 2024,
  month = sep,
  pages = {195--217},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-10-20},
  abstract = {Active learning has received considerable attention as an approach to obtain high predictive performance while minimizing the labeling effort. A central component of the active learning framework concerns the selection of objects for labeling, which are used for iteratively updating the underlying model. In this work, an algorithm called CoPAL (Conformal Prediction for Active Learning) is proposed, which makes the selection of objects within active learning based on the uncertainty as quantified by conformal prediction. The efficacy of CoPAL is investigated by considering the task of estimating the remaining useful life (RUL) of assets in the domain of predictive maintenance (PdM). Experimental results are presented, encompassing diverse setups, including different models, sample selection criteria, conformal predictors, and datasets, using root mean squared error (RMSE) as the primary evaluation metric while also reporting prediction interval sizes over the iterations. The comprehensive analysis confirms the positive effect of using CoPAL for improving predictive performance.},
  langid = {english},
  keywords = {Active learning,Conformal prediction},
  file = {C:\Users\E097600\Zotero\storage\BS72KB7P\Kharazian et al. - 2024 - CoPAL Conformal Prediction in Active Learning An Algorithm for Enhancing Remaining Useful Life Esti.pdf}
}

@misc{kimDiffusionDrivenTwoStageActive2025,
  title = {Diffusion-{{Driven Two-Stage Active Learning}} for {{Low-Budget Semantic Segmentation}}},
  author = {Kim, Jeongin and Bae, Wonho and Han, YouLee and Oh, Giyeong and Yu, Youngjae and Sutherland, Danica J. and Noh, Junhyug},
  year = 2025,
  month = oct,
  number = {arXiv:2510.22229},
  eprint = {2510.22229},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.22229},
  urldate = {2025-10-28},
  abstract = {Semantic segmentation demands dense pixel-level annotations, which can be prohibitively expensive - especially under extremely constrained labeling budgets. In this paper, we address the problem of low-budget active learning for semantic segmentation by proposing a novel two-stage selection pipeline. Our approach leverages a pre-trained diffusion model to extract rich multi-scale features that capture both global structure and fine details. In the first stage, we perform a hierarchical, representation-based candidate selection by first choosing a small subset of representative pixels per image using MaxHerding, and then refining these into a diverse global pool. In the second stage, we compute an entropy-augmented disagreement score (eDALD) over noisy multi-scale diffusion features to capture both epistemic uncertainty and prediction confidence, selecting the most informative pixels for annotation. This decoupling of diversity and uncertainty lets us achieve high segmentation accuracy with only a tiny fraction of labeled pixels. Extensive experiments on four benchmarks (CamVid, ADE-Bed, Cityscapes, and Pascal-Context) demonstrate that our method significantly outperforms existing baselines under extreme pixel-budget regimes. Our code is available at https://github.com/jn-kim/two-stage-edald.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Computer Vision and Pattern Recognition,diffusion,semantic segmentation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\R4USGRC8\\Kim et al. - 2025 - Diffusion-Driven Two-Stage Active Learning for Low-Budget Semantic Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\C8N57TZ7\\2510.html}
}

@misc{kimProxyAnchorLoss2020,
  title = {Proxy {{Anchor Loss}} for {{Deep Metric Learning}}},
  author = {Kim, Sungyeon and Kim, Dongwon and Cho, Minsu and Kwak, Suha},
  year = 2020,
  month = mar,
  number = {arXiv:2003.13911},
  eprint = {2003.13911},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.13911},
  urldate = {2024-03-12},
  abstract = {Existing metric learning losses can be categorized into two classes: pair-based and proxy-based losses. The former class can leverage fine-grained semantic relations between data points, but slows convergence in general due to its high training complexity. In contrast, the latter class enables fast and reliable convergence, but cannot consider the rich data-to-data relations. This paper presents a new proxy-based loss that takes advantages of both pair- and proxy-based methods and overcomes their limitations. Thanks to the use of proxies, our loss boosts the speed of convergence and is robust against noisy labels and outliers. At the same time, it allows embedding vectors of data to interact with each other in its gradients to exploit data-to-data relations. Our method is evaluated on four public benchmarks, where a standard network trained with our loss achieves state-of-the-art performance and most quickly converges.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\E097600\Zotero\storage\SBIKYZQN\2003.html}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = 2017,
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2025-09-16},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\MR4XCBJG\\Kingma et Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;C\:\\Users\\E097600\\Zotero\\storage\\69G4548X\\1412.html}
}

@misc{kirillovPanopticFeaturePyramid2019,
  title = {Panoptic {{Feature Pyramid Networks}}},
  author = {Kirillov, Alexander and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  year = 2019,
  month = apr,
  number = {arXiv:1901.02446},
  eprint = {1901.02446},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.02446},
  urldate = {2025-01-06},
  abstract = {The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\PVQIJFZJ\\Kirillov et al. - 2019 - Panoptic Feature Pyramid Networks.pdf;C\:\\Users\\E097600\\Zotero\\storage\\P88DV6HT\\1901.html}
}

@misc{kirschBatchBALDEfficientDiverse2019,
  title = {{{BatchBALD}}: {{Efficient}} and {{Diverse Batch Acquisition}} for {{Deep Bayesian Active Learning}}},
  shorttitle = {{{BatchBALD}}},
  author = {Kirsch, Andreas and van Amersfoort, Joost and Gal, Yarin},
  year = 2019,
  month = oct,
  number = {arXiv:1906.08158},
  eprint = {1906.08158},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.08158},
  urldate = {2025-09-17},
  abstract = {We develop BatchBALD, a tractable approximation to the mutual information between a batch of points and model parameters, which we use as an acquisition function to select multiple informative points jointly for the task of deep Bayesian active learning. BatchBALD is a greedy linear-time \$1 - \textbackslash frac\textbraceleft 1\textbraceright\textbraceleft e\textbraceright\$-approximate algorithm amenable to dynamic programming and efficient caching. We compare BatchBALD to the commonly used approach for batch data acquisition and find that the current approach acquires similar and redundant points, sometimes performing worse than randomly acquiring data. We finish by showing that, using BatchBALD to consider dependencies within an acquisition batch, we achieve new state of the art performance on standard benchmarks, providing substantial data efficiency improvements in batch acquisition.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,Image classification,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\W7K2SG74\\Kirsch et al. - 2019 - BatchBALD Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\Q3KBJKHY\\1906.html}
}

@misc{kirschStochasticBatchAcquisition2023,
  title = {Stochastic {{Batch Acquisition}}: {{A Simple Baseline}} for {{Deep Active Learning}}},
  shorttitle = {Stochastic {{Batch Acquisition}}},
  author = {Kirsch, Andreas and Farquhar, Sebastian and Atighehchian, Parmida and Jesson, Andrew and {Branchaud-Charron}, Frederic and Gal, Yarin},
  year = 2023,
  month = sep,
  number = {arXiv:2106.12059},
  eprint = {2106.12059},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.12059},
  urldate = {2025-10-28},
  abstract = {We examine a simple stochastic strategy for adapting well-known single-point acquisition functions to allow batch active learning. Unlike acquiring the top-K points from the pool set, score- or rank-based sampling takes into account that acquisition scores change as new data are acquired. This simple strategy for adapting standard single-sample acquisition strategies can even perform just as well as compute-intensive state-of-the-art batch acquisition functions, like BatchBALD or BADGE, while using orders of magnitude less compute. In addition to providing a practical option for machine learning practitioners, the surprising success of the proposed method in a wide range of experimental settings raises a difficult question for the field: when are these expensive batch acquisition methods pulling their weight?},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,Image classification,meta active learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\FM9Q94KF\\Kirsch et al. - 2023 - Stochastic Batch Acquisition A Simple Baseline for Deep Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\4R98RKLA\\2106.html}
}

@misc{kirschUnifyingApproachesActive2022,
  title = {Unifying {{Approaches}} in {{Active Learning}} and {{Active Sampling}} via {{Fisher Information}} and {{Information-Theoretic Quantities}}},
  author = {Kirsch, Andreas and Gal, Yarin},
  year = 2022,
  month = nov,
  number = {arXiv:2208.00549},
  eprint = {2208.00549},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.00549},
  urldate = {2025-02-28},
  abstract = {Recently proposed methods in data subset selection, that is active learning and active sampling, use Fisher information, Hessians, similarity matrices based on gradients, and gradient lengths to estimate how informative data is for a model's training. Are these different approaches connected, and if so, how? We revisit the fundamentals of Bayesian optimal experiment design and show that these recently proposed methods can be understood as approximations to information-theoretic quantities: among them, the mutual information between predictions and model parameters, known as expected information gain or BALD in machine learning, and the mutual information between predictions of acquisition candidates and test samples, known as expected predictive information gain. We develop a comprehensive set of approximations using Fisher information and observed information and derive a unified framework that connects seemingly disparate literature. Although Bayesian methods are often seen as separate from non-Bayesian ones, the sometimes fuzzy notion of "informativeness" expressed in various non-Bayesian objectives leads to the same couple of information quantities, which were, in principle, already known by Lindley (1956) and MacKay (1992).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Information Theory},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\LI6GRCRW\\Kirsch et Gal - 2022 - Unifying Approaches in Active Learning and Active Sampling via Fisher Information and Information-Th.pdf;C\:\\Users\\E097600\\Zotero\\storage\\6U37UT5N\\2208.html}
}

@misc{klieAnnotationErrorDetection2022,
  title = {Annotation {{Error Detection}}: {{Analyzing}} the {{Past}} and {{Present}} for a {{More Coherent Future}}},
  shorttitle = {Annotation {{Error Detection}}},
  author = {Klie, Jan-Christoph and Webber, Bonnie and Gurevych, Iryna},
  year = 2022,
  month = sep,
  number = {arXiv:2206.02280},
  eprint = {2206.02280},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.02280},
  urldate = {2025-02-10},
  abstract = {Annotated data is an essential ingredient in natural language processing for training and evaluating machine learning models. It is therefore very desirable for the annotations to be of high quality. Recent work, however, has shown that several popular datasets contain a surprising amount of annotation errors or inconsistencies. To alleviate this issue, many methods for annotation error detection have been devised over the years. While researchers show that their approaches work well on their newly introduced datasets, they rarely compare their methods to previous work or on the same datasets. This raises strong concerns on methods' general performance and makes it difficult to asses their strengths and weaknesses. We therefore reimplement 18 methods for detecting potential annotation errors and evaluate them on 9 English datasets for text classification as well as token and span labeling. In addition, we define a uniform evaluation setup including a new formalization of the annotation error detection task, evaluation protocol and general best practices. To facilitate future research and reproducibility, we release our datasets and implementations in an easy-to-use and open source software package.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\ICQI95NL\\Klie et al. - 2022 - Annotation Error Detection Analyzing the Past and Present for a More Coherent Future.pdf;C\:\\Users\\E097600\\Zotero\\storage\\MYKDSR7C\\2206.html}
}

@article{korschSimplifiedConcreteDropout2025,
  title = {Simplified {{Concrete Dropout}} - {{Improving}} the {{Generation}} of {{Attribution Masks}} for {{Fine-grained Classification}}},
  author = {Korsch, Dimitri and Shadaydeh, Maha and Denzler, Joachim},
  year = 2025,
  month = aug,
  journal = {International Journal of Computer Vision},
  volume = {133},
  number = {8},
  pages = {5857--5871},
  issn = {1573-1405},
  doi = {10.1007/s11263-025-02453-z},
  urldate = {2025-09-22},
  abstract = {In fine-grained classification, which is classifying images into subcategories within a common broader category, it is crucial to have precise visual explanations of the classification model's decision. While commonly used attention- or gradient-based methods deliver either too coarse or too noisy explanations unsuitable for highlighting subtle visual differences reliably, perturbation-based methods can precisely locate pixels causally responsible for the predicted category. The fill-in of the dropout (FIDO) algorithm is one of those methods, which utilizes concrete dropout (CD) to sample a set of attribution masks and updates the sampling parameters based on the output of the classification model. In this paper, we present a solution against the high variance in the gradient estimates, a known problem of the FIDO algorithm that has been mitigated until now by large mini-batch updates of the sampling parameters. First, our solution allows for estimating the parameters with smaller mini-batch sizes without losing the quality of the estimates but with a reduced computational effort. Next, our method produces finer and more coherent attribution masks. Finally, we use the resulting attribution masks to improve the classification performance on three fine-grained datasets without additional fine-tuning steps and achieve results that are otherwise only achieved if ground truth bounding boxes are used.},
  langid = {english},
  keywords = {attribution masks,concrete dropout,fine-grained classification,gradient stability,Perturbation-based counterfactuals},
  file = {C:\Users\E097600\Zotero\storage\UH8C7PHQ\Korsch et al. - 2025 - Simplified Concrete Dropout - Improving the Generation of Attribution Masks for Fine-grained Classif.pdf}
}

@article{kottkeChallengesReliableRealistic2017,
  title = {Challenges of {{Reliable}}, {{Realistic}} and {{Comparable Active Learning Evaluation}}},
  author = {Kottke, Daniel and Calma, Adrian and Huseljic, Denis and Krempl, G.M. and Sick, Bernhard},
  year = 2017,
  month = sep,
  journal = {Proceedings of the Workshop and Tutorial on Interactive Adaptive Learning},
  pages = {2--14},
  abstract = {Active learning has the potential to save costs by intelligent use of resources in form of some expert's knowledge. Nevertheless, these methods are still not established in real-world applications as they can not be evaluated properly in the specific scenario because evaluation data is missing. In this article, we provide a summary of different evaluation methodologies by discussing them in terms of being reproducible, comparable, and realistic. A pilot study which compares the results of different exhaustive evaluations suggests a lack in repetitions in many articles. Furthermore, we aim to start a discussion on a gold standard evaluation setup for active learning that ensures comparability without reimplementing algorithms.},
  keywords = {Active learning,Classification,Data Mining,Evaluation,method,Semi-supervised Learning},
  file = {C:\Users\E097600\Zotero\storage\XRIUHTGI\Kottke et al. - 2017 - Challenges of Reliable, Realistic and Comparable Active Learning Evaluation.pdf}
}

@misc{kouwIntroductionDomainAdaptation2019,
  title = {An Introduction to Domain Adaptation and Transfer Learning},
  author = {Kouw, Wouter M. and Loog, Marco},
  year = 2019,
  month = jan,
  number = {arXiv:1812.11806},
  eprint = {1812.11806},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.11806},
  urldate = {2025-11-05},
  abstract = {In machine learning, if the training data is an unbiased sample of an underlying distribution, then the learned classification function will make accurate predictions for new samples. However, if the training data is not an unbiased sample, then there will be differences between how the training data is distributed and how the test data is distributed. Standard classifiers cannot cope with changes in data distributions between training and test phases, and will not perform well. Domain adaptation and transfer learning are sub-fields within machine learning that are concerned with accounting for these types of changes. Here, we present an introduction to these fields, guided by the question: when and how can a classifier generalize from a source to a target domain? We will start with a brief introduction into risk minimization, and how transfer learning and domain adaptation expand upon this framework. Following that, we discuss three special cases of data set shift, namely prior, covariate and concept shift. For more complex domain shifts, there are a wide variety of approaches. These are categorized into: importance-weighting, subspace mapping, domain-invariant spaces, feature augmentation, minimax estimators and robust algorithms. A number of points will arise, which we will discuss in the last section. We conclude with the remark that many open questions will have to be addressed before transfer learners and domain-adaptive classifiers become practical.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,domain adaptation,review,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\DJE6ENCN\\Kouw et Loog - 2019 - An introduction to domain adaptation and transfer learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\LAFH8MQX\\1812.html}
}

@misc{krishnanMitigatingSamplingBias2021,
  title = {Mitigating {{Sampling Bias}} and {{Improving Robustness}} in {{Active Learning}}},
  author = {Krishnan, Ranganath and Sinha, Alok and Ahuja, Nilesh and Subedar, Mahesh and Tickoo, Omesh and Iyer, Ravi},
  year = 2021,
  month = sep,
  journal = {arXiv.org},
  urldate = {2025-03-19},
  abstract = {This paper presents simple and efficient methods to mitigate sampling bias in active learning while achieving state-of-the-art accuracy and model robustness. We introduce supervised contrastive active learning by leveraging the contrastive loss for active learning under a supervised setting. We propose an unbiased query strategy that selects informative data samples of diverse feature representations with our methods: supervised contrastive active learning (SCAL) and deep feature modeling (DFM). We empirically demonstrate our proposed methods reduce sampling bias, achieve state-of-the-art accuracy and model calibration in an active learning setup with the query computation 26x faster than Bayesian active learning by disagreement and 11x faster than CoreSet. The proposed SCAL method outperforms by a big margin in robustness to dataset shift and out-of-distribution.},
  howpublished = {https://arxiv.org/abs/2109.06321v1},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\3P2XLMSY\Krishnan et al. - 2021 - Mitigating Sampling Bias and Improving Robustness in Active Learning.pdf}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = 2017,
  month = may,
  journal = {Commun. ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782},
  doi = {10.1145/3065386},
  urldate = {2025-02-07},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  file = {C:\Users\E097600\Zotero\storage\9XE35AAB\Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional neural networks.pdf}
}

@article{kuleszaDeterminantalPointProcesses2012,
  title = {Determinantal Point Processes for Machine Learning},
  author = {Kulesza, Alex and Taskar, Ben},
  year = 2012,
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {5},
  number = {2-3},
  eprint = {1207.6083},
  primaryclass = {stat},
  pages = {123--286},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000044},
  urldate = {2025-05-06},
  abstract = {Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. We provide a gentle introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and show how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories.},
  archiveprefix = {arXiv},
  keywords = {basics,Computer Science - Information Retrieval,Computer Science - Machine Learning,determinantal point process,dpp,introduction,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\NUMBKHE7\\Kulesza et Taskar - 2012 - Determinantal point processes for machine learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ECUPJCEY\\1207.html}
}

@inproceedings{kuoCostSensitiveActiveLearning2018,
  title = {Cost-{{Sensitive Active Learning}} for {{Intracranial Hemorrhage Detection}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2018},
  author = {Kuo, Weicheng and H{\"a}ne, Christian and Yuh, Esther and Mukherjee, Pratik and Malik, Jitendra},
  editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and {Alberola-L{\'o}pez}, Carlos and Fichtinger, Gabor},
  year = 2018,
  pages = {715--723},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-00931-1_82},
  abstract = {Deep learning for clinical applications is subject to stringent performance requirements, which raises a need for large labeled datasets. However, the enormous cost of labeling medical data makes this challenging. In this paper, we build a cost-sensitive active learning system for the problem of intracranial hemorrhage detection and segmentation on head computed tomography (CT). We show that our ensemble method compares favorably with the state-of-the-art, while running faster and using less memory. Moreover, our experiments are done using a substantially larger dataset than earlier papers on this topic. Since the labeling time could vary tremendously across examples, we model the labeling time and optimize the return on investment. We validate this idea by core-set selection on our large labeled dataset and by growing it with data from the wild.},
  isbn = {978-3-030-00931-1},
  langid = {english},
  keywords = {Active learning,Artificial intelligence,Computer aided diagnosis,Segmentation,semantic segmentation},
  file = {C:\Users\E097600\Zotero\storage\YS4S2LBW\Kuo et al. - 2018 - Cost-Sensitive Active Learning for Intracranial Hemorrhage Detection.pdf}
}

@misc{lahlouDEUPDirectEpistemic2023,
  title = {{{DEUP}}: {{Direct Epistemic Uncertainty Prediction}}},
  shorttitle = {{{DEUP}}},
  author = {Lahlou, Salem and Jain, Moksh and Nekoei, Hadi and Butoi, Victor Ion and Bertin, Paul and {Rector-Brooks}, Jarrid and Korablyov, Maksym and Bengio, Yoshua},
  year = 2023,
  month = feb,
  number = {arXiv:2102.08501},
  eprint = {2102.08501},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.08501},
  urldate = {2025-03-03},
  abstract = {Epistemic Uncertainty is a measure of the lack of knowledge of a learner which diminishes with more evidence. While existing work focuses on using the variance of the Bayesian posterior due to parameter uncertainty as a measure of epistemic uncertainty, we argue that this does not capture the part of lack of knowledge induced by model misspecification. We discuss how the excess risk, which is the gap between the generalization error of a predictor and the Bayes predictor, is a sound measure of epistemic uncertainty which captures the effect of model misspecification. We thus propose a principled framework for directly estimating the excess risk by learning a secondary predictor for the generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. We discuss the merits of this novel measure of epistemic uncertainty, and highlight how it differs from variance-based measures of epistemic uncertainty and addresses its major pitfall. Our framework, Direct Epistemic Uncertainty Prediction (DEUP) is particularly interesting in interactive learning environments, where the learner is allowed to acquire novel examples in each round. Through a wide set of experiments, we illustrate how existing methods in sequential model optimization can be improved with epistemic uncertainty estimates from DEUP, and how DEUP can be used to drive exploration in reinforcement learning. We also evaluate the quality of uncertainty estimates from DEUP for probabilistic image classification and predicting synergies of drug combinations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\BWC9JDYJ\\Lahlou et al. - 2023 - DEUP Direct Epistemic Uncertainty Prediction.pdf;C\:\\Users\\E097600\\Zotero\\storage\\3FG4P3T3\\2102.html}
}

@misc{liBALBalancingDiversity2023,
  title = {{{BAL}}: {{Balancing Diversity}} and {{Novelty}} for {{Active Learning}}},
  shorttitle = {{{BAL}}},
  author = {Li, Jingyao and Chen, Pengguang and Yu, Shaozuo and Liu, Shu and Jia, Jiaya},
  year = 2023,
  month = dec,
  number = {arXiv:2312.15944},
  eprint = {2312.15944},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.15944},
  urldate = {2024-03-08},
  abstract = {The objective of Active Learning is to strategically label a subset of the dataset to maximize performance within a predetermined labeling budget. In this study, we harness features acquired through self-supervised learning. We introduce a straightforward yet potent metric, Cluster Distance Difference, to identify diverse data. Subsequently, we introduce a novel framework, Balancing Active Learning (BAL), which constructs adaptive sub-pools to balance diverse and uncertain data. Our approach outperforms all established active learning methods on widely recognized benchmarks by 1.20\%. Moreover, we assess the efficacy of our proposed framework under extended settings, encompassing both larger and smaller labeling budgets. Experimental results demonstrate that, when labeling 80\% of the samples, the performance of the current SOTA method declines by 0.74\%, whereas our proposed BAL achieves performance comparable to the full dataset. Codes are available at https://github.com/JulietLJY/BAL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\JHXKIUSB\\Li et al. - 2023 - BAL Balancing Diversity and Novelty for Active Le.pdf;C\:\\Users\\E097600\\Zotero\\storage\\Y7R9HLW2\\2312.html}
}

@article{liBALBalancingDiversity2024,
  title = {{{BAL}}: {{Balancing Diversity}} and {{Novelty}} for {{Active Learning}}},
  shorttitle = {{{BAL}}},
  author = {Li, Jingyao and Chen, Pengguang and Yu, Shaozuo and Liu, Shu and Jia, Jiaya},
  year = 2024,
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {46},
  number = {5},
  pages = {3653--3664},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2023.3345844},
  urldate = {2025-10-29},
  abstract = {The objective of Active Learning is to strategically label a subset of the dataset to maximize performance within a predetermined labeling budget. In this study, we harness features acquired through self-supervised learning. We introduce a straightforward yet potent metric, Cluster Distance Difference, to identify diverse data. Subsequently, we introduce a novel framework, Balancing Active Learning (BAL), which constructs adaptive sub-pools to balance diverse and uncertain data. Our approach outperforms all established active learning methods on widely recognized benchmarks by 1.20\%. Moreover, we assess the efficacy of our proposed framework under extended settings, encompassing both larger and smaller labeling budgets. Experimental results demonstrate that, when labeling 80\% of the samples, the performance of the current SOTA method declines by 0.74\%, whereas our proposed BAL achieves performance comparable to the full dataset.},
  keywords = {Active learning,contrastive learning,Feature extraction,hybrid,Image classification,Image color analysis,Labeling,Measurement uncertainty,Task analysis,Uncertainty},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\HXRBVZLY\\Li et al. - 2024 - BAL Balancing Diversity and Novelty for Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\GEPIY8J4\\10372131.html}
}

@misc{liBenchmarkingDetectionTransfer2021,
  title = {Benchmarking {{Detection Transfer Learning}} with {{Vision Transformers}}},
  author = {Li, Yanghao and Xie, Saining and Chen, Xinlei and Dollar, Piotr and He, Kaiming and Girshick, Ross},
  year = 2021,
  month = nov,
  number = {arXiv:2111.11429},
  eprint = {2111.11429},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.11429},
  urldate = {2025-01-06},
  abstract = {Object detection is a central downstream task used to test if pre-trained network parameters confer benefits, such as improved accuracy or training speed. The complexity of object detection methods can make this benchmarking non-trivial when new architectures, such as Vision Transformer (ViT) models, arrive. These difficulties (e.g., architectural incompatibility, slow training, high memory consumption, unknown training formulae, etc.) have prevented recent studies from benchmarking detection transfer learning with standard ViT models. In this paper, we present training techniques that overcome these challenges, enabling the use of standard ViT models as the backbone of Mask R-CNN. These tools facilitate the primary goal of our study: we compare five ViT initializations, including recent state-of-the-art self-supervised learning methods, supervised initialization, and a strong random initialization baseline. Our results show that recent masking-based unsupervised learning methods may, for the first time, provide convincing transfer learning improvements on COCO, increasing box AP up to 4\% (absolute) over supervised and prior self-supervised pre-training methods. Moreover, these masking-based initializations scale better, with the improvement growing as model size increases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\YSCZKQ4L\\Li et al. - 2021 - Benchmarking Detection Transfer Learning with Vision Transformers.pdf;C\:\\Users\\E097600\\Zotero\\storage\\PW5EEZPK\\2111.html}
}

@misc{liDeepActiveLearning2024,
  title = {Deep {{Active Learning}} with {{Noise Stability}}},
  author = {Li, Xingjian and Yang, Pengkun and Gu, Yangcheng and Zhan, Xueying and Wang, Tianyang and Xu, Min and Xu, Chengzhong},
  year = 2024,
  month = feb,
  number = {arXiv:2205.13340},
  eprint = {2205.13340},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.13340},
  urldate = {2025-02-03},
  abstract = {Uncertainty estimation for unlabeled data is crucial to active learning. With a deep neural network employed as the backbone model, the data selection process is highly challenging due to the potential over-confidence of the model inference. Existing methods resort to special learning fashions (e.g. adversarial) or auxiliary models to address this challenge. This tends to result in complex and inefficient pipelines, which would render the methods impractical. In this work, we propose a novel algorithm that leverages noise stability to estimate data uncertainty. The key idea is to measure the output derivation from the original observation when the model parameters are randomly perturbed by noise. We provide theoretical analyses by leveraging the small Gaussian noise theory and demonstrate that our method favors a subset with large and diverse gradients. Our method is generally applicable in various tasks, including computer vision, natural language processing, and structural data analysis. It achieves competitive performance compared against state-of-the-art active learning baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\Q3LRIGDN\\Li et al. - 2024 - Deep Active Learning with Noise Stability.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ZVA3KEN9\\2205.html}
}

@phdthesis{liesSemiSupervisedActive2023,
  title = {Semi {{Supervised Active Learning}} with {{Explicit Mislabel Modeling}}: {{An Application}} to {{Material Design}}},
  shorttitle = {Semi {{Supervised Active Learning}} with {{Explicit Mislabel Modeling}}},
  author = {Lies, Hadjadj},
  year = 2023,
  month = mar,
  urldate = {2024-12-19},
  abstract = {Machine Learning predictive models have been applied to many fields and applications so far. The majority of these learning algorithms rely on labeled training data which may be expensive to obtain as they require labeling by an expert. Additionally, with the new storage capabilities, large amounts of unlabeled data exist in abundance. In this context, the development of new frameworks to learn efficient models from a small set of labeled data, together with a large amount of unlabeled data, is a crucial emphasis of the current research community. Achieving this goal would significantly elevate the state-of-the-art machine intelligence to be comparable to or surpass the human capability of learning to generalize concepts from very few labeled examples. Semi-supervised learning and active learning are two ongoing active research sub- domains that aim to achieve this goal. In this thesis, we investigate two directions in machine learning theory for semi- supervised and active learning. First, We are interested in the generalization proper- ties of a self-training algorithm using halfspaces with explicit mislabel modeling. We propose an iterative algorithm to learn a list of halfspaces from labeled and unlabeled training data, in which each iteration consists of two steps, exploration and pruning. We derive a generalization bound for the proposed algorithm under a Massart noise mislabeling model. Second, we propose a meta-approach for pool-based active learn- ing strategies in the context of multi-class classification tasks, which relies on the proposed concept of learning on Proper Topological Regions (PTR) with an under- lying smoothness assumption on the metric space. PTR allows the pool-based active learning strategies to obtain a better initial training set than random selection and increase the training sample size during the rounds while operating in a low-budget regime scenario. Experiments carried out on various benchmarks demonstrate the ef- ficiency of our proposed approaches for semi-supervised and active learning compared to state-of-the-art methods. A third contribution of the thesis concerns the development of practical deep- learning solutions in the challenging domain of Transmission Electron Microscopy (TEM) for material design. In the context of orientation microscopy, ML-based ap- proaches still need to catch up to traditional techniques, such as template matching or the Kikuchi technique, when it comes to generalization performance over unseen orientations and phases during training. This is due mainly to the limited experi- mental data about the studied phenomena for training the models. Nevertheless, it is a realistic and practical constraint, especially for narrow-domain applications where actual data are not widely available. Some successful attempts have been made to use unsupervised learning techniques to gain more insight into the data, but cluster- ing information does not solve the orientation microscopy problem. To this end, we propose a multi-task learning framework based on neural architecture search for fast automation of phase and orientation determination in TEM images.},
  langid = {english},
  school = {Universit\'e Grenoble Alpes},
  keywords = {thesis},
  file = {C:\Users\E097600\Zotero\storage\ESU2E2UG\Lies - 2023 - Semi Supervised Active Learning with Explicit Mislabel Modeling An Application to Material Design.pdf}
}

@misc{liExploringPlainVision2022,
  title = {Exploring {{Plain Vision Transformer Backbones}} for {{Object Detection}}},
  author = {Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},
  year = 2022,
  month = jun,
  number = {arXiv:2203.16527},
  eprint = {2203.16527},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.16527},
  urldate = {2025-07-18},
  abstract = {We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP\_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Object detection,Transformer},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\SAE5R7WY\\Li et al. - 2022 - Exploring Plain Vision Transformer Backbones for Object Detection.pdf;C\:\\Users\\E097600\\Zotero\\storage\\QAL7GR7N\\2203.html}
}

@misc{lindholmAggregationStrategiesEfficient2025,
  title = {Aggregation {{Strategies}} for {{Efficient Annotation}} of {{Bioacoustic Sound Events Using Active Learning}}},
  author = {Lindholm, Richard and Marklund, Oscar and Mogren, Olof and Martinsson, John},
  year = 2025,
  month = mar,
  number = {arXiv:2503.02422},
  eprint = {2503.02422},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.02422},
  urldate = {2025-03-06},
  abstract = {The vast amounts of audio data collected in Sound Event Detection (SED) applications require efficient annotation strategies to enable supervised learning. Manual labeling is expensive and time-consuming, making Active Learning (AL) a promising approach for reducing annotation effort. We introduce Top K Entropy, a novel uncertainty aggregation strategy for AL that prioritizes the most uncertain segments within an audio recording, instead of averaging uncertainty across all segments. This approach enables the selection of entire recordings for annotation, improving efficiency in sparse data scenarios. We compare Top K Entropy to random sampling and Mean Entropy, and show that fewer labels can lead to the same model performance, particularly in datasets with sparse sound events. Evaluations are conducted on audio mixtures of sound recordings from parks with meerkat, dog, and baby crying sound events, representing real-world bioacoustic monitoring scenarios. Using Top K Entropy for active learning, we can achieve comparable performance to training on the fully labeled dataset with only 8\% of the labels. Top K Entropy outperforms Mean Entropy, suggesting that it is best to let the most uncertain segments represent the uncertainty of an audio file. The findings highlight the potential of AL for scalable annotation in audio and time-series applications, including bioacoustics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\5PT52QSR\\Lindholm et al. - 2025 - Aggregation Strategies for Efficient Annotation of Bioacoustic Sound Events Using Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ICM8WZQZ\\2503.html}
}

@misc{linFeaturePyramidNetworks2017,
  title = {Feature {{Pyramid Networks}} for {{Object Detection}}},
  author = {Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  year = 2017,
  month = apr,
  number = {arXiv:1612.03144},
  eprint = {1612.03144},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1612.03144},
  urldate = {2024-12-17},
  abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\E097600\Zotero\storage\BTLVHI2B\Lin et al. - 2017 - Feature Pyramid Networks for Object Detection.pdf}
}

@misc{linFocalLossDense2018,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  year = 2018,
  month = feb,
  number = {arXiv:1708.02002},
  eprint = {1708.02002},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.02002},
  urldate = {2025-01-22},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\FMCM4WWS\\Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf;C\:\\Users\\E097600\\Zotero\\storage\\62ZSGZ36\\1708.html}
}

@book{liquetMathematicalEngineeringDeep2024,
  title = {Mathematical {{Engineering}} of {{Deep Learning}}},
  author = {Liquet, Benoit and Moka, Sarat and Nazarathy, Yoni},
  year = 2024,
  month = oct,
  edition = {1st edition},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {Mathematical Engineering of Deep Learning provides a complete and concise overview of deep learning using the language of mathematics. The book provides a self-contained background on machine learning and optimization algorithms and progresses through the key ideas of deep learning. These ideas and architectures include deep neural networks, convolutional models, recurrent models, long/short-term memory, the attention mechanism, transformers, variational auto-encoders, diffusion models, generative adversarial networks, reinforcement learning, and graph neural networks. Concepts are presented using simple mathematical equations together with a concise description of relevant tricks of the trade. The content is the foundation for state-of-the-art artificial intelligence applications, involving images, sound, large language models, and other domains. The focus is on the basic mathematical description of algorithms and methods and does not require computer programming. The presentation is also agnostic to neuroscientific relationships, historical perspectives, and theoretical research. The benefit of such a concise approach is that a mathematically equipped reader can quickly grasp the essence of deep learning.Key Features:A perfect summary of deep learning not tied to any computer language, or computational framework.An ideal handbook of deep learning for readers that feel comfortable with mathematical notation.An up-to-date description of the most influential deep learning ideas that have made an impact on vision, sound, natural language understanding, and scientific domains.The exposition is not tied to the historical development of the field or to neuroscience, allowing the reader to quickly grasp the essentials.Deep learning is easily described through the language of mathematics at a level accessible to many professionals. Readers from fields such as engineering, statistics, physics, pure mathematics, econometrics, operations research, quantitative management, quantitative biology, applied machine learning, or applied deep learning will quickly gain insights into the key mathematical engineering components of the field.},
  langid = {english},
  keywords = {book},
  file = {C:\Users\E097600\Zotero\storage\PMN2APVW\Liquet et al. - 2024 - Mathematical Engineering of Deep Learning.pdf}
}

@misc{liReviewDeepLearningbased2024,
  title = {A Review of Deep Learning-Based Information Fusion Techniques for Multimodal Medical Image Classification},
  author = {Li, Yihao and Daho, Mostafa El Habib and Conze, Pierre-Henri and Zeghlache, Rachid and Boit{\'e}, Hugo Le and Tadayoni, Ramin and Cochener, B{\'e}atrice and Lamard, Mathieu and Quellec, Gwenol{\'e}},
  year = 2024,
  month = apr,
  number = {arXiv:2404.15022},
  eprint = {2404.15022},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.15022},
  urldate = {2025-11-18},
  abstract = {Multimodal medical imaging plays a pivotal role in clinical diagnosis and research, as it combines information from various imaging modalities to provide a more comprehensive understanding of the underlying pathology. Recently, deep learning-based multimodal fusion techniques have emerged as powerful tools for improving medical image classification. This review offers a thorough analysis of the developments in deep learning-based multimodal fusion for medical classification tasks. We explore the complementary relationships among prevalent clinical modalities and outline three main fusion schemes for multimodal classification networks: input fusion, intermediate fusion (encompassing single-level fusion, hierarchical fusion, and attention-based fusion), and output fusion. By evaluating the performance of these fusion techniques, we provide insight into the suitability of different network architectures for various multimodal fusion scenarios and application domains. Furthermore, we delve into challenges related to network architecture selection, handling incomplete multimodal data management, and the potential limitations of multimodal fusion. Finally, we spotlight the promising future of Transformer-based multimodal fusion techniques and give recommendations for future research in this rapidly evolving field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,fusion model,Image classification,Multimodal,review},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\RRXZ29DX\\Li et al. - 2024 - A review of deep learning-based information fusion techniques for multimodal medical image classific.pdf;C\:\\Users\\E097600\\Zotero\\storage\\XWFMLBYS\\2404.html}
}

@misc{liSurveyDeepActive2024,
  title = {A {{Survey}} on {{Deep Active Learning}}: {{Recent Advances}} and {{New Frontiers}}},
  shorttitle = {A {{Survey}} on {{Deep Active Learning}}},
  author = {Li, Dongyuan and Wang, Zhen and Chen, Yankai and Jiang, Renhe and Ding, Weiping and Okumura, Manabu},
  year = 2024,
  month = jul,
  number = {arXiv:2405.00334},
  eprint = {2405.00334},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.00334},
  urldate = {2025-05-27},
  abstract = {Active learning seeks to achieve strong performance with fewer training samples. It does this by iteratively asking an oracle to label new selected samples in a human-in-the-loop manner. This technique has gained increasing popularity due to its broad applicability, yet its survey papers, especially for deep learning-based active learning (DAL), remain scarce. Therefore, we conduct an advanced and comprehensive survey on DAL. We first introduce reviewed paper collection and filtering. Second, we formally define the DAL task and summarize the most influential baselines and widely used datasets. Third, we systematically provide a taxonomy of DAL methods from five perspectives, including annotation types, query strategies, deep model architectures, learning paradigms, and training processes, and objectively analyze their strengths and weaknesses. Then, we comprehensively summarize main applications of DAL in Natural Language Processing (NLP), Computer Vision (CV), and Data Mining (DM), etc. Finally, we discuss challenges and perspectives after a detailed analysis of current studies. This work aims to serve as a useful and quick guide for researchers in overcoming difficulties in DAL. We hope that this survey will spur further progress in this burgeoning field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\DRX5Y27W\\Li et al. - 2024 - A Survey on Deep Active Learning Recent Advances and New Frontiers.pdf;C\:\\Users\\E097600\\Zotero\\storage\\EQZ4PRBR\\2405.html}
}

@article{liTransformerBasedVisualSegmentation2024,
  title = {Transformer-{{Based Visual Segmentation}}: {{A Survey}}},
  shorttitle = {Transformer-{{Based Visual Segmentation}}},
  author = {Li, Xiangtai and Ding, Henghui and Yuan, Haobo and Zhang, Wenwei and Pang, Jiangmiao and Cheng, Guangliang and Chen, Kai and Liu, Ziwei and Loy, Chen Change},
  year = 2024,
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {46},
  number = {12},
  pages = {10138--10163},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2024.3434373},
  urldate = {2025-07-23},
  abstract = {Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several specific subfields, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research.},
  keywords = {dense prediction,image segmentation,Image segmentation,Measurement,Object detection,review,scene understanding,Surveys,Task analysis,Transformers,video segmentation,Vision transformer review,Visualization},
  file = {C:\Users\E097600\Zotero\storage\TBFMXVXV\Li et al. - 2024 - Transformer-Based Visual Segmentation A Survey.pdf}
}

@inproceedings{liuActiveLearningHuman2017,
  title = {Active {{Learning}} for {{Human Pose Estimation}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Liu, Buyu and Ferrari, Vittorio},
  year = 2017,
  month = oct,
  pages = {4373--4382},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.468},
  urldate = {2024-03-06},
  abstract = {Annotating human poses in realistic scenes is very time consuming, yet necessary for training human pose estimators. We propose to address this problem in an active learning framework, which alternates between requesting the most useful annotations among a large set of unlabelled images, and re-training the pose estimator. To this end, (1) we propose an uncertainty estimator specific for body joint predictions, which takes into account the spatial distribution of the responses of the current pose estimator on the unlabelled images; (2) we propose a dynamic combination of influence and uncertainty cues, where their weights vary during the active learning process according to the reliability of the current pose estimator; (3) we introduce a computer assisted annotation interface, which reduces the time necessary for a human annotator to click on a joint by discretizing the image into regions generated by the current pose estimator. Experiments using the MPII and LSP datasets with both simulated and real annotators show that (1) the proposed active selection scheme outperforms several baselines; (2) our computer-assisted interface can further reduce annotation effort; and (3) our technique can further improve the performance of a pose estimator even when starting from an already strong one.},
  keywords = {Entropy,Measurement uncertainty,Pose estimation,Space heating,Training,Uncertainty},
  file = {C:\Users\E097600\Zotero\storage\6D4K35L2\8237730.html}
}

@misc{liuLSCALELatentSpace2022,
  title = {{{LSCALE}}: {{Latent Space Clustering-Based Active Learning}} for {{Node Classification}}},
  shorttitle = {{{LSCALE}}},
  author = {Liu, Juncheng and Wang, Yiwei and Hooi, Bryan and Yang, Renchi and Xiao, Xiaokui},
  year = 2022,
  month = jul,
  number = {arXiv:2012.07065},
  eprint = {2012.07065},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.07065},
  urldate = {2024-03-07},
  abstract = {Node classification on graphs is an important task in many practical domains. It usually requires labels for training, which can be difficult or expensive to obtain in practice. Given a budget for labelling, active learning aims to improve performance by carefully choosing which nodes to label. Previous graph active learning methods learn representations using labelled nodes and select some unlabelled nodes for label acquisition. However, they do not fully utilize the representation power present in unlabelled nodes. We argue that the representation power in unlabelled nodes can be useful for active learning and for further improving performance of active learning for node classification. In this paper, we propose a latent space clustering-based active learning framework for node classification (LSCALE), where we fully utilize the representation power in both labelled and unlabelled nodes. Specifically, to select nodes for labelling, our framework uses the K-Medoids clustering algorithm on a latent space based on a dynamic combination of both unsupervised features and supervised features. In addition, we design an incremental clustering module to avoid redundancy between nodes selected at different steps. Extensive experiments on five datasets show that our proposed framework LSCALE consistently and significantly outperforms the stateof-the-art approaches by a large margin.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\FL2BJIWN\\Liu et al. - 2022 - LSCALE Latent Space Clustering-Based Active Learn.pdf;C\:\\Users\\E097600\\Zotero\\storage\\FAXWR2MG\\2012.html}
}

@misc{liuSemiSupervisedConfidenceLevelbasedContrastive2022,
  title = {Semi-{{Supervised Confidence-Level-based Contrastive Discrimination}} for {{Class-Imbalanced Semantic Segmentation}}},
  author = {Liu, Kangcheng},
  year = 2022,
  month = nov,
  number = {arXiv:2211.15066},
  eprint = {2211.15066},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.15066},
  urldate = {2025-09-19},
  abstract = {To overcome the data-hungry challenge, we have proposed a semi-supervised contrastive learning framework for the task of class-imbalanced semantic segmentation. First and foremost, to make the model operate in a semi-supervised manner, we proposed the confidence-level-based contrastive learning to achieve instance discrimination in an explicit manner, and make the low-confidence low-quality features align with the high-confidence counterparts. Moreover, to tackle the problem of class imbalance in crack segmentation and road components extraction, we proposed the data imbalance loss to replace the traditional cross entropy loss in pixel-level semantic segmentation. Finally, we have also proposed an effective multi-stage fusion network architecture to improve semantic segmentation performance. Extensive experiments on the real industrial crack segmentation and the road segmentation demonstrate the superior effectiveness of the proposed framework. Our proposed method can provide satisfactory segmentation results with even merely 3.5\% labeled data.},
  archiveprefix = {arXiv},
  keywords = {Class imbalance,Computer Science - Computer Vision and Pattern Recognition,semantic segmentation,Semi-supervised},
  file = {C:\Users\E097600\Zotero\storage\EJ5FTZDC\Liu - 2022 - Semi-Supervised Confidence-Level-based Contrastive Discrimination for Class-Imbalanced Semantic Segm.pdf}
}

@misc{liuSystematicReviewAvailable2024,
  title = {A {{Systematic Review}} of {{Available Datasets}} in {{Additive Manufacturing}}},
  author = {Liu, Xiao and Mileo, Alessandra and Smeaton, Alan F.},
  year = 2024,
  month = jan,
  number = {arXiv:2401.15448},
  eprint = {2401.15448},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.15448},
  urldate = {2024-03-04},
  abstract = {In-situ monitoring incorporating data from visual and other sensor technologies, allows the collection of extensive datasets during the Additive Manufacturing (AM) process. These datasets have potential for determining the quality of the manufactured output and the detection of defects through the use of Machine Learning during the manufacturing process. Open and annotated datasets derived from AM processes are necessary for the machine learning community to address this opportunity, which creates difficulties in the application of computer vision-related machine learning in AM. This systematic review investigates the availability of open image-based datasets originating from AM processes that align with a number of pre-defined selection criteria. The review identifies existing gaps among the current image-based datasets in the domain of AM, and points to the need for greater availability of open datasets in order to allow quality assessment and defect detection during additive manufacturing, to develop.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\9795EVEV\\Liu et al. - 2024 - A Systematic Review of Available Datasets in Addit.pdf;C\:\\Users\\E097600\\Zotero\\storage\\APXTZ4WQ\\2401.html}
}

@misc{liYourDiffusionModel2023,
  title = {Your {{Diffusion Model}} Is {{Secretly}} a {{Zero-Shot Classifier}}},
  author = {Li, Alexander C. and Prabhudesai, Mihir and Duggal, Shivam and Brown, Ellis and Pathak, Deepak},
  year = 2023,
  month = sep,
  number = {arXiv:2303.16203},
  eprint = {2303.16203},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.16203},
  urldate = {2025-05-06},
  abstract = {The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has significantly stronger multimodal compositional reasoning ability than competing discriminative approaches. Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. Our models achieve strong classification performance using only weak augmentations and exhibit qualitatively better "effective robustness" to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks. Results and visualizations at https://diffusion-classifier.github.io/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\PJMH89XD\\Li et al. - 2023 - Your Diffusion Model is Secretly a Zero-Shot Classifier.pdf;C\:\\Users\\E097600\\Zotero\\storage\\DQFU23NG\\2303.html}
}

@misc{longFullyConvolutionalNetworks2015,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = 2015,
  month = mar,
  number = {arXiv:1411.4038},
  eprint = {1411.4038},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1411.4038},
  urldate = {2024-12-31},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\NA7YPF7F\\Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\RM6P4SM8\\1411.html}
}

@misc{lowellPracticalObstaclesDeploying2019,
  title = {Practical {{Obstacles}} to {{Deploying Active Learning}}},
  author = {Lowell, David and Lipton, Zachary C. and Wallace, Byron C.},
  year = 2019,
  month = nov,
  number = {arXiv:1807.04801},
  eprint = {1807.04801},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.04801},
  urldate = {2025-10-09},
  abstract = {Active learning (AL) is a widely-used training strategy for maximizing predictive performance subject to a fixed annotation budget. In AL one iteratively selects training examples for annotation, often those for which the current model is most uncertain (by some measure). The hope is that active sampling leads to better performance than would be achieved under independent and identically distributed (i.i.d.) random samples. While AL has shown promise in retrospective evaluations, these studies often ignore practical obstacles to its use. In this paper we show that while AL may provide benefits when used with specific models and for particular domains, the benefits of current approaches do not generalize reliably across models and tasks. This is problematic because in practice one does not have the opportunity to explore and compare alternative AL strategies. Moreover, AL couples the training dataset with the model used to guide its acquisition. We find that subsequently training a successor model with an actively-acquired dataset does not consistently outperform training on i.i.d. sampled data. Our findings raise the question of whether the downsides inherent to AL are worth the modest and inconsistent performance gains it tends to afford.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\JABFDCT7\\Lowell et al. - 2019 - Practical Obstacles to Deploying Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\Y68MAW6X\\1807.html}
}

@article{lughoferOnlineActiveLearning2017,
  title = {On-Line Active Learning: {{A}} New Paradigm to Improve Practical Useability of Data Stream Modeling Methods},
  shorttitle = {On-Line Active Learning},
  author = {Lughofer, Edwin},
  year = 2017,
  month = nov,
  journal = {Information Sciences},
  volume = {415--416},
  pages = {356--376},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2017.06.038},
  urldate = {2024-12-23},
  abstract = {The central purpose of this survey is to provide readers an insight into the recent advances and challenges in on-line active learning. Active learning has attracted the data mining and machine learning community since around 20 years. This is because it served for important purposes to increase practical applicability of machine learning techniques, such as (i) to reduce annotation and measurement costs for operators and measurement equipments, (ii) to reduce manual labeling effort for experts and (iii) to reduce computation time for model training. Almost all of the current techniques focus on the classical pool-based approach, which is off-line by nature as iterating over a pool of (unlabeled) reference samples a multiple times to choose the most promising ones for improving the performance of the classifiers. This is achieved by (time-intensive) re-training cycles on all labeled samples available so far. For the on-line, stream mining case, the challenge is that the sample selection strategy has to operate in a fast, ideally single-pass manner. Some first approaches have been proposed during the last decade (starting from around 2005) with the usage of machine learning (ML) oriented incremental classifiers, which are able to update their parameters based on selected samples, but not their structures. Since 2012, on-line active learning concepts have been proposed in connection with the paradigm of evolving models, which are able to expand their knowledge into feature space regions so far unexplored. This opened the possibility to address a particular type of uncertainty, namely that one which stems from a significant novelty content in streams, as, e.g., caused by drifts, new operation modes, changing system behaviors or non-stationary environments. We will provide an overview about the concepts and techniques for sample selection and active learning within these two principal major research lines (incremental ML models versus evolving systems), a comparison of their essential characteristics and properties (raising some advantages and disadvantages), and a study on possible evaluation techniques for them. We conclude with an overview of real-world application examples where various on-line AL approaches have been already successfully applied in order to significantly reduce user's interaction efforts and costs for model updates.},
  keywords = {Data stream mining,Evolving models,Incremental ML and DM methods,Interaction effort and cost reduction,On-line active learning,Single-pass sample selection,stream based AL,Uncertainty and novelty in streams},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\JQRI8HGX\\Lughofer - 2017 - On-line active learning A new paradigm to improve practical useability of data stream modeling meth.pdf;C\:\\Users\\E097600\\Zotero\\storage\\KCCRI6TF\\S0020025517308083.html}
}

@misc{maBreakingBarrierSelective2024,
  title = {Breaking the {{Barrier}}: {{Selective Uncertainty-based Active Learning}} for {{Medical Image Segmentation}}},
  shorttitle = {Breaking the {{Barrier}}},
  author = {Ma, Siteng and Wu, Haochang and Lawlor, Aonghus and Dong, Ruihai},
  year = 2024,
  month = jan,
  number = {arXiv:2401.16298},
  eprint = {2401.16298},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.16298},
  urldate = {2024-08-23},
  abstract = {Active learning (AL) has found wide applications in medical image segmentation, aiming to alleviate the annotation workload and enhance performance. Conventional uncertainty-based AL methods, such as entropy and Bayesian, often rely on an aggregate of all pixel-level metrics. However, in imbalanced settings, these methods tend to neglect the significance of target regions, eg., lesions, and tumors. Moreover, uncertainty-based selection introduces redundancy. These factors lead to unsatisfactory performance, and in many cases, even underperform random sampling. To solve this problem, we introduce a novel approach called the Selective Uncertainty-based AL, avoiding the conventional practice of summing up the metrics of all pixels. Through a filtering process, our strategy prioritizes pixels within target areas and those near decision boundaries. This resolves the aforementioned disregard for target areas and redundancy. Our method showed substantial improvements across five different uncertainty-based methods and two distinct datasets, utilizing fewer labeled data to reach the supervised baseline and consistently achieving the highest overall performance. Our code is available at https://github.com/HelenMa9998/Selective\textbackslash\_Uncertainty\textbackslash\_AL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\PPG9M2ST\\Ma et al. - 2024 - Breaking the Barrier Selective Uncertainty-based .pdf;C\:\\Users\\E097600\\Zotero\\storage\\HANRRYAE\\2401.html}
}

@misc{maggioActiveSamplingData2023,
  title = {Active {{Sampling}}: {{Data Selection}} for {{Efficient Model Training}}},
  shorttitle = {Active {{Sampling}}},
  author = {Maggio, Simona},
  year = 2023,
  month = mar,
  urldate = {2024-03-05},
  abstract = {With this technical article, master the art of active sampling and selecting the right data for ML tasks.},
  howpublished = {https://blog.dataiku.com/active-sampling-data-selection-for-efficient-model-training},
  langid = {american},
  file = {C:\Users\E097600\Zotero\storage\8ZV5QYFF\active-sampling-data-selection-for-efficient-model-training.html}
}

@misc{maierTakeGoodhartSeriously2025,
  title = {Take {{Goodhart Seriously}}: {{Principled Limit}} on {{General-Purpose AI Optimization}}},
  shorttitle = {Take {{Goodhart Seriously}}},
  author = {Maier, Antoine and Maier, Aude and David, Tom},
  year = 2025,
  month = oct,
  number = {arXiv:2510.02840},
  eprint = {2510.02840},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.02840},
  urldate = {2025-10-22},
  abstract = {A common but rarely examined assumption in machine learning is that training yields models that actually satisfy their specified objective function. We call this the Objective Satisfaction Assumption (OSA). Although deviations from OSA are acknowledged, their implications are overlooked. We argue, in a learning-paradigm-agnostic framework, that OSA fails in realistic conditions: approximation, estimation, and optimization errors guarantee systematic deviations from the intended objective, regardless of the quality of its specification. Beyond these technical limitations, perfectly capturing and translating the developer's intent, such as alignment with human preferences, into a formal objective is practically impossible, making misspecification inevitable. Building on recent mathematical results, absent a mathematical characterization of these gaps, they are indistinguishable from those that collapse into Goodhart's law failure modes under strong optimization pressure. Because the Goodhart breaking point cannot be located ex ante, a principled limit on the optimization of General-Purpose AI systems is necessary. Absent such a limit, continued optimization is liable to push systems into predictable and irreversible loss of control.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\7QXNATCI\\Maier et al. - 2025 - Take Goodhart Seriously Principled Limit on General-Purpose AI Optimization.pdf;C\:\\Users\\E097600\\Zotero\\storage\\IUJ2B6VV\\2510.html}
}

@article{malialisSiameseDuoActiveLearning2025,
  title = {{{SiameseDuo}}++: {{Active Learning}} from {{Data Streams}} with {{Dual Augmented Siamese Networks}}},
  shorttitle = {{{SiameseDuo}}++},
  author = {Malialis, Kleanthis and Filippou, Stylianos and Panayiotou, Christos G. and Polycarpou, Marios M.},
  year = 2025,
  month = jul,
  journal = {Neurocomputing},
  volume = {637},
  eprint = {2504.04613},
  primaryclass = {cs},
  pages = {130083},
  issn = {09252312},
  doi = {10.1016/j.neucom.2025.130083},
  urldate = {2025-04-28},
  abstract = {Data stream mining, also known as stream learning, is a growing area which deals with learning from high-speed arriving data. Its relevance has surged recently due to its wide range of applicability, such as, critical infrastructure monitoring, social media analysis, and recommender systems. The design of stream learning methods faces significant research challenges; from the nonstationary nature of the data (referred to as concept drift) and the fact that data streams are typically not annotated with the ground truth, to the requirement that such methods should process large amounts of data in real-time with limited memory. This work proposes the SiameseDuo++ method, which uses active learning to automatically select instances for a human expert to label according to a budget. Specifically, it incrementally trains two siamese neural networks which operate in synergy, augmented by generated examples. Both the proposed active learning strategy and augmentation operate in the latent space. SiameseDuo++ addresses the aforementioned challenges by operating with limited memory and limited labelling budget. Simulation experiments show that the proposed method outperforms strong baselines and state-of-the-art methods in terms of learning speed and/or performance. To promote open science we publicly release our code and datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\Q7SVABCB\\Malialis et al. - 2025 - SiameseDuo++ Active Learning from Data Streams with Dual Augmented Siamese Networks.pdf;C\:\\Users\\E097600\\Zotero\\storage\\TZVIFMQY\\2504.html}
}

@book{manokhinMasteringModernTime,
  title = {Mastering {{Modern Time Series Forecasting}} : {{The Complete Guide}} to {{Statistical}}, {{Machine Learning}} \& {{Deep Learning Models}} in {{Python}}},
  shorttitle = {Mastering {{Modern Time Series Forecasting}}},
  author = {Manokhin, Valery},
  urldate = {2025-11-26},
  abstract = {📘 Mastering Modern Time Series Forecasting (early access)The book trusted by data science leaders in 100+ countries. Unlock the toolkit behind today's most powerful forecasting systems. 💸 Pricing Price increase to \$80+ as content grows. A tremendous amount of work and expertise has gone into this book, which is designed to deliver exponential improvement to your forecasting skills, your company's bottom line and ROI, and your career. Forecasting is one of the most in-demand skills across nearly every industry today. As the content continues to grow, if you find value in it---or simply want to support the project---you're welcome to contribute whatever it's worth to you ❤️. 🧠 Why This Book Stands Out🔑 Forecasting models are only 5\% of the equation.The other 95\%? It's the hard-earned knowledge of metrics, validation, deployment, failure modes, and real-world constraints --- insights that are often missing or buried in internet noise and social media fluff.🔍 It starts with what actually matters: solid foundations.Learn how to properly evaluate forecasts, recognize when they're failing, and build with confidence --- not on shaky assumptions, but on methods that stand up to real-world pressure.💎 You'll also learn how to assess the forecastability of a time series --- a critical step for managing your time, setting stakeholder expectations, and realistically estimating how far forecasting accuracy can be pushed before diminishing returns kick in.🧠 Built for understanding --- not just coding.Go beyond black-box code. Grasp model mechanics and decision-making logic to truly understand how and why things work.💻 Clear, transparent production grade code.No obfuscation, no throwaway scripts. Every example is fully documented, reusable, and ready for real-world use.🔄 Continuously improved through real feedback.This is a living resource shaped by an active community of readers. Many improvements and additions come directly from their thoughtful feedback --- and all readers get lifetime updates, including new chapters and bonus tools. Thank you to all contributors --- your insights are recognized and appreciated in the book.📚 Comprehensive, real-world coverage.From classical time series models to deep learning, transformers and foundational models, the book covers a wide range --- but always with a practical lens. Every method has been tested in production or validated against strong academic benchmarks. No fluff, just tools that work.📈 Real ROI --- for your company and your career.Readers often see immediate improvements in model accuracy, interpretability, and stakeholder trust. No more silent failures or fragile production systems. This book helps you build forecasting solutions that earn trust, drive business results, and accelerate your career.✍️ About the AuthorWritten by Valeriy Manokhin, PhD, MBA, CQF --- a seasoned forecasting expert, experienced data scientist, and machine learning expert and researcher with publications in top peer reviewed machine learning journals.Valeriy has advised both startups and large enterprises, helping them build and rebuild forecasting systems at scale. He has led successful forecasting initiatives for global organizations --- including winning competitive tenders from multinational companies, outperforming major consulting firms like BCG and specialized AI startups focused on forecasting. He has delivered production-grade solutions for industry leaders such as Stanley Black \&amp; Decker and GfK.His methods have driven multimillion-dollar business impact, and his training programs have reached professionals in over 40 countries. This book is now used in more than 100+ countries and has become a \#1-ranked title in Machine Learning, Forecasting, and Time Series across major publishing platforms.🌍 Trusted By and Taught ToValeriy's expertise is trusted by leaders at:Amazon, Apple, Google, Meta, Nike, BlackRock, Morgan Stanley, Target, NTT Data, Mars Inc., Lidl, Publicis Sapient, and more.His frameworks are followed by academics and researchers from:University of Chicago, KTH (Sweden), UBC (Canada), DTU (Denmark), and other world-class institutions.👤 Students include:VPs of Engineering, AI Leads, Principal \&amp; Lead Data Scientists, ML Engineers, Consultants, Professors, Founders, Researchers, and PhD students.🎓 Want a Live, Interactive Learning Experience?Pair this book with the Modern Forecasting Mastery course on Maven.Join live cohort sessions with Valeriy, get direct feedback, and build models with peers.Next cohort {$\rightarrow$} maven.com/valeriy-manokhin/modern-forecasting-mastery📦 What You Get📥 Instant access to the book --- start reading immediately.🔄 Free updates --- including new chapters, bug fixes, and bonus content.💬 Exclusive access to the private Discord community --- connect with fellow readers, get additional materials, early bonuses, special discounts, and join live events with the author.🔓 Pro Edition Bonus Pack (Early Access) 🔥🔥🔥 Includes everything above, plus:✅ Premium Forecasting Templates --- plug-and-play workflows✅ Extended Case Studies --- deep analyses across major industries✅ Behind-the-Scenes Notebooks --- annotated walkthroughs and exploratory pipelines✅ Forecast Model Selection Toolkit --- Python notebooks to benchmark, optimize, and compare📈 Ideal for professionals and teams who want to build and deploy faster---and sidestep the guesswork.https://valeman.gumroad.com/l/MasteringModernTimeSeriesForecastingProReady to take your forecasting skills from stats to neural nets, and from theory to real-world deployment?👉 Hit ``Buy Now'' and start mastering forecasting like never before.},
  langid = {english},
  keywords = {book,Conformal prediction,time series},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\25MHFZ8B\\Manokhin - Mastering Modern Time Series Forecasting  The Complete Guide to Statistical, Machine Learning & Dee.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ICFETMJF\\MasteringModernTimeSeriesForecasting.html}
}

@book{manokhinPracticalGuideApplied2023,
  title = {Practical {{Guide}} to {{Applied Conformal Prediction}} in {{Python}}: {{Learn}} and Apply the Best Uncertainty Frameworks to Your Industry Applications},
  shorttitle = {Practical {{Guide}} to {{Applied Conformal Prediction}} in {{Python}}},
  author = {Manokhin, Valery},
  year = 2023,
  month = dec,
  publisher = {Packt Publishing},
  address = {Place of publication not identified},
  abstract = {Elevate your machine learning skills using the Conformal Prediction framework for uncertainty quantification. Dive into unique strategies, overcome real-world challenges, and become confident and precise with forecasting.Key Features: Master Conformal Prediction, a fast-growing ML framework, with Python applicationsExplore cutting-edge methods to measure and manage uncertainty in industry applicationsUnderstand how Conformal Prediction differs from traditional machine learningBook Description: In the rapidly evolving landscape of machine learning, the ability to accurately quantify uncertainty is pivotal. The book addresses this need by offering an in-depth exploration of Conformal Prediction, a cutting-edge framework to manage uncertainty in various ML applications.Learn how Conformal Prediction excels in calibrating classification models, produces well-calibrated prediction intervals for regression, and resolves challenges in time series forecasting and imbalanced data. Discover specialised applications of conformal prediction in cutting-edge domains like computer vision and NLP. Each chapter delves into specific aspects, offering hands-on insights and best practices for enhancing prediction reliability. The book concludes with a focus on multi-class classification nuances, providing expert-level proficiency to seamlessly integrate Conformal Prediction into diverse industries. With practical examples in Python using real-world datasets, expert insights, and open-source library applications, you will gain a solid understanding of this modern framework for uncertainty quantification.By the end of this book, you will be able to master Conformal Prediction in Python with a blend of theory and practical application, enabling you to confidently apply this powerful framework to quantify uncertainty in diverse fields.What You Will Learn: The fundamental concepts and principles of conformal predictionLearn how conformal prediction differs from traditional ML methodsApply real-world examples to your own industry applicationsExplore advanced topics - imbalanced data and multi-class CPDive into the details of the conformal prediction frameworkBoost your career as a data scientist, ML engineer, or researcherLearn to apply conformal prediction to forecasting and NLPWho this book is for: Ideal for readers with a basic understanding of machine learning concepts and Python programming, this book caters to data scientists, ML engineers, academics, and anyone keen on advancing their skills in uncertainty quantification in ML.},
  isbn = {978-1-80512-276-0},
  langid = {english},
  keywords = {book},
  file = {C:\Users\E097600\Zotero\storage\GR64TXR6\Manokhin - 2023 - Practical Guide to Applied Conformal Prediction in Python Learn and apply the best uncertainty fram.pdf}
}

@misc{maoCrossEntropyLossFunctions2023,
  title = {Cross-{{Entropy Loss Functions}}: {{Theoretical Analysis}} and {{Applications}}},
  shorttitle = {Cross-{{Entropy Loss Functions}}},
  author = {Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  year = 2023,
  month = jun,
  number = {arXiv:2304.07288},
  eprint = {2304.07288},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.07288},
  urldate = {2025-02-07},
  abstract = {Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of loss functions, comp-sum losses, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other cross-entropy-like loss functions. We give the first \$H\$-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set \$H\$ used. We further show that our bounds are tight. These bounds depend on quantities called minimizability gaps. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses. We also introduce a new family of loss functions, smooth adversarial comp-sum losses, that are derived from their comp-sum counterparts by adding in a related smooth term. We show that these loss functions are beneficial in the adversarial setting by proving that they admit \$H\$-consistency bounds. This leads to new adversarial robustness algorithms that consist of minimizing a regularized smooth adversarial comp-sum loss. While our main purpose is a theoretical analysis, we also present an extensive empirical analysis comparing comp-sum losses. We further report the results of a series of experiments demonstrating that our adversarial robustness algorithms outperform the current state-of-the-art, while also achieving a superior non-adversarial accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\HABZMJC9\\Mao et al. - 2023 - Cross-Entropy Loss Functions Theoretical Analysis and Applications.pdf;C\:\\Users\\E097600\\Zotero\\storage\\3LL9FY4U\\2304.html}
}

@book{MarkovRandomFields2011,
  title = {Markov {{Random Fields}} for {{Vision}} and {{Image Processing}}},
  year = 2011,
  month = jul,
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/8579.001.0001},
  urldate = {2025-10-01},
  abstract = {State-of-the-art research on MRFs, successful MRF applications, and advanced topics for future study.This volume demonstrates the power of the Markov rando},
  isbn = {978-0-262-29835-3},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\R5V7TSQE\Markov-Random-Fields-for-Vision-and-Image.html}
}

@misc{marnetUncertaintyDrivenActive2024,
  title = {Uncertainty {{Driven Active Learning}} for {{Image Segmentation}} in {{Underwater Inspection}}},
  author = {Marnet, Luiza Ribeiro and Brodskiy, Yury and Grasshof, Stella and Wasowski, Andrzej},
  year = 2024,
  month = mar,
  number = {arXiv:2403.14002},
  eprint = {2403.14002},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.14002},
  urldate = {2025-01-16},
  abstract = {Active learning aims to select the minimum amount of data to train a model that performs similarly to a model trained with the entire dataset. We study the potential of active learning for image segmentation in underwater infrastructure inspection tasks, where large amounts of data are typically collected. The pipeline inspection images are usually semantically repetitive but with great variations in quality. We use mutual information as the acquisition function, calculated using Monte Carlo dropout. To assess the effectiveness of the framework, DenseNet and HyperSeg are trained with the CamVid dataset using active learning. In addition, HyperSeg is trained with a pipeline inspection dataset of over 50,000 images. For the pipeline dataset, HyperSeg with active learning achieved 67.5\% meanIoU using 12.5\% of the data, and 61.4\% with the same amount of randomly selected images. This shows that using active learning for segmentation models in underwater inspection tasks can lower the cost significantly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\ZH8N55XY\\Marnet et al. - 2024 - Uncertainty Driven Active Learning for Image Segmentation in Underwater Inspection.pdf;C\:\\Users\\E097600\\Zotero\\storage\\4QVFG2NB\\2403.html}
}

@misc{martin-ramiroBoostingDefectDetection2023,
  title = {Boosting {{Defect Detection}} in {{Manufacturing}} Using {{Tensor Convolutional Neural Networks}}},
  author = {{Martin-Ramiro}, Pablo and {de la Maza}, Unai Sainz and Orus, Roman and Mugel, Samuel},
  year = 2023,
  month = dec,
  number = {arXiv:2401.01373},
  eprint = {2401.01373},
  primaryclass = {quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.01373},
  urldate = {2024-03-04},
  abstract = {Defect detection is one of the most important yet challenging tasks in the quality control stage in the manufacturing sector. In this work, we introduce a Tensor Convolutional Neural Network (T-CNN) and examine its performance on a real defect detection application in one of the components of the ultrasonic sensors produced at Robert Bosch's manufacturing plants. Our quantum-inspired T-CNN operates on a reduced model parameter space to substantially improve the training speed and performance of an equivalent CNN model without sacrificing accuracy. More specifically, we demonstrate how T-CNNs are able to reach the same performance as classical CNNs as measured by quality metrics, with up to fifteen times fewer parameters and 4\% to 19\% faster training times. Our results demonstrate that the T-CNN greatly outperforms the results of traditional human visual inspection, providing value in a current real application in manufacturing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantum Physics},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\V2K6U5BI\\Martin-Ramiro et al. - 2023 - Boosting Defect Detection in Manufacturing using T.pdf;C\:\\Users\\E097600\\Zotero\\storage\\MV4V6WGD\\2401.html}
}

@book{martinCleanCodeHandbook2008,
  title = {Clean {{Code}}: {{A Handbook}} of {{Agile Software Craftsmanship}}},
  shorttitle = {Clean {{Code}}},
  author = {Martin, Robert C.},
  year = 2008,
  month = aug,
  edition = {1st edition},
  publisher = {Pearson},
  address = {Upper Saddle River, NJ Munich},
  abstract = {Even bad code can function. But if code isn't clean, it can bring a development organization to its knees. Every year, countless hours and significant resources are lost because of poorly written code. But it doesn't have to be that way.  Noted software expert Robert C. Martin, presents a revolutionary paradigm with Clean Code: A Handbook of Agile Software Craftsmanship. Martin, who has helped bring agile principles from a practitioner's point of view to tens of thousands of programmers, has teamed up with his colleagues from Object Mentor to distill their best agile practice of cleaning code ``on the fly'' into a book that will instill within you the values of software craftsman, and make you a better programmer\texthorizontalbar but only if you work at it.  What kind of work will you be doing? You'll be reading code\texthorizontalbar lots of code. And you will be challenged to think about what's right about that code, and what's wrong with it. More importantly you will be challenged to reassess your professional values and your commitment to your craft.   Clean Codeis divided into three parts. The first describes the principles, patterns, and practices of writing clean code. The second part consists of several case studies of increasing complexity. Each case study is an exercise in cleaning up code\texthorizontalbar of transforming a code base that has some problems into one that is sound and efficient. The third part is the payoff: a single chapter containing a list of heuristics and ``smells'' gathered while creating the case studies. The result is a knowledge base that describes the way we think when we write, read, and clean code.  Readers will come away from this book understanding  How to tell the difference between good and bad code How to write good code and how to transform bad code into good code How to create good names, good functions, good objects, and good classes How to format code for maximum readability How to implement complete error handling without obscuring code logic How to unit test and practice test-driven development What ``smells'' and heuristics can help you identify bad codeThis book is a must for any developer, software engineer, project manager, team lead, or systems analyst with an interest in producing better code.},
  isbn = {978-0-13-235088-4},
  langid = {english},
  keywords = {book},
  file = {C:\Users\E097600\Zotero\storage\PAVFNMF5\Martin - 2008 - Clean Code A Handbook of Agile Software Craftsmanship.pdf}
}

@article{mayrZeroShotParagraphlevelHandwriting2025,
  title = {Zero-{{Shot Paragraph-level Handwriting Imitation}} with {{Latent Diffusion Models}}},
  author = {Mayr, Martin and Dreier, Marcel and Kordon, Florian and Seuret, Mathias and Z{\"o}llner, Jochen and Wu, Fei and Maier, Andreas and Christlein, Vincent},
  year = 2025,
  month = jul,
  journal = {International Journal of Computer Vision},
  issn = {1573-1405},
  doi = {10.1007/s11263-025-02525-0},
  urldate = {2025-09-22},
  abstract = {The imitation of cursive handwriting is mainly limited to generating handwritten words or lines. Multiple synthetic outputs must be stitched together to create paragraphs or whole pages, whereby consistency and layout information are lost. To close this gap, we propose a method for imitating handwriting at the paragraph level that also works for unseen writing styles. Therefore, we introduce a modified latent diffusion model that enriches the encoder-decoder mechanism with specialized loss functions that explicitly preserve the style and content. We enhance the attention mechanism of the diffusion model with adaptive 2D positional encoding and the conditioning mechanism to work with two modalities simultaneously: a style image and the target text. This significantly improves the realism of the generated handwriting. We set a new benchmark in our comprehensive evaluation, achieving 61~\% mAP and 56~\% top-1 accuracy in style preservation, significantly outperforming the previous best method (37~\% mAP, 30~\% top-1). We are making our code publicly available for reproducibility, supporting research in this area and research into potential countermeasures: https://github.com/M4rt1nM4yr/paragraph\_handwriting\_imitation\_ldm},
  langid = {english},
  keywords = {Computer vision,Document analysis,Handwriting imitation,Image generation,Latent diffusion models},
  file = {C:\Users\E097600\Zotero\storage\ZES9VWA7\Mayr et al. - 2025 - Zero-Shot Paragraph-level Handwriting Imitation with Latent Diffusion Models.pdf}
}

@misc{mendez-ruizSuSanaDistanciaAll2023,
  title = {{{SuSana Distancia}} Is All You Need: {{Enforcing}} Class Separability in Metric Learning via Two Novel Distance-Based Loss Functions for Few-Shot Image Classification},
  shorttitle = {{{SuSana Distancia}} Is All You Need},
  author = {{Mendez-Ruiz}, Mauricio and {Gonzalez-Zapata}, Jorge and {Reyes-Amezcua}, Ivan and {Flores-Araiza}, Daniel and {Lopez-Tiro}, Francisco and {Mendez-Vazquez}, Andres and {Ochoa-Ruiz}, Gilberto},
  year = 2023,
  month = may,
  number = {arXiv:2305.09062},
  eprint = {2305.09062},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.09062},
  urldate = {2024-03-04},
  abstract = {Few-shot learning is a challenging area of research that aims to learn new concepts with only a few labeled samples of data. Recent works based on metric-learning approaches leverage the meta-learning approach, which is encompassed by episodic tasks that make use a support (training) and query set (test) with the objective of learning a similarity comparison metric between those sets. Due to the lack of data, the learning process of the embedding network becomes an important part of the few-shot task. Previous works have addressed this problem using metric learning approaches, but the properties of the underlying latent space and the separability of the difference classes on it was not entirely enforced. In this work, we propose two different loss functions which consider the importance of the embedding vectors by looking at the intra-class and inter-class distance between the few data. The first loss function is the Proto-Triplet Loss, which is based on the original triplet loss with the modifications needed to better work on few-shot scenarios. The second loss function, which we dub ICNN loss is based on an inter and intra class nearest neighbors score, which help us to assess the quality of embeddings obtained from the trained network. Our results, obtained from a extensive experimental setup show a significant improvement in accuracy in the miniImagenNet benchmark compared to other metric-based few-shot learning methods by a margin of 2\%, demonstrating the capability of these loss functions to allow the network to generalize better to previously unseen classes. In our experiments, we demonstrate competitive generalization capabilities to other domains, such as the Caltech CUB, Dogs and Cars datasets compared with the state of the art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\7ZRC8NUP\\Mendez-Ruiz et al. - 2023 - SuSana Distancia is all you need Enforcing class .pdf;C\:\\Users\\E097600\\Zotero\\storage\\N2NTW84F\\2305.html}
}

@misc{meyerBENUsingConfidenceGuided2025,
  title = {{{BEN}}: {{Using Confidence-Guided Matting}} for {{Dichotomous Image Segmentation}}},
  shorttitle = {{{BEN}}},
  author = {Meyer, Maxwell and Spruyt, Jack},
  year = 2025,
  month = jan,
  number = {arXiv:2501.06230},
  eprint = {2501.06230},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.06230},
  urldate = {2025-02-03},
  abstract = {Current approaches to dichotomous image segmentation (DIS) treat image matting and object segmentation as fundamentally different tasks. As improvements in image segmentation become increasingly challenging to achieve, combining image matting and grayscale segmentation techniques offers promising new directions for architectural innovation. Inspired by the possibility of aligning these two model tasks, we propose a new architectural approach for DIS called Confidence-Guided Matting (CGM). We created the first CGM model called Background Erase Network (BEN). BEN is comprised of two components: BEN Base for initial segmentation and BEN Refiner for confidence refinement. Our approach achieves substantial improvements over current state-of-the-art methods on the DIS5K validation dataset, demonstrating that matting-based refinement can significantly enhance segmentation quality. This work opens new possibilities for cross-pollination between matting and segmentation techniques in computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\8J2D6HI3\\Meyer et Spruyt - 2025 - BEN Using Confidence-Guided Matting for Dichotomous Image Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\9VB2YAWJ\\2501.html}
}

@article{mignotAutomaticInspectionSystem2024,
  title = {An Automatic Inspection System for the Detection of Tire Surface Defects and Their Severity Classification through a Two-Stage Multimodal Deep Learning Approach},
  author = {Mignot, Thomas and Ponchon, Fran{\c c}ois and Derville, Alexandre and Duffner, Stefan and Garcia, Christophe},
  year = 2024,
  month = may,
  journal = {Journal of Intelligent Manufacturing},
  issn = {1572-8145},
  doi = {10.1007/s10845-024-02378-3},
  urldate = {2024-05-28},
  abstract = {In the tire manufacturing field, the pursuit of uncompromised product quality stands as a cornerstone. This paper introduces an innovative multimodal approach aimed at automating the tire quality control process through the use of deep learning on data obtained from stereo-photometric cameras meticulously integrated into a purpose-built, sophisticated tire acquisition system capable of comprehensive data capture across all tire zones. The defects sought exhibit significant variations in size (ranging from a few millimeters to several tens of centimeters) and type (including abnormal stains during processing, marks resulting from demolding issues, foreign particles, air bubbles, deformations, etc.). Our proposed methodology comprises two distinct stages: an initial instance segmentation phase for defect detection and localization, followed by a classification stage based on severity levels, integrating features extracted from the detection network of the first stage alongside tire metadata. Experimental validation demonstrates that the proposed approach achieves automation objectives, attaining satisfactory results in terms of defect detection and classification according to severity, with a F1 score between 0.7 and 0.89 depending on the tire zone. In addition, this study presents a novel method applicable to all tire areas, addressing a wide variety of defects within the domain.},
  langid = {english},
  keywords = {Deep learning,Mask R-CNN,Multimodal,Severity classification,Surface defect detection,Tire quality control},
  file = {C:\Users\E097600\Zotero\storage\YHAUTHZW\Mignot et al. - 2024 - An automatic inspection system for the detection o.pdf}
}

@article{mikriukovCorrectionLocalConcept2025,
  title = {Correction: {{Local Concept Embeddings}} for {{Analysis}} of {{Concept Distributions}} in {{Vision DNN Feature Spaces}}},
  shorttitle = {Correction},
  author = {Mikriukov, Georgii and Schwalbe, Gesina and Bade, Korinna},
  year = 2025,
  month = jun,
  journal = {International Journal of Computer Vision},
  issn = {1573-1405},
  doi = {10.1007/s11263-025-02501-8},
  urldate = {2025-09-22},
  langid = {english},
  keywords = {Computer vision},
  file = {C:\Users\E097600\Zotero\storage\88K5U3BD\Mikriukov et al. - 2025 - Correction Local Concept Embeddings for Analysis of Concept Distributions in Vision DNN Feature Spa.pdf}
}

@misc{milletariVNetFullyConvolutional2016,
  title = {V-{{Net}}: {{Fully Convolutional Neural Networks}} for {{Volumetric Medical Image Segmentation}}},
  shorttitle = {V-{{Net}}},
  author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
  year = 2016,
  month = jun,
  number = {arXiv:1606.04797},
  eprint = {1606.04797},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1606.04797},
  urldate = {2025-01-17},
  abstract = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\Z23INAJM\\Milletari et al. - 2016 - V-Net Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\S77WWXF2\\1606.html}
}

@article{mimouniDomainSpecificKnowledge2020,
  title = {Domain {{Specific Knowledge Graph Embedding}} for {{Analogical Link Discovery}}},
  author = {Mimouni, Nada and Moissinac, Jean-Claude and Vu, Anh Tuan},
  year = 2020,
  month = jun,
  abstract = {General purpose knowledge bases such as DBpedia and Wikidata are valuable resources for various AI tasks. They describe real-world facts as entities and relations between them and they are typically incomplete. Knowledge base completion refers to the task of adding new missing links between entities to build new triples. In this work, we propose an approach for discovering implicit triples using observed ones in the incomplete graph leveraging analogy structures deducted from a knowledge graph embedding model. We use a neural language modelling approach where semantic regularities between words are preserved, which we adapt to entities and relations. We consider domain specific views from large input graphs as the basis for the training, which we call context graphs, as a reduced and meaningful context for a set of entities from a given domain. Results show that analogical inferences in the projected vector space is relevant to a link prediction task in domain knowledge bases.},
  file = {C:\Users\E097600\Zotero\storage\LZBAUIQN\Mimouni et al. - 2020 - Domain Specific Knowledge Graph Embedding for Analogical Link Discovery.pdf}
}

@inproceedings{mittalBestPracticesActive2024,
  title = {Best {{Practices}} in~{{Active Learning}} for~{{Semantic Segmentation}}},
  booktitle = {Pattern {{Recognition}}},
  author = {Mittal, Sudhanshu and Niemeijer, Joshua and Sch{\"a}fer, J{\"o}rg P. and Brox, Thomas},
  editor = {K{\"o}the, Ullrich and Rother, Carsten},
  year = 2024,
  pages = {427--442},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-54605-1_28},
  abstract = {Active learning is particularly of interest for semantic segmentation, where annotations are costly. Previous academic studies focused on datasets that are already very diverse and where the model is trained in a supervised manner with a large annotation budget. In contrast, data collected in many driving scenarios is highly redundant, and most medical applications are subject to very constrained annotation budgets. This work investigates the various types of existing active learning methods for semantic segmentation under diverse conditions across three dimensions - data distribution w.r.t. different redundancy levels, integration of semi-supervised learning, and different labeling budgets. We find that these three underlying factors are decisive for the selection of the best active learning approach. As an outcome of our study, we provide a comprehensive usage guide to obtain the best performance for each case. It is the first systematic study that investigates these dimensions covering a wide range of settings including more than 3K model training runs. In this work, we also propose an exemplary evaluation task for driving scenarios, where data has high redundancy, to showcase the practical implications of our research findings.},
  isbn = {978-3-031-54605-1},
  langid = {english},
  keywords = {Active learning,method,review,semantic segmentation},
  file = {C:\Users\E097600\Zotero\storage\CGN7MVF3\Mittal et al. - 2024 - Best Practices in Active Learning for Semantic Segmentation.pdf}
}

@misc{mittalPartingIllusionsDeep2019,
  title = {Parting with {{Illusions}} about {{Deep Active Learning}}},
  author = {Mittal, Sudhanshu and Tatarchenko, Maxim and {\c C}i{\c c}ek, {\"O}zg{\"u}n and Brox, Thomas},
  year = 2019,
  month = dec,
  number = {arXiv:1912.05361},
  eprint = {1912.05361},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.05361},
  urldate = {2024-03-15},
  abstract = {Active learning aims to reduce the high labeling cost involved in training machine learning models on large datasets by efficiently labeling only the most informative samples. Recently, deep active learning has shown success on various tasks. However, the conventional evaluation scheme used for deep active learning is below par. Current methods disregard some apparent parallel work in the closely related fields. Active learning methods are quite sensitive w.r.t. changes in the training procedure like data augmentation. They improve by a large-margin when integrated with semi-supervised learning, but barely perform better than the random baseline. We re-implement various latest active learning approaches for image classification and evaluate them under more realistic settings. We further validate our findings for semantic segmentation. Based on our observations, we realistically assess the current state of the field and propose a more suitable evaluation protocol.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\JET427CA\\Mittal et al. - 2019 - Parting with Illusions about Deep Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\FDHS7S3E\\1912.html}
}

@article{mohamedReviewVisualisationasexplanationTechniques2022,
  title = {A Review of Visualisation-as-Explanation Techniques for Convolutional Neural Networks and Their Evaluation},
  author = {Mohamed, Elhassan and Sirlantzis, Konstantinos and Howells, Gareth},
  year = 2022,
  month = jul,
  journal = {Displays},
  volume = {73},
  pages = {102239},
  issn = {0141-9382},
  doi = {10.1016/j.displa.2022.102239},
  urldate = {2025-11-06},
  abstract = {Visualisation techniques are powerful tools to understand the behaviour of Artificial Intelligence (AI) systems. They can be used to identify important features contributing to the network decisions, investigate biases in datasets, and find weaknesses in the system's structure (e.g., network architectures). Lawmakers and regulators may not allow the use of smart systems if these systems cannot explain the logic underlying a decision or action taken. These systems are required to offer a high level of 'transparency' to be approved for deployment. Model transparency is vital for safety--critical applications such as autonomous navigation and operation systems (e.g., autonomous trains or cars), where prediction errors may have serious implications. Thus, being highly accurate without explaining the basis of their performance is not enough to satisfy regulatory requirements. The lack of system interpretability is a major obstacle to the wider adoption of AI in safety--critical applications. Explainable Artificial Intelligence (XAI) techniques applied to intelligent systems to justify their decisions offers a possible solution. In this review, we present state-of-the-art explanation techniques in detail. We focus our presentation and critical discussion on visualisation methods for the most adopted architecture in use, the Convolutional Neural Networks (CNNs), applied to the domain of image classification. Further, we discuss the evaluation techniques for different explanation methods, which shows that some of the most visually appealing methods are unreliable and can be considered a simple feature or edge detector. In contrast, robust methods can give insights into the model behaviour, which helps to enhance the model performance and boost the confidence in the model's predictions. Besides, the applications of XAI techniques show their importance in many fields such as medicine and industry. We hope that this review proves a valuable contribution for researchers in the field of XAI.},
  keywords = {Activation heatmaps,Architecture understanding,Black-box representations,CNN visualisation,Convolutional neural networks,Explainable AI,Feature visualisation,Interpretable neural networks,Saliency maps,XAI},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\H56FWNGK\\Mohamed et al. - 2022 - A review of visualisation-as-explanation techniques for convolutional neural networks and their eval.pdf;C\:\\Users\\E097600\\Zotero\\storage\\7PHY4NN3\\S014193822200066X.html}
}

@misc{mohanDeepMetricLearning2023,
  title = {Deep {{Metric Learning}} for {{Computer Vision}}: {{A Brief Overview}}},
  shorttitle = {Deep {{Metric Learning}} for {{Computer Vision}}},
  author = {Mohan, Deen Dayal and Jawade, Bhavin and Setlur, Srirangaraj and Govindaraj, Venu},
  year = 2023,
  month = dec,
  number = {arXiv:2312.10046},
  eprint = {2312.10046},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10046},
  urldate = {2024-03-06},
  abstract = {Objective functions that optimize deep neural networks play a vital role in creating an enhanced feature representation of the input data. Although cross-entropy-based loss formulations have been extensively used in a variety of supervised deep-learning applications, these methods tend to be less adequate when there is large intra-class variance and low inter-class variance in input data distribution. Deep Metric Learning seeks to develop methods that aim to measure the similarity between data samples by learning a representation function that maps these data samples into a representative embedding space. It leverages carefully designed sampling strategies and loss functions that aid in optimizing the generation of a discriminative embedding space even for distributions having low inter-class and high intra-class variances. In this chapter, we will provide an overview of recent progress in this area and discuss state-of-the-art Deep Metric Learning approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\9YBS938S\\Mohan et al. - 2023 - Deep Metric Learning for Computer Vision A Brief .pdf;C\:\\Users\\E097600\\Zotero\\storage\\LQWE2WZ4\\2312.html}
}

@inproceedings{mohanMovingRightDirection2020,
  title = {Moving in the {{Right Direction}}: {{A Regularization}} for {{Deep Metric Learning}}},
  shorttitle = {Moving in the {{Right Direction}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Mohan, Deen Dayal and Sankaran, Nishant and Fedorishin, Dennis and Setlur, Srirangaraj and Govindaraju, Venu},
  year = 2020,
  month = jun,
  pages = {14579--14587},
  issn = {2575-7075},
  doi = {10.1109/CVPR42600.2020.01460},
  urldate = {2024-03-13},
  abstract = {Deep metric learning leverages carefully designed sampling strategies and loss functions that aid in optimizing the generation of a discriminable embedding space. While effective sampling of pairs is critical for shaping the metric space during training, the relative interactions between pairs, and consequently the forces exerted on these pairs that direct their displacement in the embedding space can significantly impact the formation of well separated clusters. In this work, we identify a shortcoming of existing loss formulations which fail to consider more optimal directions of pair displacements as another criterion for optimization. We propose a novel direction regularization to explicitly account for the layout of sampled pairs and attempt to introduce orthogonality in the representations. The proposed regularization is easily integrated into existing loss functions providing considerable performance improvements. We experimentally validate our hypothesis on the Cars-196, CUB-200 and InShop datasets and outperform existing methods to yield state-of-the-art results on these datasets.},
  keywords = {Extraterrestrial measurements,Feature extraction,Force,Learning systems,Optimization,Training},
  file = {C:\Users\E097600\Zotero\storage\I9P5MCLY\9157099.html}
}

@misc{mokModalityAgnosticStructuralImage2024,
  title = {Modality-{{Agnostic Structural Image Representation Learning}} for {{Deformable Multi-Modality Medical Image Registration}}},
  author = {Mok, Tony C. W. and Li, Zi and Bai, Yunhao and Zhang, Jianpeng and Liu, Wei and Zhou, Yan-Jie and Yan, Ke and Jin, Dakai and Shi, Yu and Yin, Xiaoli and Lu, Le and Zhang, Ling},
  year = 2024,
  month = feb,
  number = {arXiv:2402.18933},
  eprint = {2402.18933},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.18933},
  urldate = {2024-03-05},
  abstract = {Establishing dense anatomical correspondence across distinct imaging modalities is a foundational yet challenging procedure for numerous medical image analysis studies and image-guided radiotherapy. Existing multi-modality image registration algorithms rely on statistical-based similarity measures or local structural image representations. However, the former is sensitive to locally varying noise, while the latter is not discriminative enough to cope with complex anatomical structures in multimodal scans, causing ambiguity in determining the anatomical correspondence across scans with different modalities. In this paper, we propose a modality-agnostic structural representation learning method, which leverages Deep Neighbourhood Self-similarity (DNS) and anatomy-aware contrastive learning to learn discriminative and contrast-invariance deep structural image representations (DSIR) without the need for anatomical delineations or pre-aligned training images. We evaluate our method on multiphase CT, abdomen MR-CT, and brain MR T1w-T2w registration. Comprehensive results demonstrate that our method is superior to the conventional local structural representation and statistical-based similarity measures in terms of discriminability and accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\68AQVBHT\\Mok et al. - 2024 - Modality-Agnostic Structural Image Representation .pdf;C\:\\Users\\E097600\\Zotero\\storage\\4TC6IIT5\\2402.html}
}

@book{molnarIntroductionConformalPrediction2023,
  title = {Introduction {{To Conformal Prediction With Python}}: {{A Short Guide For Quantifying Uncertainty Of Machine Learning Models}}},
  shorttitle = {Introduction {{To Conformal Prediction With Python}}},
  author = {Molnar, Christoph},
  year = 2023,
  month = feb,
  publisher = {Independently published},
  address = {M\"unchen, Germany},
  abstract = {Introduction To Conformal Prediction With Python is the quickest way to learn an easy-to-use and very general technique for uncertainty quantification."This concise book is accessible, lucid, and full of helpful code snippets. It explains the mathematical ideas with clarity and provides the reader with practical examples that illustrate the essence of conformal prediction, a powerful idea for uncertainty quantification."-- Junaid Butt, Research Software Engineer, IBM Research"Modern statistics can be a difficult topic, but Christoph has managed to make it feel easy, practical, and fun! Reading this book is a great first step towards gaining mastery of conformal prediction and related topics."-- Anastasios Angelopoulos, Researcher at the University of California, BerkeleySummaryA prerequisite for trust in machine learning is uncertainty quantification. Without it, an accurate prediction and a wild guess look the same.Yet many machine learning models come without uncertainty quantification. And while there are many approaches to uncertainty -- from Bayesian posteriors to bootstrapping -- we have no guarantees that these approaches will perform well on new data."I really enjoyed reading the book. The data science and machine learning community needs more people like Christoph Molnar who are able to translate emerging breakthrough research into digestible concepts. I can see this book becoming a key piece in accelerating the rate of adoption of conformal ML."-- Guilherme Del Nero Maia, Principal Data Science at JabilAt first glance conformal prediction seems like yet another contender. But conformal prediction can work in combination with any other uncertainty approach and has many advantages that make it stand out:Guaranteed coverage: Prediction regions generated by conformal prediction come with coverage guarantees of the true outcomeEasy to use: Conformal prediction approaches can be implemented from scratch with just a few lines of codeModel-agnostic: Conformal prediction works with any machine learning modelDistribution-free: Conformal prediction makes no distributional assumptionsNo retraining required: Conformal prediction can be used without retraining the modelBroad application: conformal prediction works for classification, regression, time series forecasting, and many other tasksSound good?Then this is the right book for you to learn about this versatile, easy-to-use yet powerful tool for taming the uncertainty of your models.This book:Teaches the intuition behind conformal predictionDemonstrates how conformal prediction works for classification and regressionShows how to apply conformal prediction using Python and MAPIEEnables you to quickly learn new conformal algorithmsWith the knowledge in this book, you'll be ready to quantify the uncertainty of any model."This book is a comprehensive guide and resource for anyone who wants to learn how to quantify uncertainty with conformal prediction by using python. Christoph's writing is clear and engaging. He provides practical examples that help readers understand how to apply conformal prediction techniques/concepts to real-world problems."-- Tony Zhang, Data Scientist at Munich Re},
  isbn = {979-8-3775-0935-6},
  langid = {english},
  keywords = {coo},
  file = {C:\Users\E097600\Zotero\storage\T2DFTDLH\Molnar - 2023 - Introduction To Conformal Prediction With Python A Short Guide For Quantifying Uncertainty Of Machi.pdf}
}

@book{montalionLearningSystemsThinking2024,
  title = {Learning {{Systems Thinking}}: {{Essential Nonlinear Skills}} and {{Practices}} for {{Software Professionals}}},
  shorttitle = {Learning {{Systems Thinking}}},
  author = {Montalion, Diana},
  year = 2024,
  month = aug,
  edition = {1st edition},
  publisher = {O'Reilly Media},
  address = {Sebastopol},
  abstract = {Welcome to the systems age, where software professionals are no longer building software; we're building systems of software. Change is continuously deployed across software ecosystems coordinated by responsive infrastructure.  In this world of increasing relational complexity, we need to think differently. Many of our challenges are systemic. This book shows you how systems thinking can guide you through the complexity of modern systems. Rather than relying on traditional reductionistic approaches, author Diana Montalion shows you how to expand your skill set so we can think, communicate, and act as healthy systems.  Systems thinking is a practice that improves your effectiveness and enables you to lead impactful change. Through a series of practices and real-world scenarios, you'll learn to shift your perspective in order to design, develop, and deliver better outcomes.  You'll learn: How linear thinking limits your ability to solve system challenges Common obstacles to systems thinking and how to move past them New skills and practices that will transform how you think, learn, and lead Methods for thinking well with others and creating sound recommendations How to measure success in the midst of complexity and uncertainty},
  isbn = {978-1-0981-5133-1},
  langid = {english},
  keywords = {book},
  file = {C:\Users\E097600\Zotero\storage\3HMJRVIT\Montalion - 2024 - Learning Systems Thinking Essential Nonlinear Skills and Practices for Software Professionals.pdf}
}

@misc{monteiroConditionalRandomFields2018,
  title = {Conditional {{Random Fields}} as {{Recurrent Neural Networks}} for {{3D Medical Imaging Segmentation}}},
  author = {Monteiro, Miguel and Figueiredo, M{\'a}rio A. T. and Oliveira, Arlindo L.},
  year = 2018,
  month = jul,
  number = {arXiv:1807.07464},
  eprint = {1807.07464},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.07464},
  urldate = {2025-03-24},
  abstract = {The Conditional Random Field as a Recurrent Neural Network layer is a recently proposed algorithm meant to be placed on top of an existing Fully-Convolutional Neural Network to improve the quality of semantic segmentation. In this paper, we test whether this algorithm, which was shown to improve semantic segmentation for 2D RGB images, is able to improve segmentation quality for 3D multi-modal medical images. We developed an implementation of the algorithm which works for any number of spatial dimensions, input/output image channels, and reference image channels. As far as we know this is the first publicly available implementation of this sort. We tested the algorithm with two distinct 3D medical imaging datasets, we concluded that the performance differences observed were not statistically significant. Finally, in the discussion section of the paper, we go into the reasons as to why this technique transfers poorly from natural images to medical images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\F2JT789N\\Monteiro et al. - 2018 - Conditional Random Fields as Recurrent Neural Networks for 3D Medical Imaging Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\CJUZ3Z93\\1807.html}
}

@misc{monteiroStochasticSegmentationNetworks2020,
  title = {Stochastic {{Segmentation Networks}}: {{Modelling Spatially Correlated Aleatoric Uncertainty}}},
  shorttitle = {Stochastic {{Segmentation Networks}}},
  author = {Monteiro, Miguel and Folgoc, Lo{\"i}c Le and de Castro, Daniel Coelho and Pawlowski, Nick and Marques, Bernardo and Kamnitsas, Konstantinos and van der Wilk, Mark and Glocker, Ben},
  year = 2020,
  month = dec,
  number = {arXiv:2006.06015},
  eprint = {2006.06015},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.06015},
  urldate = {2025-01-21},
  abstract = {In image segmentation, there is often more than one plausible solution for a given input. In medical imaging, for example, experts will often disagree about the exact location of object boundaries. Estimating this inherent uncertainty and predicting multiple plausible hypotheses is of great interest in many applications, yet this ability is lacking in most current deep learning methods. In this paper, we introduce stochastic segmentation networks (SSNs), an efficient probabilistic method for modelling aleatoric uncertainty with any image segmentation network architecture. In contrast to approaches that produce pixel-wise estimates, SSNs model joint distributions over entire label maps and thus can generate multiple spatially coherent hypotheses for a single image. By using a low-rank multivariate normal distribution over the logit space to model the probability of the label map given the image, we obtain a spatially consistent probability distribution that can be efficiently computed by a neural network without any changes to the underlying architecture. We tested our method on the segmentation of real-world medical data, including lung nodules in 2D CT and brain tumours in 3D multimodal MRI scans. SSNs outperform state-of-the-art for modelling correlated uncertainty in ambiguous images while being much simpler, more flexible, and more efficient.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\SA4RBYEC\\Monteiro et al. - 2020 - Stochastic Segmentation Networks Modelling Spatially Correlated Aleatoric Uncertainty.pdf;C\:\\Users\\E097600\\Zotero\\storage\\R4SULP3U\\2006.html}
}

@inproceedings{morrisonUncertaintyawareInstanceSegmentation2019,
  title = {Uncertainty-Aware {{Instance Segmentation}} Using {{Dropout Sampling}}},
  booktitle = {{{CVPR}}},
  author = {Morrison, Doug and Anton, Milan},
  year = 2019,
  month = jan,
  publisher = {CVPR},
  abstract = {Vision is an integral part of many robotic systems, and especially so when a robot must interact with its environment. In such cases, decisions made based on erroneous visual detections can have disastrous consequences. Hence, being able to accurately measure the uncertainty associated with visual information is highly important for making informed decisions. However, this uncertainty is often not captured by classic computer vision systems or metrics. In this paper we address the task of instance segmentation in a robotics context, where we are concerned with uncertainty associated with not only the class of an object (semantic uncertainty) but also its location (spatial uncertainty). We apply dropout sampling to the state-of-the-art instance segmentation network Mask-RCNN to provide estimates of both semantic uncertainty and spatial uncertainty. We show that a metric which combines both uncertainty measures provides an estimate of uncertainty which improves over either one individually. Additionally, we apply our technique to the ACRV Probabilistic Object Detection dataset where it achieves a score of 14.65.},
  keywords = {Active learning,dropout,Instance segmentation},
  file = {C:\Users\E097600\Zotero\storage\9ZJA8V4S\rvc_4.pdf}
}

@misc{moserCoresetSelectionCoreset2025,
  title = {A {{Coreset Selection}} of {{Coreset Selection Literature}}: {{Introduction}} and {{Recent Advances}}},
  shorttitle = {A {{Coreset Selection}} of {{Coreset Selection Literature}}},
  author = {Moser, Brian B. and Shanbhag, Arundhati S. and Frolov, Stanislav and Raue, Federico and Folz, Joachim and Dengel, Andreas},
  year = 2025,
  month = may,
  number = {arXiv:2505.17799},
  eprint = {2505.17799},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.17799},
  urldate = {2025-05-27},
  abstract = {Coreset selection targets the challenge of finding a small, representative subset of a large dataset that preserves essential patterns for effective machine learning. Although several surveys have examined data reduction strategies before, most focus narrowly on either classical geometry-based methods or active learning techniques. In contrast, this survey presents a more comprehensive view by unifying three major lines of coreset research, namely, training-free, training-oriented, and label-free approaches, into a single taxonomy. We present subfields often overlooked by existing work, including submodular formulations, bilevel optimization, and recent progress in pseudo-labeling for unlabeled datasets. Additionally, we examine how pruning strategies influence generalization and neural scaling laws, offering new insights that are absent from prior reviews. Finally, we compare these methods under varying computational, robustness, and performance demands and highlight open challenges, such as robustness, outlier filtering, and adapting coreset selection to foundation models, for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,coreset,review},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\R8MWWVBC\\Moser et al. - 2025 - A Coreset Selection of Coreset Selection Literature Introduction and Recent Advances.pdf;C\:\\Users\\E097600\\Zotero\\storage\\KB4LMHF9\\2505.html}
}

@misc{moserCoresetSelectionCoreset2025a,
  title = {A {{Coreset Selection}} of {{Coreset Selection Literature}}: {{Introduction}} and {{Recent Advances}}},
  shorttitle = {A {{Coreset Selection}} of {{Coreset Selection Literature}}},
  author = {Moser, Brian B. and Shanbhag, Arundhati S. and Frolov, Stanislav and Raue, Federico and Folz, Joachim and Dengel, Andreas},
  year = 2025,
  month = may,
  journal = {arXiv.org},
  urldate = {2025-06-02},
  abstract = {Coreset selection targets the challenge of finding a small, representative subset of a large dataset that preserves essential patterns for effective machine learning. Although several surveys have examined data reduction strategies before, most focus narrowly on either classical geometry-based methods or active learning techniques. In contrast, this survey presents a more comprehensive view by unifying three major lines of coreset research, namely, training-free, training-oriented, and label-free approaches, into a single taxonomy. We present subfields often overlooked by existing work, including submodular formulations, bilevel optimization, and recent progress in pseudo-labeling for unlabeled datasets. Additionally, we examine how pruning strategies influence generalization and neural scaling laws, offering new insights that are absent from prior reviews. Finally, we compare these methods under varying computational, robustness, and performance demands and highlight open challenges, such as robustness, outlier filtering, and adapting coreset selection to foundation models, for future research.},
  howpublished = {https://arxiv.org/abs/2505.17799v1},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\R77ZWR2X\Moser et al. - 2025 - A Coreset Selection of Coreset Selection Literature Introduction and Recent Advances.pdf}
}

@misc{motsoehliBalancingAccuracyCalibration2025,
  title = {Balancing {{Accuracy}}, {{Calibration}}, and {{Efficiency}} in {{Active Learning}} with {{Vision Transformers Under Label Noise}}},
  author = {Mots'oehli, Moseli and Mogale, Hope and Baek, Kyungim},
  year = 2025,
  month = may,
  number = {arXiv:2505.04375},
  eprint = {2505.04375},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.04375},
  urldate = {2025-05-13},
  abstract = {Fine-tuning pre-trained convolutional neural networks on ImageNet for downstream tasks is well-established. Still, the impact of model size on the performance of vision transformers in similar scenarios, particularly under label noise, remains largely unexplored. Given the utility and versatility of transformer architectures, this study investigates their practicality under low-budget constraints and noisy labels. We explore how classification accuracy and calibration are affected by symmetric label noise in active learning settings, evaluating four vision transformer configurations (Base and Large with 16x16 and 32x32 patch sizes) and three Swin Transformer configurations (Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label noise rates. Our findings show that larger ViT models (ViTl32 in particular) consistently outperform their smaller counterparts in both accuracy and calibration, even under moderate to high label noise, while Swin Transformers exhibit weaker robustness across all noise levels. We find that smaller patch sizes do not always lead to better performance, as ViTl16 performs consistently worse than ViTl32 while incurring a higher computational cost. We also find that information-based Active Learning strategies only provide meaningful accuracy improvements at moderate label noise rates, but they result in poorer calibration compared to models trained on randomly acquired labels, especially at high label noise rates. We hope these insights provide actionable guidance for practitioners looking to deploy vision transformers in resource-constrained environments, where balancing model complexity, label noise, and compute efficiency is critical in model fine-tuning or distillation.},
  archiveprefix = {arXiv},
  keywords = {Calibration,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Image classification,Vision transformer},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\UW952HQB\\Mots'oehli et al. - 2025 - Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under La.pdf;C\:\\Users\\E097600\\Zotero\\storage\\HV5DWS5A\\2505.html}
}

@misc{movshovitz-attiasNoFussDistance2017,
  title = {No {{Fuss Distance Metric Learning}} Using {{Proxies}}},
  author = {{Movshovitz-Attias}, Yair and Toshev, Alexander and Leung, Thomas K. and Ioffe, Sergey and Singh, Saurabh},
  year = 2017,
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-03-12},
  abstract = {We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity. Traditionally, for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship -- an anchor point \$x\$ is similar to a set of positive points \$Y\$, and dissimilar to a set of negative points \$Z\$, and a loss defined over these distances is minimized. While the specifics of the optimization differ, in this work we collectively call this type of supervision Triplets and all methods that follow this pattern Triplet-Based methods. These methods are challenging to optimize. A main issue is the need for finding informative triplets, which is usually achieved by a variety of tricks such as increasing the batch size, hard or semi-hard triplet mining, etc. Even with these tricks, the convergence rate of such methods is slow. In this paper we propose to optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points which are learned as well. These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss. This proxy-based loss is empirically better behaved. As a result, the proxy-loss improves on state-of-art results for three standard zero-shot learning datasets, by up to 15\% points, while converging three times as fast as other triplet-based losses.},
  howpublished = {https://arxiv.org/abs/1703.07464v3},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\BLUM5DTA\Movshovitz-Attias et al. - 2017 - No Fuss Distance Metric Learning using Proxies.pdf}
}

@misc{mucllariNoiseTolerantCoresetBasedClass2025,
  title = {Noise-{{Tolerant Coreset-Based Class Incremental Continual Learning}}},
  author = {Mucllari, Edison and Raghavan, Aswin and Daniels, Zachary Alan},
  year = 2025,
  month = apr,
  number = {arXiv:2504.16763},
  eprint = {2504.16763},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.16763},
  urldate = {2025-04-28},
  abstract = {Many applications of computer vision require the ability to adapt to novel data distributions after deployment. Adaptation requires algorithms capable of continual learning (CL). Continual learners must be plastic to adapt to novel tasks while minimizing forgetting of previous tasks.However, CL opens up avenues for noise to enter the training pipeline and disrupt the CL. This work focuses on label noise and instance noise in the context of class-incremental learning (CIL), where new classes are added to a classifier over time, and there is no access to external data from past classes. We aim to understand the sensitivity of CL methods that work by replaying items from a memory constructed using the idea of Coresets. We derive a new bound for the robustness of such a method to uncorrelated instance noise under a general additive noise threat model, revealing several insights. Putting the theory into practice, we create two continual learning algorithms to construct noise-tolerant replay buffers. We empirically compare the effectiveness of prior memory-based continual learners and the proposed algorithms under label and uncorrelated instance noise on five diverse datasets. We show that existing memory-based CL are not robust whereas the proposed methods exhibit significant improvements in maximizing classification accuracy and minimizing forgetting in the noisy CIL setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\GXYZIEK9\\Mucllari et al. - 2025 - Noise-Tolerant Coreset-Based Class Incremental Continual Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\E89GBYQ4\\2504.html}
}

@misc{munjalRobustReproducibleActive2022,
  title = {Towards {{Robust}} and {{Reproducible Active Learning Using Neural Networks}}},
  author = {Munjal, Prateek and Hayat, Nasir and Hayat, Munawar and Sourati, Jamshid and Khan, Shadab},
  year = 2022,
  month = jun,
  number = {arXiv:2002.09564},
  eprint = {2002.09564},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.09564},
  urldate = {2024-03-26},
  abstract = {Active learning (AL) is a promising ML paradigm that has the potential to parse through large unlabeled data and help reduce annotation cost in domains where labeling data can be prohibitive. Recently proposed neural network based AL methods use different heuristics to accomplish this goal. In this study, we demonstrate that under identical experimental settings, different types of AL algorithms (uncertainty based, diversity based, and committee based) produce an inconsistent gain over random sampling baseline. Through a variety of experiments, controlling for sources of stochasticity, we show that variance in performance metrics achieved by AL algorithms can lead to results that are not consistent with the previously reported results. We also found that under strong regularization, AL methods show marginal or no advantage over the random sampling baseline under a variety of experimental conditions. Finally, we conclude with a set of recommendations on how to assess the results using a new AL algorithm to ensure results are reproducible and robust under changes in experimental conditions. We share our codes to facilitate AL evaluations. We believe our findings and recommendations will help advance reproducible research in AL using neural networks. We open source our code at https://github.com/PrateekMunjal/TorchAL},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\IHX8P49W\\Munjal et al. - 2022 - Towards Robust and Reproducible Active Learning Us.pdf;C\:\\Users\\E097600\\Zotero\\storage\\R2MGQS8A\\2002.html}
}

@misc{nadagoudaActiveMetricLearning2022,
  title = {Active Metric Learning and Classification Using Similarity Queries},
  author = {Nadagouda, Namrata and Xu, Austin and Davenport, Mark A.},
  year = 2022,
  month = feb,
  number = {arXiv:2202.01953},
  eprint = {2202.01953},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.01953},
  urldate = {2024-03-13},
  abstract = {Active learning is commonly used to train label-efficient models by adaptively selecting the most informative queries. However, most active learning strategies are designed to either learn a representation of the data (e.g., embedding or metric learning) or perform well on a task (e.g., classification) on the data. However, many machine learning tasks involve a combination of both representation learning and a task-specific goal. Motivated by this, we propose a novel unified query framework that can be applied to any problem in which a key component is learning a representation of the data that reflects similarity. Our approach builds on similarity or nearest neighbor (NN) queries which seek to select samples that result in improved embeddings. The queries consist of a reference and a set of objects, with an oracle selecting the object most similar (i.e., nearest) to the reference. In order to reduce the number of solicited queries, they are chosen adaptively according to an information theoretic criterion. We demonstrate the effectiveness of the proposed strategy on two tasks -- active metric learning and active classification -- using a variety of synthetic and real world datasets. In particular, we demonstrate that actively selected NN queries outperform recently developed active triplet selection methods in a deep metric learning setting. Further, we show that in classification, actively selecting class labels can be reformulated as a process of selecting the most informative NN query, allowing direct application of our method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\QRTFUM5F\\Nadagouda et al. - 2022 - Active metric learning and classification using si.pdf;C\:\\Users\\E097600\\Zotero\\storage\\9EL84Q5U\\2202.html}
}

@misc{nagarajRegretfulDecisionsLabel2025,
  title = {Regretful {{Decisions}} under {{Label Noise}}},
  author = {Nagaraj, Sujay and Liu, Yang and Calmon, Flavio P. and Ustun, Berk},
  year = 2025,
  month = apr,
  number = {arXiv:2504.09330},
  eprint = {2504.09330},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.09330},
  urldate = {2025-04-28},
  abstract = {Machine learning models are routinely used to support decisions that affect individuals -- be it to screen a patient for a serious illness or to gauge their response to treatment. In these tasks, we are limited to learning models from datasets with noisy labels. In this paper, we study the instance-level impact of learning under label noise. We introduce a notion of regret for this regime which measures the number of unforeseen mistakes due to noisy labels. We show that standard approaches to learning under label noise can return models that perform well at a population level while subjecting individuals to a lottery of mistakes. We present a versatile approach to estimate the likelihood of mistakes at the individual level from a noisy dataset by training models over plausible realizations of datasets without label noise. This is supported by a comprehensive empirical study of label noise in clinical prediction tasks. Our results reveal how failure to anticipate mistakes can compromise model reliability and adoption, and demonstrate how we can address these challenges by anticipating and avoiding regretful decisions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\N4WFSCFD\\Nagaraj et al. - 2025 - Regretful Decisions under Label Noise.pdf;C\:\\Users\\E097600\\Zotero\\storage\\EECAMQ4W\\2504.html}
}

@article{nayalLikelihoodRatioBasedApproach2025,
  title = {A {{Likelihood Ratio-Based Approach}} to {{Segmenting Unknown Objects}}},
  author = {Nayal, Nazir and Shoeb, Youssef and G{\"u}ney, Fatma},
  year = 2025,
  month = jul,
  journal = {International Journal of Computer Vision},
  issn = {1573-1405},
  doi = {10.1007/s11263-025-02509-0},
  urldate = {2025-09-22},
  abstract = {Addressing the Out-of-Distribution (OoD) segmentation task is a prerequisite for perception systems operating in an open-world environment. Large foundational models are frequently used in downstream tasks, however, their potential for OoD remains mostly unexplored. We seek to leverage a large foundational model to achieve robust representation. Outlier supervision is a widely used strategy for improving OoD detection of the existing segmentation networks. However, current approaches for outlier supervision involve retraining parts of the original network, which is typically disruptive to the model's learned feature representation. Furthermore, retraining becomes infeasible in the case of large foundational models. Our goal is to retrain for outlier segmentation without compromising the strong representation space of the foundational model. To this end, we propose an adaptive, lightweight unknown estimation module (UEM) for outlier supervision that significantly enhances the OoD segmentation performance without affecting the learned feature representation of the original network. UEM learns a distribution for outliers and a generic distribution for known classes. Using the learned distributions, we propose a likelihood-ratio-based outlier scoring function that fuses the confidence of UEM with that of the pixel-wise segmentation inlier network to detect unknown objects. We also propose an objective to optimize this score directly. Our approach achieves a new state-of-the-art across multiple datasets, outperforming the previous best method by 5.74\% average precision points while having a lower false-positive rate. Importantly, strong inlier performance remains unaffected. The code and pre-trained models are available at: https://github.com/NazirNayal8/UEM-likelihood-ratio.},
  langid = {english},
  keywords = {Anomaly Segmentation,Computer Science - Computer Vision and Pattern Recognition,Computer vision,Foundational Models for OoD,Likelihood Ratio,OoD Segmentation,Out-of-Distribution Detection,Unknown Segmentation},
  file = {C:\Users\E097600\Zotero\storage\78B4DCCQ\Nayal et al. - 2025 - A Likelihood Ratio-Based Approach to Segmenting Unknown Objects.pdf}
}

@misc{nguyenProvablyImprovingGeneralization2025,
  title = {Provably {{Improving Generalization}} of {{Few-Shot Models}} with {{Synthetic Data}}},
  author = {Nguyen, Lan-Cuong and {Nguyen-Tri}, Quan and Khanh, Bang Tran and Le, Dung D. and {Tran-Thanh}, Long and Than, Khoat},
  year = 2025,
  month = may,
  journal = {arXiv.org},
  urldate = {2025-06-02},
  abstract = {Few-shot image classification remains challenging due to the scarcity of labeled training examples. Augmenting them with synthetic data has emerged as a promising way to alleviate this issue, but models trained on synthetic samples often face performance degradation due to the inherent gap between real and synthetic distributions. To address this limitation, we develop a theoretical framework that quantifies the impact of such distribution discrepancies on supervised learning, specifically in the context of image classification. More importantly, our framework suggests practical ways to generate good synthetic samples and to train a predictor with high generalization ability. Building upon this framework, we propose a novel theoretical-based algorithm that integrates prototype learning to optimize both data partitioning and model training, effectively bridging the gap between real few-shot data and synthetic data. Extensive experiments results show that our approach demonstrates superior performance compared to state-of-the-art methods, outperforming them across multiple datasets.},
  howpublished = {https://arxiv.org/abs/2505.24190v1},
  langid = {english},
  keywords = {few shot learning,image generation},
  file = {C:\Users\E097600\Zotero\storage\BIHWYJ98\Nguyen et al. - 2025 - Provably Improving Generalization of Few-Shot Models with Synthetic Data.pdf}
}

@misc{nguyenWeNeedAll2025,
  title = {Do {{We Need All}} the {{Synthetic Data}}? {{Towards Targeted Synthetic Image Augmentation}} via {{Diffusion Models}}},
  shorttitle = {Do {{We Need All}} the {{Synthetic Data}}?},
  author = {Nguyen, Dang and Li, Jiping and Zheng, Jinghao and Mirzasoleiman, Baharan},
  year = 2025,
  month = may,
  number = {arXiv:2505.21574},
  eprint = {2505.21574},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.21574},
  urldate = {2025-06-02},
  abstract = {Synthetically augmenting training datasets with diffusion models has been an effective strategy for improving generalization of image classifiers. However, existing techniques struggle to ensure the diversity of generation and increase the size of the data by up to 10-30x to improve the in-distribution performance. In this work, we show that synthetically augmenting part of the data that is not learned early in training outperforms augmenting the entire dataset. By analyzing a two-layer CNN, we prove that this strategy improves generalization by promoting homogeneity in feature learning speed without amplifying noise. Our extensive experiments show that by augmenting only 30\%-40\% of the data, our method boosts the performance by up to 2.8\% in a variety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10, CIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM. Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on CIFAR-100 and TinyImageNet. It can also easily stack with existing weak and strong augmentation strategies to further boost the performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\E8BDV72A\\Nguyen et al. - 2025 - Do We Need All the Synthetic Data Towards Targeted Synthetic Image Augmentation via Diffusion Model.pdf;C\:\\Users\\E097600\\Zotero\\storage\\9P3KLPUB\\2505.html}
}

@misc{niuUnsupervisedUniversalImage2023,
  title = {Unsupervised {{Universal Image Segmentation}}},
  author = {Niu, Dantong and Wang, Xudong and Han, Xinyang and Lian, Long and Herzig, Roei and Darrell, Trevor},
  year = 2023,
  month = dec,
  number = {arXiv:2312.17243},
  eprint = {2312.17243},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.17243},
  urldate = {2024-03-06},
  abstract = {Several unsupervised image segmentation approaches have been proposed which eliminate the need for dense manually-annotated segmentation masks; current models separately handle either semantic segmentation (e.g., STEGO) or class-agnostic instance segmentation (e.g., CutLER), but not both (i.e., panoptic segmentation). We propose an Unsupervised Universal Segmentation model (U2Seg) adept at performing various image segmentation tasks -- instance, semantic and panoptic -- using a novel unified framework. U2Seg generates pseudo semantic labels for these segmentation tasks via leveraging self-supervised models followed by clustering; each cluster represents different semantic and/or instance membership of pixels. We then self-train the model on these pseudo semantic labels, yielding substantial performance gains over specialized methods tailored to each task: a +2.6 AP\$\textasciicircum\textbraceleft\textbackslash text\textbraceleft box\textbraceright\textbraceright\$ boost vs. CutLER in unsupervised instance segmentation on COCO and a +7.0 PixelAcc increase (vs. STEGO) in unsupervised semantic segmentation on COCOStuff. Moreover, our method sets up a new baseline for unsupervised panoptic segmentation, which has not been previously explored. U2Seg is also a strong pretrained model for few-shot segmentation, surpassing CutLER by +5.0 AP\$\textasciicircum\textbraceleft\textbackslash text\textbraceleft mask\textbraceright\textbraceright\$ when trained on a low-data regime, e.g., only 1\% COCO labels. We hope our simple yet effective method can inspire more research on unsupervised universal image segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\5DYYLPMD\\Niu et al. - 2023 - Unsupervised Universal Image Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ZVS68Z7J\\2312.html}
}

@misc{nogueiraCoreSetSelectionDataefficient2025,
  title = {Core-{{Set Selection}} for {{Data-efficient Land Cover Segmentation}}},
  author = {Nogueira, Keiller and Zaytar, Akram and Ma, Wanli and Roscher, Ribana and H{\"a}nsch, Ronny and Robinson, Caleb and Ortiz, Anthony and Nsutezo, Simone and Dodhia, Rahul and Ferres, Juan M. Lavista and Karaku{\c s}, Oktay and Rosin, Paul L.},
  year = 2025,
  month = may,
  number = {arXiv:2505.01225},
  eprint = {2505.01225},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.01225},
  urldate = {2025-05-05},
  abstract = {The increasing accessibility of remotely sensed data and the potential of such data to inform large-scale decision-making has driven the development of deep learning models for many Earth Observation tasks. Traditionally, such models must be trained on large datasets. However, the common assumption that broadly larger datasets lead to better outcomes tends to overlook the complexities of the data distribution, the potential for introducing biases and noise, and the computational resources required for processing and storing vast datasets. Therefore, effective solutions should consider both the quantity and quality of data. In this paper, we propose six novel core-set selection methods for selecting important subsets of samples from remote sensing image segmentation datasets that rely on imagery only, labels only, and a combination of each. We benchmark these approaches against a random-selection baseline on three commonly used land cover classification datasets: DFC2022, Vaihingen, and Potsdam. In each of the datasets, we demonstrate that training on a subset of samples outperforms the random baseline, and some approaches outperform training on all available data. This result shows the importance and potential of data-centric learning for the remote sensing domain. The code is available at https://github.com/keillernogueira/data-centric-rs-classification/.},
  archiveprefix = {arXiv},
  keywords = {Active sampling,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\ZU4WHIIS\\Nogueira et al. - 2025 - Core-Set Selection for Data-efficient Land Cover Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\RSXIQYJX\\2505.html}
}

@misc{nuggehalliDIRECTDeepActive2024,
  title = {{{DIRECT}}: {{Deep Active Learning}} under {{Imbalance}} and {{Label Noise}}},
  shorttitle = {{{DIRECT}}},
  author = {Nuggehalli, Shyam and Zhang, Jifan and Jain, Lalit and Nowak, Robert},
  year = 2024,
  month = feb,
  number = {arXiv:2312.09196},
  eprint = {2312.09196},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.09196},
  urldate = {2025-09-18},
  abstract = {Class imbalance is a prevalent issue in real world machine learning applications, often leading to poor performance in rare and minority classes. With an abundance of wild unlabeled data, active learning is perhaps the most effective technique in solving the problem at its root -- collecting a more balanced and informative set of labeled examples during annotation. Label noise is another common issue in data annotation jobs, which is especially challenging for active learning methods. In this work, we conduct the first study of active learning under both class imbalance and label noise. We propose a novel algorithm that robustly identifies the class separation threshold and annotates the most uncertain examples that are closest from it. Through a novel reduction to one-dimensional active learning, our algorithm DIRECT is able to leverage the classic active learning literature to address issues such as batch labeling and tolerance towards label noise. We present extensive experiments on imbalanced datasets with and without label noise. Our results demonstrate that DIRECT can save more than 60\% of the annotation budget compared to state-of-art active learning algorithms and more than 80\% of annotation budget compared to random sampling.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Class imbalance,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Image classification,label noise},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\BJALETNX\\Nuggehalli et al. - 2024 - DIRECT Deep Active Learning under Imbalance and Label Noise.pdf;C\:\\Users\\E097600\\Zotero\\storage\\TVHDHP52\\2312.html}
}

@misc{nuggehalliImprovedAlgorithmDeep2025,
  title = {Improved {{Algorithm}} for {{Deep Active Learning}} under {{Imbalance}} via {{Optimal Separation}}},
  author = {Nuggehalli, Shyam and Zhang, Jifan and Jain, Lalit and Nowak, Robert},
  year = 2025,
  month = jun,
  number = {arXiv:2312.09196},
  eprint = {2312.09196},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.09196},
  urldate = {2025-09-18},
  abstract = {Class imbalance severely impacts machine learning performance on minority classes in real-world applications. While various solutions exist, active learning offers a fundamental fix by strategically collecting balanced, informative labeled examples from abundant unlabeled data. We introduce DIRECT, an algorithm that identifies class separation boundaries and selects the most uncertain nearby examples for annotation. By reducing the problem to one-dimensional active learning, DIRECT leverages established theory to handle batch labeling and label noise -- another common challenge in data annotation that particularly affects active learning methods. Our work presents the first comprehensive study of active learning under both class imbalance and label noise. Extensive experiments on imbalanced datasets show DIRECT reduces annotation costs by over 60\textbackslash\% compared to state-of-the-art active learning methods and over 80\textbackslash\% versus random sampling, while maintaining robustness to label noise.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\T7B8XKKR\\Nuggehalli et al. - 2025 - Improved Algorithm for Deep Active Learning under Imbalance via Optimal Separation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\SRPSJJZ3\\2312.html}
}

@inproceedings{nunnariStudyFusionPixels2020,
  title = {A {{Study}} on the {{Fusion}} of {{Pixels}} and {{Patient Metadata}} in {{CNN-Based Classification}} of {{Skin Lesion Images}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}}},
  author = {Nunnari, Fabrizio and Bhuvaneshwara, Chirag and Ezema, Abraham Obinwanne and Sonntag, Daniel},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A. Min and Weippl, Edgar},
  year = 2020,
  month = aug,
  series = {Machine {{Learning}} and {{Knowledge Extraction}}},
  volume = {LNCS-12279},
  pages = {191--208},
  publisher = {Springer International Publishing},
  address = {Dublin, Ireland},
  doi = {10.1007/978-3-030-57321-8_11},
  urldate = {2025-11-17},
  abstract = {We present a study on the fusion of pixel data and patient metadata (age, gender, and body location) for improving the classification of skin lesion images. The experiments have been conducted with the ISIC 2019 skin lesion classification challenge data set. Taking two plain convolutional neural networks (CNNs) as a baseline, metadata are merged using either non-neural machine learning methods (tree-based and support vector machines) or shallow neural networks. Results show that shallow neural networks outperform other approaches in all overall evaluation measures. However, despite the increase in the classification accuracy (up~to +19.1\%), interestingly, the average per-class sensitivity decreases in three out of four cases for CNNs, thus suggesting that using metadata penalizes the prediction accuracy for lower represented classes. A study on the patient metadata shows that age is the most useful metadatum as a decision criterion, followed by body location and gender.},
  keywords = {Convolutional neural network,Data fusion,Machine learning,Patient metadata,Skin lesion classification},
  file = {C:\Users\E097600\Zotero\storage\884KR7B9\Nunnari et al. - 2020 - A Study on the Fusion of Pixels and Patient Metadata in CNN-Based Classification of Skin Lesion Imag.pdf}
}

@misc{OnlineAdaptiveAsymmetric,
  title = {Online {{Adaptive Asymmetric Active Learning}} for {{Budgeted Imbalanced Data}} \textbar{} {{Proceedings}} of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  urldate = {2025-02-13},
  howpublished = {https://dl.acm.org/doi/10.1145/3219819.3219948},
  file = {C:\Users\E097600\Zotero\storage\JI6YK5A2\3219819.html}
}

@misc{oquabDINOv2LearningRobust2024,
  title = {{{DINOv2}}: {{Learning Robust Visual Features}} without {{Supervision}}},
  shorttitle = {{{DINOv2}}},
  author = {Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and {El-Nouby}, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Herv{\'e} and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  year = 2024,
  month = feb,
  number = {arXiv:2304.07193},
  eprint = {2304.07193},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.07193},
  urldate = {2024-03-13},
  abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\W5IPK2IX\\Oquab et al. - 2024 - DINOv2 Learning Robust Visual Features without Su.pdf;C\:\\Users\\E097600\\Zotero\\storage\\AKU2S33G\\2304.html}
}

@misc{OverviewActiveLearning2020,
  title = {Overview of {{Active Learning}} for {{Deep Learning}}},
  year = 2020,
  month = feb,
  journal = {Jacob Gildenblat},
  urldate = {2024-03-05},
  abstract = {Overview of different Active Learning algorithms for Deep Learning.},
  howpublished = {http://jacobgil.github.io/deeplearning/activelearning},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\GW7H98CS\activelearning.html}
}

@misc{panEnhancedSampleSelection2025,
  title = {Enhanced {{Sample Selection}} with {{Confidence Tracking}}: {{Identifying Correctly Labeled}} yet {{Hard-to-Learn Samples}} in {{Noisy Data}}},
  shorttitle = {Enhanced {{Sample Selection}} with {{Confidence Tracking}}},
  author = {Pan, Weiran and Wei, Wei and Zhu, Feida and Deng, Yong},
  year = 2025,
  month = apr,
  number = {arXiv:2504.17474},
  eprint = {2504.17474},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.17474},
  urldate = {2025-04-25},
  abstract = {We propose a novel sample selection method for image classification in the presence of noisy labels. Existing methods typically consider small-loss samples as correctly labeled. However, some correctly labeled samples are inherently difficult for the model to learn and can exhibit high loss similar to mislabeled samples in the early stages of training. Consequently, setting a threshold on per-sample loss to select correct labels results in a trade-off between precision and recall in sample selection: a lower threshold may miss many correctly labeled hard-to-learn samples (low recall), while a higher threshold may include many mislabeled samples (low precision). To address this issue, our goal is to accurately distinguish correctly labeled yet hard-to-learn samples from mislabeled ones, thus alleviating the trade-off dilemma. We achieve this by considering the trends in model prediction confidence rather than relying solely on loss values. Empirical observations show that only for correctly labeled samples, the model's prediction confidence for the annotated labels typically increases faster than for any other classes. Based on this insight, we propose tracking the confidence gaps between the annotated labels and other classes during training and evaluating their trends using the Mann-Kendall Test. A sample is considered potentially correctly labeled if all its confidence gaps tend to increase. Our method functions as a plug-and-play component that can be seamlessly integrated into existing sample selection techniques. Experiments on several standard benchmarks and real-world datasets demonstrate that our method enhances the performance of existing methods for learning with noisy labels.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Active sampling,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,label noise},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\5KPFS3GU\\Pan et al. - 2025 - Enhanced Sample Selection with Confidence Tracking Identifying Correctly Labeled yet Hard-to-Learn.pdf;C\:\\Users\\E097600\\Zotero\\storage\\JUSD6LB2\\2504.html}
}

@misc{pangWhenVLMsMeet2025,
  title = {When {{VLMs Meet Image Classification}}: {{Test Sets Renovation}} via {{Missing Label Identification}}},
  shorttitle = {When {{VLMs Meet Image Classification}}},
  author = {Pang, Zirui and Tan, Haosheng and Pu, Yuhan and Deng, Zhijie and Shen, Zhouan and Hu, Keyu and Wei, Jiaheng},
  year = 2025,
  month = may,
  journal = {arXiv.org},
  urldate = {2025-06-02},
  abstract = {Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet serve as critical tools for model evaluation. However, despite the cleaning efforts, these datasets still suffer from pervasive noisy labels and often contain missing labels due to the co-existing image pattern where multiple classes appear in an image sample. This results in misleading model comparisons and unfair evaluations. Existing label cleaning methods focus primarily on noisy labels, but the issue of missing labels remains largely overlooked. Motivated by these challenges, we present a comprehensive framework named REVEAL, integrating state-of-the-art pre-trained vision-language models (e.g., LLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods (e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and missing label detection in widely-used image classification test sets. REVEAL detects potential noisy labels and omissions, aggregates predictions from various methods, and refines label accuracy through confidence-informed predictions and consensus-based filtering. Additionally, we provide a thorough analysis of state-of-the-art vision-language models and pre-trained image classifiers, highlighting their strengths and limitations within the context of dataset renovation by revealing 10 observations. Our method effectively reveals missing labels from public datasets and provides soft-labeled results with likelihoods. Through human verifications, REVEAL significantly improves the quality of 6 benchmark test sets, highly aligning to human judgments and enabling more accurate and meaningful comparisons in image classification.},
  howpublished = {https://arxiv.org/abs/2505.16149v1},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\4Z9IS5DP\Pang et al. - 2025 - When VLMs Meet Image Classification Test Sets Renovation via Missing Label Identification.pdf}
}

@misc{panIDInitUniversalStable2025,
  title = {{{IDInit}}: {{A Universal}} and {{Stable Initialization Method}} for {{Neural Network Training}}},
  shorttitle = {{{IDInit}}},
  author = {Pan, Yu and Wang, Chaozheng and Wu, Zekai and Wang, Qifan and Zhang, Min and Xu, Zenglin},
  year = 2025,
  month = mar,
  number = {arXiv:2503.04626},
  eprint = {2503.04626},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.04626},
  urldate = {2025-04-17},
  abstract = {Deep neural networks have achieved remarkable accomplishments in practice. The success of these networks hinges on effective initialization methods, which are vital for ensuring stable and rapid convergence during training. Recently, initialization methods that maintain identity transition within layers have shown good efficiency in network training. These techniques (e.g., Fixup) set specific weights to zero to achieve identity control. However, settings of remaining weight (e.g., Fixup uses random values to initialize non-zero weights) will affect the inductive bias that is achieved only by a zero weight, which may be harmful to training. Addressing this concern, we introduce fully identical initialization (IDInit), a novel method that preserves identity in both the main and sub-stem layers of residual networks. IDInit employs a padded identity-like matrix to overcome rank constraints in non-square weight matrices. Furthermore, we show the convergence problem of an identity matrix can be solved by stochastic gradient descent. Additionally, we enhance the universality of IDInit by processing higher-order weights and addressing dead neuron problems. IDInit is a straightforward yet effective initialization method, with improved convergence, stability, and performance across various settings, including large-scale datasets and deep models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Deep learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\FM6V3HEP\\Pan et al. - 2025 - IDInit A Universal and Stable Initialization Method for Neural Network Training.pdf;C\:\\Users\\E097600\\Zotero\\storage\\R7DSKR9L\\2503.html}
}

@misc{PDFLowBudget,
  title = {({{PDF}}) {{Low Budget Active Learning}} via {{Wasserstein Distance}}: {{An Integer Programming Approach}}},
  shorttitle = {({{PDF}}) {{Low Budget Active Learning}} via {{Wasserstein Distance}}},
  journal = {ResearchGate},
  doi = {10.48550/arXiv.2106.02968},
  urldate = {2025-09-16},
  abstract = {PDF \textbar{} Given restrictions on the availability of data, active learning is the process of training a model with limited labeled data by selecting a core... \textbar{} Find, read and cite all the research you need on ResearchGate},
  howpublished = {https://www.researchgate.net/publication/352209624\_Low\_Budget\_Active\_Learning\_via\_Wasserstein\_Distance\_An\_Integer\_Programming\_Approach},
  langid = {english},
  keywords = {Active learning,Distribution shift,domain adaptation,exploration,Image classification,ssl,Wasserstein distance},
  file = {C:\Users\E097600\Zotero\storage\DTM4AW3I\352209624_Low_Budget_Active_Learning_via_Wasserstein_Distance_An_Integer_Programming_Approach.html}
}

@article{pengReducingAnnotatingLoad2024,
  title = {Reducing Annotating Load: {{Active}} Learning with Synthetic Images in Surgical Instrument Segmentation},
  shorttitle = {Reducing Annotating Load},
  author = {Peng, Haonan and Lin, Shan and King, Daniel and Su, Yun-Hsuan and Abuzeid, Waleed M. and Bly, Randall A. and Moe, Kris S. and Hannaford, Blake},
  year = 2024,
  month = oct,
  journal = {Medical Image Analysis},
  volume = {97},
  pages = {103246},
  issn = {1361-8415},
  doi = {10.1016/j.media.2024.103246},
  urldate = {2025-11-18},
  abstract = {Accurate instrument segmentation in the endoscopic vision of minimally invasive surgery is challenging due to complex instruments and environments. Deep learning techniques have shown competitive performance in recent years. However, deep learning usually requires a large amount of labeled data to achieve accurate prediction, which poses a significant workload. To alleviate this workload, we propose an active learning-based framework to generate synthetic images for efficient neural network training. In each active learning iteration, a small number of informative unlabeled images are first queried by active learning and manually labeled. Next, synthetic images are generated based on these selected images. The instruments and backgrounds are cropped out and randomly combined with blending and fusion near the boundary. The proposed method leverages the advantage of both active learning and synthetic images. The effectiveness of the proposed method is validated on two sinus surgery datasets and one intraabdominal surgery dataset. The results indicate a considerable performance improvement, especially when the size of the annotated dataset is small. All the code is open-sourced at: https://github.com/HaonanPeng/active\_syn\_generator.},
  keywords = {Active learning,image generation,Instance segmentation,Medical image synthesis,Robot instrument segmentation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\HBXI6HJY\\Peng et al. - 2024 - Reducing annotating load Active learning with synthetic images in surgical instrument segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\LVPN85E8\\S1361841524001713.html}
}

@misc{phamPCAAEPrincipalComponent2020,
  title = {{{PCAAE}}: {{Principal Component Analysis Autoencoder}} for Organising the Latent Space of Generative Networks},
  shorttitle = {{{PCAAE}}},
  author = {Pham, Chi-Hieu and Ladjal, Sa{\"i}d and Newson, Alasdair},
  year = 2020,
  month = jun,
  number = {arXiv:2006.07827},
  eprint = {2006.07827},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.07827},
  urldate = {2024-03-04},
  abstract = {Autoencoders and generative models produce some of the most spectacular deep learning results to date. However, understanding and controlling the latent space of these models presents a considerable challenge. Drawing inspiration from principal component analysis and autoencoder, we propose the Principal Component Analysis Autoencoder (PCAAE). This is a novel autoencoder whose latent space verifies two properties. Firstly, the dimensions are organised in decreasing importance with respect to the data at hand. Secondly, the components of the latent space are statistically independent. We achieve this by progressively increasing the latent space during training, and with a covariance loss applied to the latent codes. The resulting autoencoder produces a latent space which separates the intrinsic attributes of the data into different components of the latent space, in a completely unsupervised manner. We also describe an extension of our approach to the case of powerful, pre-trained GANs. We show results on both synthetic examples of shapes and on a state-of-the-art GAN. For example, we are able to separate the color shade scale of hair and skin, pose of faces and the gender in the CelebA, without accessing any labels. We compare the PCAAE with other state-of-the-art approaches, in particular with respect to the ability to disentangle attributes in the latent space. We hope that this approach will contribute to better understanding of the intrinsic latent spaces of powerful deep generative models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\4UPEF287\\Pham et al. - 2020 - PCAAE Principal Component Analysis Autoencoder fo.pdf;C\:\\Users\\E097600\\Zotero\\storage\\Y7A2SDSN\\2006.html}
}

@misc{philipsenDistanceLatentSpace2020,
  title = {Distance in {{Latent Space}} as {{Novelty Measure}}},
  author = {Philipsen, Mark Philip and Moeslund, Thomas Baltzer},
  year = 2020,
  month = mar,
  number = {arXiv:2003.14043},
  eprint = {2003.14043},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.14043},
  urldate = {2024-03-05},
  abstract = {Deep Learning performs well when training data densely covers the experience space. For complex problems this makes data collection prohibitively expensive. We propose to intelligently select samples when constructing data sets in order to best utilize the available labeling budget. The selection methodology is based on the presumption that two dissimilar samples are worth more than two similar samples in a data set. Similarity is measured based on the Euclidean distance between samples in the latent space produced by a DNN. By using a self-supervised method to construct the latent space, it is ensured that the space fits the data well and that any upfront labeling effort can be avoided. The result is more efficient, diverse, and balanced data set, which produce equal or superior results with fewer labeled examples.},
  archiveprefix = {arXiv},
  keywords = {agnostique,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\P5CLX5EQ\\Philipsen et Moeslund - 2020 - Distance in Latent Space as Novelty Measure.pdf;C\:\\Users\\E097600\\Zotero\\storage\\TVJK9MG3\\2003.html}
}

@misc{pogorzelskiEnhancingActiveLearning2024,
  title = {Enhancing {{Active Learning}} for {{Sentinel}} 2 {{Imagery}} through {{Contrastive Learning}} and {{Uncertainty Estimation}}},
  author = {Pogorzelski, David and Arlinghaus, Peter and Zhang, Wenyan},
  year = 2024,
  month = jun,
  number = {arXiv:2405.13285},
  eprint = {2405.13285},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.13285},
  urldate = {2025-01-16},
  abstract = {In this paper, we introduce a novel method designed to enhance label efficiency in satellite imagery analysis by integrating semi-supervised learning (SSL) with active learning strategies. Our approach utilizes contrastive learning together with uncertainty estimations via Monte Carlo Dropout (MC Dropout), with a particular focus on Sentinel-2 imagery analyzed using the Eurosat dataset. We explore the effectiveness of our method in scenarios featuring both balanced and unbalanced class distributions. Our results show that the proposed method performs better than several other popular methods in this field, enabling significant savings in labeling effort while maintaining high classification accuracy. These findings highlight the potential of our approach to facilitate scalable and cost-effective satellite image analysis, particularly advantageous for extensive environmental monitoring and land use classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\KZIK7TBW\\Pogorzelski et al. - 2024 - Enhancing Active Learning for Sentinel 2 Imagery through Contrastive Learning and Uncertainty Estima.pdf;C\:\\Users\\E097600\\Zotero\\storage\\GIDNMCC8\\2405.html}
}

@article{pooleActiveTransferLearning2025,
  title = {Active Transfer Learning for Structural Health Monitoring},
  author = {Poole, J. and Dervilis, N. and Worden, K. and Gardner, P. and Giglioni, V. and Mills, R. S. and Hughes, A. J.},
  year = 2025,
  month = dec,
  journal = {Mechanical Systems and Signal Processing},
  volume = {241},
  eprint = {2510.27525},
  primaryclass = {cs},
  pages = {113260},
  issn = {08883270},
  doi = {10.1016/j.ymssp.2025.113260},
  urldate = {2025-11-03},
  abstract = {Data for training structural health monitoring (SHM) systems are often expensive and/or impractical to obtain, particularly for labelled data. Population-based SHM (PBSHM) aims to address this limitation by leveraging data from multiple structures. However, data from different structures will follow distinct distributions, potentially leading to large generalisation errors for models learnt via conventional machine learning methods. To address this issue, transfer learning -- in the form of domain adaptation (DA) -- can be used to align the data distributions. Most previous approaches have only considered \textbackslash emph\textbraceleft unsupervised\textbraceright{} DA, where no labelled target data are available; they do not consider how to incorporate these technologies in an online framework -- updating as labels are obtained throughout the monitoring campaign. This paper proposes a Bayesian framework for DA in PBSHM, that can improve unsupervised DA mappings using a limited quantity of labelled target data. In addition, this model is integrated into an active sampling strategy to guide inspections to select the most informative observations to label -- leading to further reductions in the required labelled data to learn a target classifier. The effectiveness of this methodology is evaluated on a population of experimental bridges. Specifically, this population includes data corresponding to several damage states, as well as, a comprehensive set of environmental conditions. It is found that combining transfer learning and active learning can improve data efficiency when learning classification models in label-scarce scenarios. This result has implications for data-informed operation and maintenance of structures, suggesting a reduction in inspections over the operational lifetime of a structure -- and therefore a reduction in operational costs -- can be achieved.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,domain adaptation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\9LJDSDZU\\Poole et al. - 2025 - Active transfer learning for structural health monitoring.pdf;C\:\\Users\\E097600\\Zotero\\storage\\XR3SI7ID\\2510.html}
}

@misc{popDeepEnsembleBayesian2018,
  title = {Deep {{Ensemble Bayesian Active Learning}} : {{Addressing}} the {{Mode Collapse}} Issue in {{Monte Carlo}} Dropout via {{Ensembles}}},
  shorttitle = {Deep {{Ensemble Bayesian Active Learning}}},
  author = {Pop, Remus and Fulop, Patric},
  year = 2018,
  month = nov,
  number = {arXiv:1811.03897},
  eprint = {1811.03897},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.03897},
  urldate = {2025-09-17},
  abstract = {In image classification tasks, the ability of deep CNNs to deal with complex image data has proven to be unrivalled. However, they require large amounts of labeled training data to reach their full potential. In specialised domains such as healthcare, labeled data can be difficult and expensive to obtain. Active Learning aims to alleviate this problem, by reducing the amount of labelled data needed for a specific task while delivering satisfactory performance. We propose DEBAL, a new active learning strategy designed for deep neural networks. This method improves upon the current state-of-the-art deep Bayesian active learning method, which suffers from the mode collapse problem. We correct for this deficiency by making use of the expressive power and statistical properties of model ensembles. Our proposed method manages to capture superior data uncertainty, which translates into improved classification performance. We demonstrate empirically that our ensemble method yields faster convergence of CNNs trained on the MNIST and CIFAR-10 datasets.},
  archiveprefix = {arXiv},
  keywords = {Active learning,bayesian,cifar10,Class imbalance,Computer Science - Machine Learning,Image classification,mnist,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\VDR5BZHA\\Pop et Fulop - 2018 - Deep Ensemble Bayesian Active Learning  Addressing the Mode Collapse issue in Monte Carlo dropout v.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ZW49WFTB\\1811.html}
}

@misc{principatoConformalPredictionHierarchical2025,
  title = {Conformal {{Prediction}} for {{Hierarchical Data}}},
  author = {Principato, Guillaume and Stoltz, Gilles and {Amara-Ouali}, Yvenn and Goude, Yannig and Hamrouche, Bachir and Poggi, Jean-Michel},
  year = 2025,
  month = oct,
  number = {arXiv:2411.13479},
  eprint = {2411.13479},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.13479},
  urldate = {2025-12-05},
  abstract = {We consider conformal prediction for multivariate data and focus on hierarchical data, where some components are linear combinations of others. Intuitively, the hierarchical structure can be leveraged to reduce the size of prediction regions for the same coverage level. We implement this intuition by including a projection step (also called a reconciliation step) in the split conformal prediction [SCP] procedure, and prove that the resulting prediction regions are indeed globally smaller. We do so both under the classic objective of joint coverage and under a new and challenging task: component-wise coverage, for which efficiency results are more difficult to obtain. The associated strategies and their analyses are based both on the literature of SCP and of forecast reconciliation, which we connect. We also illustrate the theoretical findings, for different scales of hierarchies on simulated data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Conformal prediction,Statistics - Applications,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\98Z9MTFK\\Principato et al. - 2025 - Conformal Prediction for Hierarchical Data.pdf;C\:\\Users\\E097600\\Zotero\\storage\\U9M4MRPB\\2411.html}
}

@misc{pruthiEstimatingTrainingData2020,
  title = {Estimating {{Training Data Influence}} by {{Tracing Gradient Descent}}},
  author = {Pruthi, Garima and Liu, Frederick and Sundararajan, Mukund and Kale, Satyen},
  year = 2020,
  month = nov,
  number = {arXiv:2002.08484},
  eprint = {2002.08484},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.08484},
  urldate = {2025-03-26},
  abstract = {We introduce a method called TracIn that computes the influence of a training example on a prediction made by the model. The idea is to trace how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TracIn via: (a) a first-order gradient approximation to the exact computation, (b) saved checkpoints of standard training procedures, and (c) cherry-picking layers of a deep neural network. In contrast with previously proposed methods, TracIn is simple to implement; all it needs is the ability to work with gradients, checkpoints, and loss functions. The method is general. It applies to any machine learning model trained using stochastic gradient descent or a variant of it, agnostic of architecture, domain and task. We expect the method to be widely useful within processes that study and improve training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\FSTV3MYE\\Pruthi et al. - 2020 - Estimating Training Data Influence by Tracing Gradient Descent.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ABNQTUZ2\\2002.html}
}

@inproceedings{pulgarImpactImbalancedData2017,
  title = {On the {{Impact}} of {{Imbalanced Data}} in~{{Convolutional Neural Networks Performance}}},
  booktitle = {Hybrid {{Artificial Intelligent Systems}}},
  author = {Pulgar, Francisco J. and Rivera, Antonio J. and Charte, Francisco and {del Jesus}, Mar{\'i}a J.},
  editor = {{Mart{\'i}nez de Pis{\'o}n}, Francisco Javier and Urraca, Rub{\'e}n and Quinti{\'a}n, H{\'e}ctor and Corchado, Emilio},
  year = 2017,
  pages = {220--232},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-59650-1_19},
  abstract = {In recent years, new proposals have emerged for tackling the classification problem based on Deep Learning (DL) techniques. These proposals have shown good results in certain fields, such as image recognition. However, there are factors that must be analyzed to determine how they influence the results obtained by these new algorithms. In this paper, the classification of imbalanced data with convolutional neural networks (CNNs) is analyzed. To do this, a series of tests will be performed in which the classification of real images of traffic signals by CNNs will be performed based on data with different imbalance levels.},
  isbn = {978-3-319-59650-1},
  langid = {english},
  keywords = {Class imbalance}
}

@misc{qinInfoCoevolutionEfficientFramework2025,
  title = {Info-{{Coevolution}}: {{An Efficient Framework}} for {{Data Model Coevolution}}},
  shorttitle = {Info-{{Coevolution}}},
  author = {Qin, Ziheng and Xu, Hailun and Yew, Wei Chee and Jia, Qi and Luo, Yang and Sarkar, Kanchan and Guan, Danhui and Wang, Kai and You, Yang},
  year = 2025,
  month = jun,
  journal = {arXiv.org},
  urldate = {2025-06-11},
  abstract = {Machine learning relies heavily on data, yet the continuous growth of real-world data poses challenges for efficient dataset construction and training. A fundamental yet unsolved question is: given our current model and data, does a new data (sample/batch) need annotation/learning? Conventional approaches retain all available data, leading to non-optimal data and training efficiency. Active learning aims to reduce data redundancy by selecting a subset of samples to annotate, while it increases pipeline complexity and introduces bias. In this work, we propose Info-Coevolution, a novel framework that efficiently enables models and data to coevolve through online selective annotation with no bias. Leveraging task-specific models (and open-source models), it selectively annotates and integrates online and web data to improve datasets efficiently. For real-world datasets like ImageNet-1K, Info-Coevolution reduces annotation and training costs by 32\textbackslash\% without performance loss. It is able to automatically give the saving ratio without tuning the ratio. It can further reduce the annotation ratio to 50\textbackslash\% with semi-supervised learning. We also explore retrieval-based dataset enhancement using unlabeled open-source data. Code is available at https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.},
  howpublished = {https://arxiv.org/abs/2506.08070v1},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\FW7TVCHK\Qin et al. - 2025 - Info-Coevolution An Efficient Framework for Data Model Coevolution.pdf}
}

@misc{quetinShouldWePretrain2025,
  title = {Should We Pre-Train a Decoder in Contrastive Learning for Dense Prediction Tasks?},
  author = {Quetin, S{\'e}bastien and Ghosh, Tapotosh and Maleki, Farhad},
  year = 2025,
  month = mar,
  number = {arXiv:2503.17526},
  eprint = {2503.17526},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.17526},
  urldate = {2025-03-25},
  abstract = {Contrastive learning in self-supervised settings primarily focuses on pre-training encoders, while decoders are typically introduced and trained separately for downstream dense prediction tasks. This conventional approach, however, overlooks the potential benefits of jointly pre-training both the encoder and decoder. In this paper, we propose DeCon: a framework-agnostic adaptation to convert an encoder-only self-supervised learning (SSL) contrastive approach to an efficient encoder-decoder framework that can be pre-trained in a contrastive manner. We first update the existing architecture to accommodate a decoder and its respective contrastive loss. We then introduce a weighted encoder-decoder contrastive loss with non-competing objectives that facilitates the joint encoder-decoder architecture pre-training. We adapt two established contrastive SSL frameworks tailored for dense prediction tasks, achieve new state-of-the-art results in COCO object detection and instance segmentation, and match state-of-the-art performance on Pascal VOC semantic segmentation. We show that our approach allows for pre-training a decoder and enhances the representation power of the encoder and its performance in dense prediction tasks. This benefit holds across heterogeneous decoder architectures between pre-training and fine-tuning and persists in out-of-domain, limited-data scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\YTZK3DSD\\Quetin et al. - 2025 - Should we pre-train a decoder in contrastive learning for dense prediction tasks.pdf;C\:\\Users\\E097600\\Zotero\\storage\\HUS8VL9I\\2503.html}
}

@misc{quPredictionIntervalsModel2025,
  title = {Prediction {{Intervals}} for {{Model Averaging}}},
  author = {Qu, Zhongjun and Wang, Wendun and Zhang, Xiaomeng},
  year = 2025,
  month = oct,
  number = {arXiv:2510.16224},
  eprint = {2510.16224},
  primaryclass = {econ},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.16224},
  urldate = {2025-10-23},
  abstract = {A rich set of frequentist model averaging methods has been developed, but their applications have largely been limited to point prediction, as measuring prediction uncertainty in general settings remains an open problem. In this paper we propose prediction intervals for model averaging based on conformal inference. These intervals cover out-of-sample realizations of the outcome variable with a pre-specified probability, providing a way to assess predictive uncertainty beyond point prediction. The framework allows general model misspecification and applies to averaging across multiple models that can be nested, disjoint, overlapping, or any combination thereof, with weights that may depend on the estimation sample. We establish coverage guarantees under two sets of assumptions: exact finite-sample validity under exchangeability, relevant for cross-sectional data, and asymptotic validity under stationarity, relevant for time-series data. We first present a benchmark algorithm and then introduce a locally adaptive refinement and split-sample procedures that broaden applicability. The methods are illustrated with a cross-sectional application to real estate appraisal and a time-series application to equity premium forecasting.},
  archiveprefix = {arXiv},
  keywords = {Conformal prediction,Economics - Econometrics,ensemble methods},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\S9XJLNK3\\Qu et al. - 2025 - Prediction Intervals for Model Averaging.pdf;C\:\\Users\\E097600\\Zotero\\storage\\WLAV8SZT\\2510.html}
}

@article{rachnareddyEvaluatingFusionTeachingLearning,
  title = {Evaluating a {{Fusion Teaching-Learning Method}} for {{Enhancing Comprehension}}, {{Retention}}, and {{Clinical Application}} of {{ENT Surgical Anatomy}} in {{MBBS Students}}},
  author = {Rachna Reddy, Baddam and Abhinav Kiran, Gudise and Vadla, Shruthi and Vangoori, Yakaiah},
  journal = {Cureus},
  volume = {17},
  number = {7},
  pages = {e88220},
  issn = {2168-8184},
  doi = {10.7759/cureus.88220},
  urldate = {2025-11-18},
  abstract = {Background: Traditional methods of anatomy instruction often fall short in promoting clinically relevant, long-term understanding, particularly in surgical domains such as otorhinolaryngology (ENT), which require high-level spatial reasoning and procedural competence. This study aimed to evaluate the effectiveness of an innovative, multimodal Fusion Teaching-Learning Method (TLM), conceptualised as ``From Pixels to Scalpels,'' that integrates digital visualisation, hands-on engagement, and collaborative case-based tasks in enhancing the comprehension, retention, and clinical application of ENT surgical anatomy among MBBS students., Methods: A prospective, crossover observational study was conducted involving 60 Phase III Part I MBBS students at a tertiary teaching institution. Participants experienced both traditional and Fusion TLM interventions across two ENT topics, with outcomes assessed through pre-, post-, and two-week retention multiple-choice question (MCQ) tests, rubric-based team assessments, and structured perception surveys. Statistical analysis included paired and unpaired t-tests for knowledge comparisons and descriptive analysis for student feedback., Results: The Fusion TLM significantly improved post-test scores (p = 0.007 and 0.004), rubric-based application scores (p = 0.003 and 0.002), and knowledge retention (p = 0.003 and 0.001) compared to traditional methods. Over 90\% of students reported enhanced engagement, conceptual clarity, and confidence, with 75\% affirming improved self-efficacy and clinical reasoning abilities., Conclusion: The ``From Pixels to Scalpels'' model demonstrated clear pedagogical superiority, combining cognitive, psychomotor, and affective domains to foster deeper learning and clinical relevance. Its scalability, learner-centred design, and alignment with Competency-Based Medical Education (CBME) make it a promising strategy for anatomical education in resource-constrained settings.},
  pmcid = {PMC12358052},
  pmid = {40827156},
  file = {C:\Users\E097600\Zotero\storage\5QFEKIJK\Rachna Reddy et al. - Evaluating a Fusion Teaching-Learning Method for Enhancing Comprehension, Retention, and Clinical Ap.pdf}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = 2021,
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.00020},
  urldate = {2025-05-27},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\Y7DU3QHB\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf;C\:\\Users\\E097600\\Zotero\\storage\\75RG78BM\\2103.html}
}

@misc{rahmanDeepLearningDrivenSegmentation2025,
  title = {Deep {{Learning-Driven Segmentation}} of {{Ischemic Stroke Lesions Using Multi-Channel MRI}}},
  author = {Rahman, Ashiqur and Chowdhury, Muhammad E. H. and Wadud, Md Sharjis Ibne and Sarmun, Rusab and Mushtak, Adam and Zoghoul, Sohaib Bassam and {Al-Hashimi}, Israa},
  year = 2025,
  month = jan,
  number = {arXiv:2501.02287},
  eprint = {2501.02287},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.02287},
  urldate = {2025-01-17},
  abstract = {Ischemic stroke, caused by cerebral vessel occlusion, presents substantial challenges in medical imaging due to the variability and subtlety of stroke lesions. Magnetic Resonance Imaging (MRI) plays a crucial role in diagnosing and managing ischemic stroke, yet existing segmentation techniques often fail to accurately delineate lesions. This study introduces a novel deep learning-based method for segmenting ischemic stroke lesions using multi-channel MRI modalities, including Diffusion Weighted Imaging (DWI), Apparent Diffusion Coefficient (ADC), and enhanced Diffusion Weighted Imaging (eDWI). The proposed architecture integrates DenseNet121 as the encoder with Self-Organized Operational Neural Networks (SelfONN) in the decoder, enhanced by Channel and Space Compound Attention (CSCA) and Double Squeeze-and-Excitation (DSE) blocks. Additionally, a custom loss function combining Dice Loss and Jaccard Loss with weighted averages is introduced to improve model performance. Trained and evaluated on the ISLES 2022 dataset, the model achieved Dice Similarity Coefficients (DSC) of 83.88\% using DWI alone, 85.86\% with DWI and ADC, and 87.49\% with the integration of DWI, ADC, and eDWI. This approach not only outperforms existing methods but also addresses key limitations in current segmentation practices. These advancements significantly enhance diagnostic precision and treatment planning for ischemic stroke, providing valuable support for clinical decision-making.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\C6JZAGX3\\Rahman et al. - 2025 - Deep Learning-Driven Segmentation of Ischemic Stroke Lesions Using Multi-Channel MRI.pdf;C\:\\Users\\E097600\\Zotero\\storage\\2YM5YQJM\\2501.html}
}

@inproceedings{rajEffectiveClassificationImbalanced2016,
  title = {Towards {{Effective Classification}} of {{Imbalanced Data}} with {{Convolutional Neural Networks}}},
  booktitle = {Artificial {{Neural Networks}} in {{Pattern Recognition}}},
  author = {Raj, Vidwath and Magg, Sven and Wermter, Stefan},
  editor = {Schwenker, Friedhelm and Abbas, Hazem M. and El Gayar, Neamat and Trentin, Edmondo},
  year = 2016,
  pages = {150--162},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-46182-3_13},
  abstract = {Class imbalance in machine learning is a problem often found with real-world data, where data from one class clearly dominates the dataset. Most neural network classifiers fail to learn to classify such datasets correctly if class-to-class separability is poor due to a strong bias towards the majority class. In this paper we present an algorithmic solution, integrating different methods into a novel approach using a class-to-class separability score, to increase performance on poorly separable, imbalanced datasets using Cost Sensitive Neural Networks. We compare different cost functions and methods that can be used for training Convolutional Neural Networks on a highly imbalanced dataset of multi-channel time series data. Results show that, despite being imbalanced and poorly separable, performance metrics such as G-Mean as high as 92.8~\% could be reached by using cost sensitive Convolutional Neural Networks to detect patterns and correctly classify time series from 3 different datasets.},
  isbn = {978-3-319-46182-3},
  langid = {english},
  keywords = {Convolutional Neural Network,Imbalance Ratio,Imbalanced Dataset,Minority Class,Real World Dataset},
  file = {C:\Users\E097600\Zotero\storage\YPBSY3AB\Raj et al. - 2016 - Towards Effective Classification of Imbalanced Data with Convolutional Neural Networks.pdf}
}

@misc{rauchNoFreeLunch2025,
  title = {No {{Free Lunch}} in {{Active Learning}}: {{LLM Embedding Quality Dictates Query Strategy Success}}},
  shorttitle = {No {{Free Lunch}} in {{Active Learning}}},
  author = {Rauch, Lukas and Wirth, Moritz and Huseljic, Denis and Herde, Marek and Sick, Bernhard and A{\ss}enmacher, Matthias},
  year = 2025,
  month = may,
  journal = {arXiv.org},
  urldate = {2025-06-10},
  abstract = {The advent of large language models (LLMs) capable of producing general-purpose representations lets us revisit the practicality of deep active learning (AL): By leveraging frozen LLM embeddings, we can mitigate the computational costs of iteratively fine-tuning large backbones. This study establishes a benchmark and systematically investigates the influence of LLM embedding quality on query strategies in deep AL. We employ five top-performing models from the massive text embedding benchmark (MTEB) leaderboard and two baselines for ten diverse text classification tasks. Our findings reveal key insights: First, initializing the labeled pool using diversity-based sampling synergizes with high-quality embeddings, boosting performance in early AL iterations. Second, the choice of the optimal query strategy is sensitive to embedding quality. While the computationally inexpensive Margin sampling can achieve performance spikes on specific datasets, we find that strategies like Badge exhibit greater robustness across tasks. Importantly, their effectiveness is often enhanced when paired with higher-quality embeddings. Our results emphasize the need for context-specific evaluation of AL strategies, as performance heavily depends on embedding quality and the target task.},
  howpublished = {https://arxiv.org/abs/2506.01992v1},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\D4QYLX2V\Rauch et al. - 2025 - No Free Lunch in Active Learning LLM Embedding Quality Dictates Query Strategy Success.pdf}
}

@article{rawatHowUsefulImageBased2022,
  title = {How {{Useful Is Image-Based Active Learning}} for {{Plant Organ Segmentation}}?},
  author = {Rawat, Shivangana and Chandra, Akshay L. and Desai, Sai Vikas and Balasubramanian, Vineeth N. and Ninomiya, Seishi and Guo, Wei},
  year = 2022,
  month = feb,
  journal = {Plant Phenomics},
  volume = {2022},
  pages = {9795275},
  issn = {2643-6515},
  doi = {10.34133/2022/9795275},
  urldate = {2024-08-23},
  abstract = {Training deep learning models typically requires a huge amount of labeled data which is expensive to acquire, especially in dense prediction tasks such as semantic segmentation. Moreover, plant phenotyping datasets pose additional challenges of heavy occlusion and varied lighting conditions which makes annotations more time-consuming to obtain. Active learning helps in reducing the annotation cost by selecting samples for labeling which are most informative to the model, thus improving model performance with fewer annotations. Active learning for semantic segmentation has been well studied on datasets such as PASCAL VOC and Cityscapes. However, its effectiveness on plant datasets has not received much importance. To bridge this gap, we empirically study and benchmark the effectiveness of four uncertainty-based active learning strategies on three natural plant organ segmentation datasets. We also study their behaviour in response to variations in training configurations in terms of augmentations used, the scale of training images, active learning batch sizes, and train-validation set splits.},
  pmcid = {PMC8897744},
  pmid = {35280929},
  file = {C:\Users\E097600\Zotero\storage\FYJTAIWJ\Rawat et al. - 2022 - How Useful Is Image-Based Active Learning for Plan.pdf}
}

@misc{renFasterRCNNRealTime2016,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = 2016,
  month = jan,
  number = {arXiv:1506.01497},
  eprint = {1506.01497},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.01497},
  urldate = {2024-12-18},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,modele},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\FJAIFRBT\\Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf;C\:\\Users\\E097600\\Zotero\\storage\\PLA36KSQ\\1506.html}
}

@article{renGenericDeepLearningBasedApproach2018,
  title = {A {{Generic Deep-Learning-Based Approach}} for {{Automated Surface Inspection}}},
  author = {Ren, Ruoxu and Hung, Terence and Tan, Kay Chen},
  year = 2018,
  month = mar,
  journal = {IEEE transactions on cybernetics},
  volume = {48},
  number = {3},
  pages = {929--940},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2017.2668395},
  abstract = {Automated surface inspection (ASI) is a challenging task in industry, as collecting training dataset is usually costly and related methods are highly dataset-dependent. In this paper, a generic approach that requires small training data for ASI is proposed. First, this approach builds classifier on the features of image patches, where the features are transferred from a pretrained deep learning network. Next, pixel-wise prediction is obtained by convolving the trained classifier over input image. An experiment on three public and one industrial data set is carried out. The experiment involves two tasks: 1) image classification and 2) defect segmentation. The results of proposed algorithm are compared against several best benchmarks in literature. In the classification tasks, the proposed method improves accuracy by 0.66\%-25.50\%. In the segmentation tasks, the proposed method reduces error escape rates by 6.00\%-19.00\% in three defect types and improves accuracies by 2.29\%-9.86\% in all seven defect types. In addition, the proposed method achieves 0.0\% error escape rate in the segmentation task of industrial data.},
  langid = {english},
  pmid = {28252414}
}

@misc{renSurveyDeepActive2021,
  title = {A {{Survey}} of {{Deep Active Learning}}},
  author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
  year = 2021,
  month = dec,
  number = {arXiv:2009.00236},
  eprint = {2009.00236},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.00236},
  urldate = {2024-03-05},
  abstract = {Active learning (AL) attempts to maximize the performance gain of the model by marking the fewest samples. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize massive parameters, so that the model learns how to extract high-quality features. In recent years, due to the rapid development of internet technology, we are in an era of information torrents and we have massive amounts of data. In this way, DL has aroused strong interest of researchers and has been rapidly developed. Compared with DL, researchers have relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples. Therefore, early AL is difficult to reflect the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to the publicity of the large number of existing annotation datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, which is not allowed in some fields that require high expertise, especially in the fields of speech recognition, information extraction, medical images, etc. Therefore, AL has gradually received due attention. A natural idea is whether AL can be used to reduce the cost of sample annotations, while retaining the powerful learning capabilities of DL. Therefore, deep active learning (DAL) has emerged. Although the related research has been quite abundant, it lacks a comprehensive survey of DAL. This article is to fill this gap, we provide a formal classification method for the existing work, and a comprehensive and systematic overview. In addition, we also analyzed and summarized the development of DAL from the perspective of application. Finally, we discussed the confusion and problems in DAL, and gave some possible development directions for DAL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\4SKS9GF8\\Ren et al. - 2021 - A Survey of Deep Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\A3T87BHA\\2009.html}
}

@misc{riveraInconsistencybasedActiveLearning2025,
  title = {Inconsistency-Based {{Active Learning}} for {{LiDAR Object Detection}}},
  author = {Rivera, Esteban and Stratil, Loic and Lienkamp, Markus},
  year = 2025,
  month = may,
  number = {arXiv:2505.00511},
  eprint = {2505.00511},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.00511},
  urldate = {2025-05-02},
  abstract = {Deep learning models for object detection in autonomous driving have recently achieved impressive performance gains and are already being deployed in vehicles worldwide. However, current models require increasingly large datasets for training. Acquiring and labeling such data is costly, necessitating the development of new strategies to optimize this process. Active learning is a promising approach that has been extensively researched in the image domain. In our work, we extend this concept to the LiDAR domain by developing several inconsistency-based sample selection strategies and evaluate their effectiveness in various settings. Our results show that using a naive inconsistency approach based on the number of detected boxes, we achieve the same mAP as the random sampling strategy with 50\% of the labeled data.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Computer Vision and Pattern Recognition,Object detection},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\VQZNYJGS\\Rivera et al. - 2025 - Inconsistency-based Active Learning for LiDAR Object Detection.pdf;C\:\\Users\\E097600\\Zotero\\storage\\HLN7J2N9\\2505.html}
}

@article{rolnickdavidTacklingClimateChange2022,
  title = {Tackling {{Climate Change}} with {{Machine Learning}}},
  author = {RolnickDavid and L, DontiPriya and H, KaackLynn and KochanskiKelly and LacosteAlexandre and SankaranKris and Slavin, RossAndrew and {Milojevic-DupontNikola} and JaquesNatasha and {Waldman-BrownAnna} and Sasha, LuccioniAlexandra and MaharajTegan and D, SherwinEvan and Karthik, MukkavilliS and P, KordingKonrad and P, GomesCarla and Y, NgAndrew and HassabisDemis and C, PlattJohn and CreutzigFelix and ChayesJennifer and BengioYoshua},
  year = 2022,
  month = feb,
  journal = {ACM Computing Surveys (CSUR)},
  publisher = {ACMPUB27New York, NY},
  doi = {10.1145/3485128},
  urldate = {2025-11-27},
  abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning (ML) experts, may wonder how we can help. Here we describe how ML can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a ...},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\DXHPS9YM\3485128.html}
}

@inproceedings{romanoClassificationValidAdaptive2020,
  title = {Classification with Valid and Adaptive Coverage},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Romano, Yaniv and Sesia, Matteo and Cand{\`e}s, Emmanuel J.},
  year = 2020,
  month = dec,
  series = {{{NIPS}} '20},
  pages = {3581--3591},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2025-11-05},
  abstract = {Conformal inference, cross-validation+, and the jackknife+ are hold-out methods that can be combined with virtually any machine learning algorithm to construct prediction sets with guaranteed marginal coverage. In this paper, we develop specialized versions of these techniques for categorical and unordered response labels that, in addition to providing marginal coverage, are also fully adaptive to complex data distributions, in the sense that they perform favorably in terms of approximate conditional coverage compared to alternative methods. The heart of our contribution is a novel conformity score, which we explicitly demonstrate to be powerful and intuitive for classification problems, but whose underlying principle is potentially far more general. Experiments on synthetic and real data demonstrate the practical value of our theoretical guarantees, as well as the statistical advantages of the proposed methods over the existing alternatives.},
  isbn = {978-1-7138-2954-6},
  keywords = {Conformal prediction,domain adaptation,Image classification},
  file = {C:\Users\E097600\Zotero\storage\RIMVNZBW\Romano et al. - 2020 - Classification with valid and adaptive coverage.pdf}
}

@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = 2015,
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1505.04597},
  urldate = {2025-01-24},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\7WUBVNC7\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\SUMB86L4\\1505.html}
}

@misc{rubDRIPDRopUnImportant2025,
  title = {{{DRIP}}: {{DRop unImportant}} Data {{Points}} -- {{Enhancing Machine Learning Efficiency}} with {{Grad-CAM-Based Real-Time Data Prioritization}} for {{On-Device Training}}},
  shorttitle = {{{DRIP}}},
  author = {R{\"u}b, Marcus and Konegen, Daniel and Sikora, Axel and {Mueller-Gritschneder}, Daniel},
  year = 2025,
  month = apr,
  journal = {arXiv.org},
  urldate = {2025-04-14},
  abstract = {Selecting data points for model training is critical in machine learning. Effective selection methods can reduce the labeling effort, optimize on-device training for embedded systems with limited data storage, and enhance the model performance. This paper introduces a novel algorithm that uses Grad-CAM to make online decisions about retaining or discarding data points. Optimized for embedded devices, the algorithm computes a unique DRIP Score to quantify the importance of each data point. This enables dynamic decision-making on whether a data point should be stored for potential retraining or discarded without compromising model performance. Experimental evaluations on four benchmark datasets demonstrate that our approach can match or even surpass the accuracy of models trained on the entire dataset, all while achieving storage savings of up to 39\textbackslash\%. To our knowledge, this is the first algorithm that makes online decisions about data point retention without requiring access to the entire dataset.},
  howpublished = {https://arxiv.org/abs/2504.08364v1},
  langid = {english},
  keywords = {Active learning,data pruning,online active learning},
  file = {C:\Users\E097600\Zotero\storage\IZFV2Y3R\Rüb et al. - 2025 - DRIP DRop unImportant data Points -- Enhancing Machine Learning Efficiency with Grad-CAM-Based Real.pdf}
}

@misc{ryaliHieraHierarchicalVision2023,
  title = {Hiera: {{A Hierarchical Vision Transformer}} without the {{Bells-and-Whistles}}},
  shorttitle = {Hiera},
  author = {Ryali, Chaitanya and Hu, Yuan-Ting and Bolya, Daniel and Wei, Chen and Fan, Haoqi and Huang, Po-Yao and Aggarwal, Vaibhav and Chowdhury, Arkabandhu and Poursaeed, Omid and Hoffman, Judy and Malik, Jitendra and Li, Yanghao and Feichtenhofer, Christoph},
  year = 2023,
  month = jun,
  number = {arXiv:2306.00989},
  eprint = {2306.00989},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.00989},
  urldate = {2025-07-28},
  abstract = {Modern hierarchical vision transformers have added several vision-specific components in the pursuit of supervised classification performance. While these components lead to effective accuracies and attractive FLOP counts, the added complexity actually makes these transformers slower than their vanilla ViT counterparts. In this paper, we argue that this additional bulk is unnecessary. By pretraining with a strong visual pretext task (MAE), we can strip out all the bells-and-whistles from a state-of-the-art multi-stage vision transformer without losing accuracy. In the process, we create Hiera, an extremely simple hierarchical vision transformer that is more accurate than previous models while being significantly faster both at inference and during training. We evaluate Hiera on a variety of tasks for image and video recognition. Our code and models are available at https://github.com/facebookresearch/hiera.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,ssl},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\PJ2ZLNYS\\Ryali et al. - 2023 - Hiera A Hierarchical Vision Transformer without the Bells-and-Whistles.pdf;C\:\\Users\\E097600\\Zotero\\storage\\2QX7D4TA\\2306.html}
}

@article{sadinleLeastAmbiguousSetValued2019,
  title = {Least {{Ambiguous Set-Valued Classifiers}} with {{Bounded Error Levels}}},
  author = {Sadinle, Mauricio and Lei, Jing and Wasserman, Larry},
  year = 2019,
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {114},
  number = {525},
  eprint = {1609.00451},
  primaryclass = {stat},
  pages = {223--234},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1395341},
  urldate = {2025-04-16},
  abstract = {In most classification tasks there are observations that are ambiguous and therefore difficult to correctly label. Set-valued classifiers output sets of plausible labels rather than a single label, thereby giving a more appropriate and informative treatment to the labeling of ambiguous instances. We introduce a framework for multiclass set-valued classification, where the classifiers guarantee user-defined levels of coverage or confidence (the probability that the true label is contained in the set) while minimizing the ambiguity (the expected size of the output). We first derive oracle classifiers assuming the true distribution to be known. We show that the oracle classifiers are obtained from level sets of the functions that define the conditional probability of each class. Then we develop estimators with good asymptotic and finite sample properties. The proposed estimators build on existing single-label classifiers. The optimal classifier can sometimes output the empty set, but we provide two solutions to fix this issue that are suitable for various practical needs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\XX7D6YE4\\Sadinle et al. - 2019 - Least Ambiguous Set-Valued Classifiers with Bounded Error Levels.pdf;C\:\\Users\\E097600\\Zotero\\storage\\9AMJBD2I\\1609.html}
}

@article{saiduActiveLearningBayesian2021,
  title = {Active {{Learning}} with {{Bayesian UNet}} for {{Efficient Semantic Image Segmentation}}},
  author = {Saidu, Isah Charles and Csat{\'o}, Lehel},
  year = 2021,
  month = feb,
  journal = {Journal of Imaging},
  volume = {7},
  number = {2},
  pages = {37},
  issn = {2313-433X},
  doi = {10.3390/jimaging7020037},
  urldate = {2024-03-13},
  abstract = {We present a sample-efficient image segmentation method using active learning, we call it Active Bayesian UNet, or AB-UNet. This is a convolutional neural network using batch normalization and max-pool dropout. The Bayesian setup is achieved by exploiting the probabilistic extension of the dropout mechanism, leading to the possibility to use the uncertainty inherently present in the system. We set up our experiments on various medical image datasets and highlight that with a smaller annotation effort our AB-UNet leads to stable training and better generalization. Added to this, we can efficiently choose from an unlabelled dataset.},
  pmcid = {PMC8321278},
  pmid = {34460636},
  file = {C:\Users\E097600\Zotero\storage\LV67GHHK\Saidu et Csató - 2021 - Active Learning with Bayesian UNet for Efficient S.pdf}
}

@misc{schiavoneMyriadALActiveFew2024,
  title = {{{MyriadAL}}: {{Active Few Shot Learning}} for {{Histopathology}}},
  shorttitle = {{{MyriadAL}}},
  author = {Schiavone, Nico and Wang, Jingyi and Li, Shuangzhi and Zemp, Roger and Li, Xingyu},
  year = 2024,
  month = apr,
  number = {arXiv:2310.16161},
  eprint = {2310.16161},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.16161},
  urldate = {2024-06-28},
  abstract = {Active Learning (AL) and Few Shot Learning (FSL) are two label-efficient methods which have achieved excellent results recently. However, most prior arts in both learning paradigms fail to explore the wealth of the vast unlabelled data. In this study, we address this issue in the scenario where the annotation budget is very limited, yet a large amount of unlabelled data for the target task is available. We frame this work in the context of histopathology where labelling is prohibitively expensive. To this end, we introduce an active few shot learning framework, Myriad Active Learning (MAL), including a contrastive-learning encoder, pseudo-label generation, and novel query sample selection in the loop. Specifically, we propose to massage unlabelled data in a self-supervised manner, where the obtained data representations and clustering knowledge form the basis to activate the AL loop. With feedback from the oracle in each AL cycle, the pseudo-labels of the unlabelled data are refined by optimizing a shallow task-specific net on top of the encoder. These updated pseudo-labels serve to inform and improve the active learning query selection process. Furthermore, we introduce a novel recipe to combine existing uncertainty measures and utilize the entire uncertainty list to reduce sample redundancy in AL. Extensive experiments on two public histopathology datasets show that MAL has superior test accuracy, macro F1-score, and label efficiency compared to prior works, and can achieve a comparable test accuracy to a fully supervised algorithm while labelling only 5\% of the dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\Z7LS4M9I\\Schiavone et al. - 2024 - MyriadAL Active Few Shot Learning for Histopathol.pdf;C\:\\Users\\E097600\\Zotero\\storage\\VAGEU5MK\\2310.html}
}

@inproceedings{schroffFaceNetUnifiedEmbedding2015,
  title = {{{FaceNet}}: {{A Unified Embedding}} for {{Face Recognition}} and {{Clustering}}},
  shorttitle = {{{FaceNet}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year = 2015,
  month = jun,
  eprint = {1503.03832},
  primaryclass = {cs},
  pages = {815--823},
  doi = {10.1109/CVPR.2015.7298682},
  urldate = {2024-03-12},
  abstract = {Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result by 30\% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\4GC2PZAA\\Schroff et al. - 2015 - FaceNet A Unified Embedding for Face Recognition .pdf;C\:\\Users\\E097600\\Zotero\\storage\\UFFXF27U\\1503.html}
}

@misc{senerActiveLearningConvolutional2018,
  title = {Active {{Learning}} for {{Convolutional Neural Networks}}: {{A Core-Set Approach}}},
  shorttitle = {Active {{Learning}} for {{Convolutional Neural Networks}}},
  author = {Sener, Ozan and Savarese, Silvio},
  year = 2018,
  month = jun,
  number = {arXiv:1708.00489},
  eprint = {1708.00489},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.00489},
  urldate = {2024-03-07},
  abstract = {Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, ie. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,exploration,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\YU3DGCCY\\Sener et Savarese - 2018 - Active Learning for Convolutional Neural Networks.pdf;C\:\\Users\\E097600\\Zotero\\storage\\UYMRIRZ4\\1708.html}
}

@inproceedings{settlesTheoriesQueriesActive2011,
  title = {From {{Theories}} to {{Queries}}: {{Active Learning}} in {{Practice}}},
  shorttitle = {From {{Theories}} to {{Queries}}},
  booktitle = {Active {{Learning}} and {{Experimental Design}} Workshop {{In}} Conjunction with {{AISTATS}} 2010},
  author = {Settles, Burr},
  year = 2011,
  month = apr,
  pages = {1--18},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  urldate = {2025-11-25},
  abstract = {This article surveys recent work in active learning aimed at making it more practical for real-world use. In general, active learning systems aim to make machine learning more economical, since they can participate in the acquisition of their own training data. An active learner might iteratively select informative query instances to be labeled by an oracle, for example. Work over the last two decades has shown that such approaches are effective at maintaining accuracy while reducing training set size in many machine learning applications. However, as we begin to deploy active learning in real ongoing learning systems and data annotation projects, we are encountering unexpected problems--due in part to practical realities that violate the basic assumptions of earlier foundational work. I review some of these issues, and discuss recent work being done to address the challenges.},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\ILSFCJR8\Settles - 2011 - From Theories to Queries Active Learning in Practice.pdf}
}

@misc{shafirActiveLearningNoisy2025,
  title = {Active {{Learning}} with a {{Noisy Annotator}}},
  author = {Shafir, Netta and Hacohen, Guy and Weinshall, Daphna},
  year = 2025,
  month = apr,
  number = {arXiv:2504.04506},
  eprint = {2504.04506},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.04506},
  urldate = {2025-04-11},
  abstract = {Active Learning (AL) aims to reduce annotation costs by strategically selecting the most informative samples for labeling. However, most active learning methods struggle in the low-budget regime where only a few labeled examples are available. This issue becomes even more pronounced when annotators provide noisy labels. A common AL approach for the low- and mid-budget regimes focuses on maximizing the coverage of the labeled set across the entire dataset. We propose a novel framework called Noise-Aware Active Sampling (NAS) that extends existing greedy, coverage-based active learning strategies to handle noisy annotations. NAS identifies regions that remain uncovered due to the selection of noisy representatives and enables resampling from these areas. We introduce a simple yet effective noise filtering approach suitable for the low-budget regime, which leverages the inner mechanism of NAS and can be applied for noise filtering before model training. On multiple computer vision benchmarks, including CIFAR100 and ImageNet subsets, NAS significantly improves performance for standard active learning methods across different noise types and rates.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\GAJUFLSF\\Shafir et al. - 2025 - Active Learning with a Noisy Annotator.pdf;C\:\\Users\\E097600\\Zotero\\storage\\RSAJ2SII\\2504.html}
}

@article{shannonMathematicalTheoryCommunication2001,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, C. E.},
  year = 2001,
  month = jan,
  journal = {SIGMOBILE Mob. Comput. Commun. Rev.},
  volume = {5},
  number = {1},
  pages = {3--55},
  issn = {1559-1662},
  doi = {10.1145/584091.584093},
  urldate = {2025-04-30}
}

@misc{sharmaImageClassificationVs2019,
  title = {Image {{Classification}} vs {{Object Detection}} vs {{Image Segmentation}}},
  author = {Sharma, Pulkit},
  year = 2019,
  month = aug,
  journal = {Analytics Vidhya},
  urldate = {2024-03-13},
  abstract = {The difference  between Image Classification, Object Detection and Image Segmentation in the context of Computer Vision},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\W225ZS54\image-classification-vs-object-detection-vs-image-segmentation-f36db85fe81.html}
}

@misc{shinAllYouNeed2021,
  title = {All You Need Are a Few Pixels: Semantic Segmentation with {{PixelPick}}},
  shorttitle = {All You Need Are a Few Pixels},
  author = {Shin, Gyungin and Xie, Weidi and Albanie, Samuel},
  year = 2021,
  month = apr,
  journal = {arXiv.org},
  urldate = {2025-04-22},
  abstract = {A central challenge for the task of semantic segmentation is the prohibitive cost of obtaining dense pixel-level annotations to supervise model training. In this work, we show that in order to achieve a good level of segmentation performance, all you need are a few well-chosen pixel labels. We make the following contributions: (i) We investigate the novel semantic segmentation setting in which labels are supplied only at sparse pixel locations, and show that deep neural networks can use a handful of such labels to good effect; (ii) We demonstrate how to exploit this phenomena within an active learning framework, termed PixelPick, to radically reduce labelling cost, and propose an efficient "mouse-free" annotation strategy to implement our approach; (iii) We conduct extensive experiments to study the influence of annotation diversity under a fixed budget, model pretraining, model capacity and the sampling mechanism for picking pixels in this low annotation regime; (iv) We provide comparisons to the existing state of the art in semantic segmentation with active learning, and demonstrate comparable performance with up to two orders of magnitude fewer pixel annotations on the CamVid, Cityscapes and PASCAL VOC 2012 benchmarks; (v) Finally, we evaluate the efficiency of our annotation pipeline and its sensitivity to annotator error to demonstrate its practicality.},
  howpublished = {https://arxiv.org/abs/2104.06394v2},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\WW64TVF6\Shin et al. - 2021 - All you need are a few pixels semantic segmentation with PixelPick.pdf}
}

@inproceedings{sibliniMasterYourMetrics2020,
  title = {Master {{Your Metrics}} with {{Calibration}}},
  booktitle = {Advances in {{Intelligent Data Analysis XVIII}}},
  author = {Siblini, Wissam and Fr{\'e}ry, Jordan and {He-Guelton}, Liyun and Obl{\'e}, Fr{\'e}d{\'e}ric and Wang, Yi-Qing},
  editor = {Berthold, Michael R. and Feelders, Ad and Krempl, Georg},
  year = 2020,
  pages = {457--469},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-44584-3_36},
  abstract = {Machine learning models deployed in real-world applications are often evaluated with precision-based metrics such as F1-score or AUC-PR (Area Under the Curve of Precision Recall). Heavily dependent on the class prior, such metrics make it difficult to interpret the variation of a model's performance over different subpopulations/subperiods in a dataset. In this paper, we propose a way to calibrate the metrics so that they can be made invariant to the prior. We conduct a large number of experiments on balanced and imbalanced data to assess the behavior of calibrated metrics and show that they improve interpretability and provide a better control over what is really measured. We describe specific real-world use-cases where calibration is beneficial such as, for instance, model monitoring in production, reporting, or fairness evaluation.},
  isbn = {978-3-030-44584-3},
  langid = {english},
  keywords = {class imbalance,Performance metrics,Precision-recall},
  file = {C:\Users\E097600\Zotero\storage\V5WCLUJM\Siblini et al. - 2020 - Master Your Metrics with Calibration.pdf}
}

@misc{siddiquiViewALActiveLearning2020,
  title = {{{ViewAL}}: {{Active Learning}} with {{Viewpoint Entropy}} for {{Semantic Segmentation}}},
  shorttitle = {{{ViewAL}}},
  author = {Siddiqui, Yawar and Valentin, Julien and Nie{\ss}ner, Matthias},
  year = 2020,
  month = mar,
  number = {arXiv:1911.11789},
  eprint = {1911.11789},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.11789},
  urldate = {2025-03-11},
  abstract = {We propose ViewAL, a novel active learning strategy for semantic segmentation that exploits viewpoint consistency in multi-view datasets. Our core idea is that inconsistencies in model predictions across viewpoints provide a very reliable measure of uncertainty and encourage the model to perform well irrespective of the viewpoint under which objects are observed. To incorporate this uncertainty measure, we introduce a new viewpoint entropy formulation, which is the basis of our active learning strategy. In addition, we propose uncertainty computations on a superpixel level, which exploits inherently localized signal in the segmentation task, directly lowering the annotation costs. This combination of viewpoint entropy and the use of superpixels allows to efficiently select samples that are highly informative for improving the network. We demonstrate that our proposed active learning strategy not only yields the best-performing models for the same amount of required labeled data, but also significantly reduces labeling effort. For instance, our method achieves 95\% of maximum achievable network performance using only 7\%, 17\%, and 24\% labeled data on SceneNet-RGBD, ScanNet, and Matterport3D, respectively. On these datasets, the best state-of-the-art method achieves the same performance with 14\%, 27\% and 33\% labeled data. Finally, we demonstrate that labeling using superpixels yields the same quality of ground-truth compared to labeling whole images, but requires 25\% less time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\BKDXLMY8\\Siddiqui et al. - 2020 - ViewAL Active Learning with Viewpoint Entropy for Semantic Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\PITQCUG3\\1911.html}
}

@misc{simeoniDINOv32025,
  title = {{{DINOv3}}},
  author = {Sim{\'e}oni, Oriane and Vo, Huy V. and Seitzer, Maximilian and Baldassarre, Federico and Oquab, Maxime and Jose, Cijo and Khalidov, Vasil and Szafraniec, Marc and Yi, Seungeun and Ramamonjisoa, Micha{\"e}l and Massa, Francisco and Haziza, Daniel and Wehrstedt, Luca and Wang, Jianyuan and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Sentana, Leonel and Roberts, Claire and Vedaldi, Andrea and Tolan, Jamie and Brandt, John and Couprie, Camille and Mairal, Julien and J{\'e}gou, Herv{\'e} and Labatut, Patrick and Bojanowski, Piotr},
  year = 2025,
  month = aug,
  number = {arXiv:2508.10104},
  eprint = {2508.10104},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.10104},
  urldate = {2025-09-30},
  abstract = {Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\UVNUFPWQ\\Siméoni et al. - 2025 - DINOv3.pdf;C\:\\Users\\E097600\\Zotero\\storage\\GLGTJLZD\\2508.html}
}

@misc{singhEffectivenessMAEPrepretraining2024,
  title = {The Effectiveness of {{MAE}} Pre-Pretraining for Billion-Scale Pretraining},
  author = {Singh, Mannat and Duval, Quentin and Alwala, Kalyan Vasudev and Fan, Haoqi and Aggarwal, Vaibhav and Adcock, Aaron and Joulin, Armand and Doll{\'a}r, Piotr and Feichtenhofer, Christoph and Girshick, Ross and Girdhar, Rohit and Misra, Ishan},
  year = 2024,
  month = jan,
  number = {arXiv:2303.13496},
  eprint = {2303.13496},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.13496},
  urldate = {2025-04-02},
  abstract = {This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video recognition, object detection, low-shot classification and zero-shot recognition. Our largest model achieves new state-of-the-art results on iNaturalist-18 (91.7\%), ImageNet-ReaL (91.1\%), 1-shot ImageNet-1k (63.6\%), and zero-shot transfer on Food-101 (96.2\%). Our study reveals that model initialization plays a significant role, even for web-scale pretraining with billions of images, and our models are available publicly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\UH7QYFQ8\\Singh et al. - 2024 - The effectiveness of MAE pre-pretraining for billion-scale pretraining.pdf;C\:\\Users\\E097600\\Zotero\\storage\\8BJCMXYS\\2303.html}
}

@misc{singhSoftLabelTrainingPreserves2025,
  title = {Soft-{{Label Training Preserves Epistemic Uncertainty}}},
  author = {Singh, Agamdeep and Tiwari, Ashish and Hasanbeig, Hosein and Gupta, Priyanshu},
  year = 2025,
  month = nov,
  number = {arXiv:2511.14117},
  eprint = {2511.14117},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2511.14117},
  urldate = {2025-11-19},
  abstract = {Many machine learning tasks involve inherent subjectivity, where annotators naturally provide varied labels. Standard practice collapses these label distributions into single labels, aggregating diverse human judgments into point estimates. We argue that this approach is epistemically misaligned for ambiguous data--the annotation distribution itself should be regarded as the ground truth. Training on collapsed single labels forces models to express false confidence on fundamentally ambiguous cases, creating a misalignment between model certainty and the diversity of human perception. We demonstrate empirically that soft-label training, which treats annotation distributions as ground truth, preserves epistemic uncertainty. Across both vision and NLP tasks, soft-label training achieves 32\% lower KL divergence from human annotations and 61\% stronger correlation between model and annotation entropy, while matching the accuracy of hard-label training. Our work repositions annotation distributions from noisy signals to be aggregated away, to faithful representations of epistemic uncertainty that models should learn to reproduce.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\TMEZDEJN\\Singh et al. - 2025 - Soft-Label Training Preserves Epistemic Uncertainty.pdf;C\:\\Users\\E097600\\Zotero\\storage\\7RUJXYLX\\2511.html}
}

@misc{sinhaVariationalAdversarialActive2019,
  title = {Variational {{Adversarial Active Learning}}},
  author = {Sinha, Samarth and Ebrahimi, Sayna and Darrell, Trevor},
  year = 2019,
  month = oct,
  number = {arXiv:1904.00370},
  eprint = {1904.00370},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.00370},
  urldate = {2025-10-29},
  abstract = {Active learning aims to develop label-efficient algorithms by sampling the most representative queries to be labeled by an oracle. We describe a pool-based semi-supervised active learning algorithm that implicitly learns this sampling mechanism in an adversarial manner. Unlike conventional active learning algorithms, our approach is task agnostic, i.e., it does not depend on the performance of the task for which we are trying to acquire labeled data. Our method learns a latent space using a variational autoencoder (VAE) and an adversarial network trained to discriminate between unlabeled and labeled data. The mini-max game between the VAE and the adversarial network is played such that while the VAE tries to trick the adversarial network into predicting that all data points are from the labeled pool, the adversarial network learns how to discriminate between dissimilarities in the latent space. We extensively evaluate our method on various image classification and semantic segmentation benchmark datasets and establish a new state of the art on \$\textbackslash text\textbraceleft CIFAR10/100\textbraceright\$, \$\textbackslash text\textbraceleft Caltech-256\textbraceright\$, \$\textbackslash text\textbraceleft ImageNet\textbraceright\$, \$\textbackslash text\textbraceleft Cityscapes\textbraceright\$, and \$\textbackslash text\textbraceleft BDD100K\textbraceright\$. Our results demonstrate that our adversarial approach learns an effective low dimensional latent space in large-scale settings and provides for a computationally efficient sampling method. Our code is available at https://github.com/sinhasam/vaal.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Image classification,semantic segmentation,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\9SJF8FFK\\Sinha et al. - 2019 - Variational Adversarial Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\KW2LC6I4\\1904.html}
}

@misc{sinhaZeroshotActiveLearning2024,
  title = {Zero-Shot {{Active Learning Using Self Supervised Learning}}},
  author = {Sinha, Abhishek and Singh, Shreya},
  year = 2024,
  month = jan,
  number = {arXiv:2401.01690},
  eprint = {2401.01690},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.01690},
  urldate = {2024-03-06},
  abstract = {Deep learning algorithms are often said to be data hungry. The performance of such algorithms generally improve as more and more annotated data is fed into the model. While collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task. Given a fixed budget available for data annotation, Active Learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget. In this work, we aim to propose a new Active Learning approach which is model agnostic as well as one doesn't require an iterative process. We aim to leverage self-supervised learnt features for the task of Active Learning. The benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\KWELMTA9\\Sinha et Singh - 2024 - Zero-shot Active Learning Using Self Supervised Le.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ZVJ5PMXK\\2401.html}
}

@misc{siWhyCanAccurate2025,
  title = {Why {{Can Accurate Models Be Learned}} from {{Inaccurate Annotations}}?},
  author = {Si, Chongjie and Cui, Yidan and Yang, Fuchao and Yang, Xiaokang and Shen, Wei},
  year = 2025,
  month = may,
  journal = {arXiv.org},
  urldate = {2025-05-27},
  abstract = {Learning from inaccurate annotations has gained significant attention due to the high cost of precise labeling. However, despite the presence of erroneous labels, models trained on noisy data often retain the ability to make accurate predictions. This intriguing phenomenon raises a fundamental yet largely unexplored question: why models can still extract correct label information from inaccurate annotations remains unexplored. In this paper, we conduct a comprehensive investigation into this issue. By analyzing weight matrices from both empirical and theoretical perspectives, we find that label inaccuracy primarily accumulates noise in lower singular components and subtly perturbs the principal subspace. Within a certain range, the principal subspaces of weights trained on inaccurate labels remain largely aligned with those learned from clean labels, preserving essential task-relevant information. We formally prove that the angles of principal subspaces exhibit minimal deviation under moderate label inaccuracy, explaining why models can still generalize effectively. Building on these insights, we propose LIP, a lightweight plug-in designed to help classifiers retain principal subspace information while mitigating noise induced by label inaccuracy. Extensive experiments on tasks with various inaccuracy conditions demonstrate that LIP consistently enhances the performance of existing algorithms. We hope our findings can offer valuable theoretical and practical insights to understand of model robustness under inaccurate supervision.},
  howpublished = {https://arxiv.org/abs/2505.16159v1},
  langid = {english},
  keywords = {noisy labels,robustness},
  file = {C:\Users\E097600\Zotero\storage\RQVNFHQS\Si et al. - 2025 - Why Can Accurate Models Be Learned from Inaccurate Annotations.pdf}
}

@misc{smithPredictionOrientedBayesianActive2023,
  title = {Prediction-{{Oriented Bayesian Active Learning}}},
  author = {Smith, Freddie Bickford and Kirsch, Andreas and Farquhar, Sebastian and Gal, Yarin and Foster, Adam and Rainforth, Tom},
  year = 2023,
  month = apr,
  number = {arXiv:2304.08151},
  eprint = {2304.08151},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.08151},
  urldate = {2025-10-31},
  abstract = {Information-theoretic approaches to active learning have traditionally focused on maximising the information gathered about the model parameters, most commonly by optimising the BALD score. We highlight that this can be suboptimal from the perspective of predictive performance. For example, BALD lacks a notion of an input distribution and so is prone to prioritise data of limited relevance. To address this we propose the expected predictive information gain (EPIG), an acquisition function that measures information gain in the space of predictions rather than parameters. We find that using EPIG leads to stronger predictive performance compared with BALD across a range of datasets and models, and thus provides an appealing drop-in replacement.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\WYAJ2SPD\\Smith et al. - 2023 - Prediction-Oriented Bayesian Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\4RE9CYKT\\2304.html}
}

@misc{snellPrototypicalNetworksFewshot2017,
  title = {Prototypical {{Networks}} for {{Few-shot Learning}}},
  author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
  year = 2017,
  month = jun,
  number = {arXiv:1703.05175},
  eprint = {1703.05175},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.05175},
  urldate = {2024-03-05},
  abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\JGTQ2E4E\\Snell et al. - 2017 - Prototypical Networks for Few-shot Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\4SKINRQS\\1703.html}
}

@inproceedings{sohnImprovedDeepMetric2016,
  title = {Improved Deep Metric Learning with Multi-Class {{N-pair}} Loss Objective},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Sohn, Kihyuk},
  year = 2016,
  month = dec,
  series = {{{NIPS}}'16},
  pages = {1857--1865},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2024-03-12},
  abstract = {Deep metric learning has gained much popularity in recent years, following the success of deep learning. However, existing frameworks of deep metric learning based on contrastive loss and triplet loss often suffer from slow convergence, partially because they employ only one negative example while not interacting with the other negative classes in each update. In this paper, we propose to address this problem with a new metric learning objective called multi-class N-pair loss. The proposed objective function firstly generalizes triplet loss by allowing joint comparison among more than one negative examples - more specifically, N-1 negative examples - and secondly reduces the computational burden of evaluating deep embedding vectors via an efficient batch construction strategy using only N pairs of examples, instead of (N+1) x N. We demonstrate the superiority of our proposed loss to the triplet loss as well as other competing loss functions for a variety of tasks on several visual recognition benchmark, including fine-grained object recognition and verification, image clustering and retrieval, and face verification and identification.},
  isbn = {978-1-5108-3881-9},
  file = {C:\Users\E097600\Zotero\storage\BBMMPVXP\Sohn - 2016 - Improved deep metric learning with multi-class N-p.pdf}
}

@misc{songLearningNoisyLabels2020,
  title = {Learning from {{Noisy Labels}} with {{Deep Neural Networks}}: {{A Survey}}},
  shorttitle = {Learning from {{Noisy Labels}} with {{Deep Neural Networks}}},
  author = {Song, Hwanjun and Kim, Minseok and Park, Dongmin and Shin, Yooju and Lee, Jae-Gil},
  year = 2020,
  month = jul,
  journal = {arXiv.org},
  urldate = {2025-04-07},
  abstract = {Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we first describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 62 state-of-the-art robust training methods, all of which are categorized into five groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we perform an in-depth analysis of noise rate estimation and summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies. All the contents will be available at https://github.com/songhwanjun/Awesome-Noisy-Labels.},
  howpublished = {https://arxiv.org/abs/2007.08199v7},
  langid = {english},
  keywords = {Deep learning,label noise,review},
  file = {C:\Users\E097600\Zotero\storage\UAR6MEV9\Song et al. - 2020 - Learning from Noisy Labels with Deep Neural Networks A Survey.pdf}
}

@misc{sProgressiveDataDropout2025,
  title = {Progressive {{Data Dropout}}: {{An Embarrassingly Simple Approach}} to {{Faster Training}}},
  shorttitle = {Progressive {{Data Dropout}}},
  author = {S, Shriram M. and Hao, Xinyue and Hou, Shihao and Lu, Yang and {Sevilla-Lara}, Laura and Arnab, Anurag and Gowda, Shreyank N.},
  year = 2025,
  month = may,
  journal = {arXiv.org},
  urldate = {2025-06-02},
  abstract = {The success of the machine learning field has reliably depended on training on large datasets. While effective, this trend comes at an extraordinary cost. This is due to two deeply intertwined factors: the size of models and the size of datasets. While promising research efforts focus on reducing the size of models, the other half of the equation remains fairly mysterious. Indeed, it is surprising that the standard approach to training remains to iterate over and over, uniformly sampling the training dataset. In this paper we explore a series of alternative training paradigms that leverage insights from hard-data-mining and dropout, simple enough to implement and use that can become the new training standard. The proposed Progressive Data Dropout reduces the number of effective epochs to as little as 12.4\% of the baseline. This savings actually do not come at any cost for accuracy. Surprisingly, the proposed method improves accuracy by up to 4.82\%. Our approach requires no changes to model architecture or optimizer, and can be applied across standard training pipelines, thus posing an excellent opportunity for wide adoption. Code can be found here: https://github.com/bazyagami/LearningWithRevision},
  howpublished = {https://arxiv.org/abs/2505.22342v1},
  langid = {english},
  keywords = {Active sampling,data dropout,data pruning,dingz,pedagogical},
  file = {C:\Users\E097600\Zotero\storage\6P2BUCPI\S et al. - 2025 - Progressive Data Dropout An Embarrassingly Simple Approach to Faster Training.pdf}
}

@misc{srivastavaVEEGANReducingMode2017,
  title = {{{VEEGAN}}: {{Reducing Mode Collapse}} in {{GANs}} Using {{Implicit Variational Learning}}},
  shorttitle = {{{VEEGAN}}},
  author = {Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael U. and Sutton, Charles},
  year = 2017,
  month = nov,
  number = {arXiv:1705.07761},
  eprint = {1705.07761},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.07761},
  urldate = {2025-09-17},
  abstract = {Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\LY7LW6XS\\Srivastava et al. - 2017 - VEEGAN Reducing Mode Collapse in GANs using Implicit Variational Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\AI4V5NW4\\1705.html}
}

@incollection{sudreGeneralisedDiceOverlap2017,
  title = {Generalised {{Dice}} Overlap as a Deep Learning Loss Function for Highly Unbalanced Segmentations},
  author = {Sudre, Carole H. and Li, Wenqi and Vercauteren, Tom and Ourselin, S{\'e}bastien and Cardoso, M. Jorge},
  year = 2017,
  volume = {10553},
  eprint = {1707.03237},
  primaryclass = {cs},
  pages = {240--248},
  doi = {10.1007/978-3-319-67558-9_28},
  urldate = {2025-01-17},
  abstract = {Deep-learning has proved in recent years to be a powerful tool for image analysis and is now widely used to segment both 2D and 3D medical images. Deep-learning segmentation frameworks rely not only on the choice of network architecture but also on the choice of loss function. When the segmentation process targets rare observations, a severe class imbalance is likely to occur between candidate labels, thus resulting in sub-optimal performance. In order to mitigate this issue, strategies such as the weighted cross-entropy function, the sensitivity function or the Dice loss function, have been proposed. In this work, we investigate the behavior of these loss functions and their sensitivity to learning rate tuning in the presence of different rates of label imbalance across 2D and 3D segmentation tasks. We also propose to use the class re-balancing properties of the Generalized Dice overlap, a known metric for segmentation assessment, as a robust and accurate deep-learning loss function for unbalanced tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\N5U4ZUJ3\\Sudre et al. - 2017 - Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations.pdf;C\:\\Users\\E097600\\Zotero\\storage\\LF3JEUK6\\1707.html}
}

@misc{sunBoundaryDifferenceUnion2023,
  title = {Boundary {{Difference Over Union Loss For Medical Image Segmentation}}},
  author = {Sun, Fan and Luo, Zhiming and Li, Shaozi},
  year = 2023,
  month = aug,
  number = {arXiv:2308.00220},
  eprint = {2308.00220},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.00220},
  urldate = {2025-01-10},
  abstract = {Medical image segmentation is crucial for clinical diagnosis. However, current losses for medical image segmentation mainly focus on overall segmentation results, with fewer losses proposed to guide boundary segmentation. Those that do exist often need to be used in combination with other losses and produce ineffective results. To address this issue, we have developed a simple and effective loss called the Boundary Difference over Union Loss (Boundary DoU Loss) to guide boundary region segmentation. It is obtained by calculating the ratio of the difference set of prediction and ground truth to the union of the difference set and the partial intersection set. Our loss only relies on region calculation, making it easy to implement and training stable without needing any additional losses. Additionally, we use the target size to adaptively adjust attention applied to the boundary regions. Experimental results using UNet, TransUNet, and Swin-UNet on two datasets (ACDC and Synapse) demonstrate the effectiveness of our proposed loss function. Code is available at https://github.com/sunfan-bvb/BoundaryDoULoss.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,loss},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\CSHCG76T\\Sun et al. - 2023 - Boundary Difference Over Union Loss For Medical Image Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\8TDD65NX\\2308.html}
}

@misc{sunUnseenVisualAnomaly2025,
  title = {Unseen {{Visual Anomaly Generation}}},
  author = {Sun, Han and Cao, Yunkang and Dong, Hao and Fink, Olga},
  year = 2025,
  month = jun,
  number = {arXiv:2406.01078},
  eprint = {2406.01078},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.01078},
  urldate = {2025-06-24},
  abstract = {Visual anomaly detection (AD) presents significant challenges due to the scarcity of anomalous data samples. While numerous works have been proposed to synthesize anomalous samples, these synthetic anomalies often lack authenticity or require extensive training data, limiting their applicability in real-world scenarios. In this work, we propose Anomaly Anything (AnomalyAny), a novel framework that leverages Stable Diffusion (SD)'s image generation capabilities to generate diverse and realistic unseen anomalies. By conditioning on a single normal sample during test time, AnomalyAny is able to generate unseen anomalies for arbitrary object types with text descriptions. Within AnomalyAny, we propose attention-guided anomaly optimization to direct SD attention on generating hard anomaly concepts. Additionally, we introduce prompt-guided anomaly refinement, incorporating detailed descriptions to further improve the generation quality. Extensive experiments on MVTec AD and VisA datasets demonstrate AnomalyAny's ability in generating high-quality unseen anomalies and its effectiveness in enhancing downstream AD performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\CIVN2DAD\\Sun et al. - 2025 - Unseen Visual Anomaly Generation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\6JVI56QW\\2406.html}
}

@misc{suttonIntroductionConditionalRandom2010,
  title = {An {{Introduction}} to {{Conditional Random Fields}}},
  author = {Sutton, Charles and McCallum, Andrew},
  year = 2010,
  month = nov,
  number = {arXiv:1011.4088},
  eprint = {1011.4088},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1011.4088},
  urldate = {2025-03-24},
  abstract = {Often we wish to predict a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling, combining the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This tutorial describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large scale CRFs. We do not assume previous knowledge of graphical modeling, so this tutorial is intended to be useful to practitioners in a wide variety of fields.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\PQL3JZ7G\\Sutton et McCallum - 2010 - An Introduction to Conditional Random Fields.pdf;C\:\\Users\\E097600\\Zotero\\storage\\8DEPEJ8Y\\1011.html}
}

@article{taghanakiComboLossHandling2019,
  title = {Combo Loss: {{Handling}} Input and Output Imbalance in Multi-Organ Segmentation},
  shorttitle = {Combo Loss},
  author = {Taghanaki, Saeid Asgari and Zheng, Yefeng and Kevin Zhou, S. and Georgescu, Bogdan and Sharma, Puneet and Xu, Daguang and Comaniciu, Dorin and Hamarneh, Ghassan},
  year = 2019,
  month = jul,
  journal = {Computerized Medical Imaging and Graphics},
  volume = {75},
  pages = {24--33},
  issn = {0895-6111},
  doi = {10.1016/j.compmedimag.2019.04.005},
  urldate = {2025-08-25},
  abstract = {Simultaneous segmentation of multiple organs from different medical imaging modalities is a crucial task as it can be utilized for computer-aided diagnosis, computer-assisted surgery, and therapy planning. Thanks to the recent advances in deep learning, several deep neural networks for medical image segmentation have been introduced successfully for this purpose. In this paper, we focus on learning a deep multi-organ segmentation network that labels voxels. In particular, we examine the critical choice of a loss function in order to handle the notorious imbalance problem that plagues both the input and output of a learning model. The input imbalance refers to the class-imbalance in the input training samples (i.e., small foreground objects embedded in an abundance of background voxels, as well as organs of varying sizes). The output imbalance refers to the imbalance between the false positives and false negatives of the inference model. In order to tackle both types of imbalance during training and inference, we introduce a new curriculum learning based loss function. Specifically, we leverage Dice similarity coefficient to deter model parameters from being held at bad local minima and at the same time gradually learn better model parameters by penalizing for false positives/negatives using a cross entropy term. We evaluated the proposed loss function on three datasets: whole body positron emission tomography (PET) scans with 5 target organs, magnetic resonance imaging (MRI) prostate scans, and ultrasound echocardigraphy images with a single target organ i.e., left ventricular. We show that a simple network architecture with the proposed integrative loss function can outperform state-of-the-art methods and results of the competing methods can be improved when our proposed loss is used.},
  keywords = {Class imbalance,Computer Science - Computer Vision and Pattern Recognition,Deep convolutional neural networks,loss function,Multi-organ segmentation,Output imbalance},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\AWBXIRMA\\Taghanaki et al. - 2019 - Combo loss Handling input and output imbalance in multi-organ segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\E5TFUQ3U\\Taghanaki et al. - 2021 - Combo Loss Handling Input and Output Imbalance in Multi-Organ Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\89FGTJFC\\1805.html;C\:\\Users\\E097600\\Zotero\\storage\\IDULWKIE\\S0895611118305688.html}
}

@article{takezoeDeepActiveLearning2023,
  title = {Deep {{Active Learning}} for {{Computer Vision}}: {{Past}} and {{Future}}},
  shorttitle = {Deep {{Active Learning}} for {{Computer Vision}}},
  author = {Takezoe, Rinyoichi and Liu, Xu and Mao, Shunan and Chen, Marco Tianyu and Feng, Zhanpeng and Zhang, Shiliang and Wang, Xiaoyu},
  year = 2023,
  journal = {APSIPA Transactions on Signal and Information Processing},
  volume = {12},
  number = {1},
  eprint = {2211.14819},
  primaryclass = {cs},
  issn = {2048-7703},
  doi = {10.1561/116.00000057},
  urldate = {2025-04-25},
  abstract = {As an important data selection schema, active learning emerges as the essential component when iterating an Artificial Intelligence (AI) model. It becomes even more critical given the dominance of deep neural network based models, which are composed of a large number of parameters and data hungry, in application. Despite its indispensable role for developing AI models, research on active learning is not as intensive as other research directions. In this paper, we present a review of active learning through deep active learning approaches from the following perspectives: 1) technical advancements in active learning, 2) applications of active learning in computer vision, 3) industrial systems leveraging or with potential to leverage active learning for data iteration, 4) current limitations and future research directions. We expect this paper to clarify the significance of active learning in a modern AI model manufacturing process and to bring additional research attention to active learning. By addressing data automation challenges and coping with automated machine learning systems, active learning will facilitate democratization of AI technologies by boosting model production at scale.},
  archiveprefix = {arXiv},
  keywords = {basics,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer vision,Deep learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\G2DA34QV\\Takezoe et al. - 2023 - Deep Active Learning for Computer Vision Past and Future.pdf;C\:\\Users\\E097600\\Zotero\\storage\\RDRT3PUB\\2211.html}
}

@article{tanBayesianEstimateMean2023,
  title = {Bayesian {{Estimate}} of {{Mean Proper Scores}} for {{Diversity-Enhanced Active Learning}}},
  author = {Tan, Wei and Du, Lan and Buntine, Wray},
  year = 2023,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {2312.10116},
  primaryclass = {cs},
  pages = {1--16},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2023.3343359},
  urldate = {2024-03-08},
  abstract = {The effectiveness of active learning largely depends on the sampling efficiency of the acquisition function. Expected Loss Reduction (ELR) focuses on a Bayesian estimate of the reduction in classification error, and more general costs fit in the same framework. We propose Bayesian Estimate of Mean Proper Scores (BEMPS) to estimate the increase in strictly proper scores such as log probability or negative mean square error within this framework. We also prove convergence results for this general class of costs. To facilitate better experimentation with the new acquisition functions, we develop a complementary batch AL algorithm that encourages diversity in the vector of expected changes in scores for unlabeled data. To allow high-performance classifiers, we combine deep ensembles, and dynamic validation set construction on pretrained models, and further speed up the ensemble process with the idea of Monte Carlo Dropout. Extensive experiments on both texts and images show that the use of mean square error and log probability with BEMPS yields robust acquisition functions and well-calibrated classifiers, and consistently outperforms the others tested. The advantages of BEMPS over the others are further supported by a set of qualitative analyses, where we visualise their sampling behaviour using data maps and t-SNE plots.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\5NRMAJVX\\Tan et al. - 2023 - Bayesian Estimate of Mean Proper Scores for Divers.pdf;C\:\\Users\\E097600\\Zotero\\storage\\NYKAHA9K\\2312.html}
}

@misc{tangDuATDualAggregationTransformer2022,
  title = {{{DuAT}}: {{Dual-Aggregation Transformer Network}} for {{Medical Image Segmentation}}},
  shorttitle = {{{DuAT}}},
  author = {Tang, Feilong and Huang, Qiming and Wang, Jinfeng and Hou, Xianxu and Su, Jionglong and Liu, Jingxin},
  year = 2022,
  month = dec,
  number = {arXiv:2212.11677},
  eprint = {2212.11677},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.11677},
  urldate = {2025-01-07},
  abstract = {Transformer-based models have been widely demonstrated to be successful in computer vision tasks by modelling long-range dependencies and capturing global representations. However, they are often dominated by features of large patterns leading to the loss of local details (e.g., boundaries and small objects), which are critical in medical image segmentation. To alleviate this problem, we propose a Dual-Aggregation Transformer Network called DuAT, which is characterized by two innovative designs, namely, the Global-to-Local Spatial Aggregation (GLSA) and Selective Boundary Aggregation (SBA) modules. The GLSA has the ability to aggregate and represent both global and local spatial features, which are beneficial for locating large and small objects, respectively. The SBA module is used to aggregate the boundary characteristic from low-level features and semantic information from high-level features for better preserving boundary details and locating the re-calibration objects. Extensive experiments in six benchmark datasets demonstrate that our proposed model outperforms state-of-the-art methods in the segmentation of skin lesion images, and polyps in colonoscopy images. In addition, our approach is more robust than existing methods in various challenging situations such as small object segmentation and ambiguous object boundaries.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\QWRRBCSV\\Tang et al. - 2022 - DuAT Dual-Aggregation Transformer Network for Medical Image Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\9FFI5LEC\\2212.html}
}

@article{tangFusionM4NetMultistageMultimodal2022,
  title = {{{FusionM4Net}}: {{A}} Multi-Stage Multi-Modal Learning Algorithm for Multi-Label Skin Lesion Classification},
  shorttitle = {{{FusionM4Net}}},
  author = {Tang, Peng and Yan, Xintong and Nan, Yang and Xiang, Shao and Krammer, Sebastian and Lasser, Tobias},
  year = 2022,
  month = feb,
  journal = {Medical Image Analysis},
  volume = {76},
  pages = {102307},
  issn = {1361-8415},
  doi = {10.1016/j.media.2021.102307},
  urldate = {2025-11-18},
  abstract = {Skin disease is one of the most common diseases in the world. Deep learning-based methods have achieved excellent skin lesion recognition performance, most of which are based on only dermoscopy images. In recent works that use multi-modality data (patient's meta-data, clinical images, and dermoscopy images), the methods adopt a one-stage fusion approach and only optimize the information fusion at the feature level. These methods do not use information fusion at the decision level and thus cannot fully use the data of all modalities. This work proposes a novel two-stage multi-modal learning algorithm (FusionM4Net) for multi-label skin diseases classification. At the first stage, we construct a FusionNet, which exploits and integrates the representation of clinical and dermoscopy images at the feature level, and then uses a Fusion Scheme 1 to conduct the information fusion at the decision level. At the second stage, to further incorporate the patient's meta-data, we propose a Fusion Scheme 2, which integrates the multi-label predictive information from the first stage and patient's meta-data information to train an SVM cluster. The final diagnosis is formed by the fusion of the predictions from the first and second stages. Our algorithm was evaluated on the seven-point checklist dataset, a well-established multi-modality multi-label skin disease dataset. Without using the patient's meta-data, the proposed FusionM4Net's first stage (FusionM4Net-FS) achieved an average accuracy of 75.7\% for multi-classification tasks and 74.9\% for diagnostic tasks, which is more accurate than other state-of-the-art methods. By further fusing the patient's meta-data at FusionM4Net's second stage (FusionM4Net-SS), the entire FusionM4Net finally boosts the average accuracy to 77.0\% and the diagnostic accuracy to 78.5\%, which indicates its robust and excellent classification performance on the label-imbalanced dataset. The corresponding code is available at: https://github.com/pixixiaonaogou/MLSDR.},
  lccn = {https://github.com/pixixiaonaogou/MLSDR},
  keywords = {Multi-label classification,Multi-modal learning,Multi-stage information fusion,Seven-points checklist criteria,Skin disease recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\ZJDLS7HB\\Tang et al. - 2022 - FusionM4Net A multi-stage multi-modal learning algorithm for multi-label skin lesion classification.pdf;C\:\\Users\\E097600\\Zotero\\storage\\6XNSAKMP\\S1361841521003522.html}
}

@misc{tangLearningSelfRegularizedAdversarial2022,
  title = {Learning {{Self-Regularized Adversarial Views}} for {{Self-Supervised Vision Transformers}}},
  author = {Tang, Tao and Li, Changlin and Wang, Guangrun and Yu, Kaicheng and Chang, Xiaojun and Liang, Xiaodan},
  year = 2022,
  month = oct,
  number = {arXiv:2210.08458},
  eprint = {2210.08458},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.08458},
  urldate = {2025-07-28},
  abstract = {Automatic data augmentation (AutoAugment) strategies are indispensable in supervised data-efficient training protocols of vision transformers, and have led to state-of-the-art results in supervised learning. Despite the success, its development and application on self-supervised vision transformers have been hindered by several barriers, including the high search cost, the lack of supervision, and the unsuitable search space. In this work, we propose AutoView, a self-regularized adversarial AutoAugment method, to learn views for self-supervised vision transformers, by addressing the above barriers. First, we reduce the search cost of AutoView to nearly zero by learning views and network parameters simultaneously in a single forward-backward step, minimizing and maximizing the mutual information among different augmented views, respectively. Then, to avoid information collapse caused by the lack of label supervision, we propose a self-regularized loss term to guarantee the information propagation. Additionally, we present a curated augmentation policy search space for self-supervised learning, by modifying the generally used search space designed for supervised learning. On ImageNet, our AutoView achieves remarkable improvement over RandAug baseline (+10.2\% k-NN accuracy), and consistently outperforms sota manually tuned view policy by a clear margin (up to +1.3\% k-NN accuracy). Extensive experiments show that AutoView pretraining also benefits downstream tasks (+1.2\% mAcc on ADE20K Semantic Segmentation and +2.8\% mAP on revisited Oxford Image Retrieval benchmark) and improves model robustness (+2.3\% Top-1 Acc on ImageNet-A and +1.0\% AUPR on ImageNet-O). Code and models will be available at https://github.com/Trent-tangtao/AutoView.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer vision,ssl},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\BHM4G6I9\\Tang et al. - 2022 - Learning Self-Regularized Adversarial Views for Self-Supervised Vision Transformers.pdf;C\:\\Users\\E097600\\Zotero\\storage\\QNXNYVZT\\2210.html}
}

@article{tharwatUsingMethodsDimensionality2024,
  title = {Using Methods from Dimensionality Reduction for Active Learning with Low Query Budget},
  author = {Tharwat, Alaa and Schenck, Wolfram},
  year = 2024,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--14},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2024.3365189},
  urldate = {2024-03-06},
  abstract = {Recently, it has been challenging to generate enough labeled data for supervised learning models from a large amount of free unlabeled data due to the high cost of the labeling process. Here, the active learning technique provides a solution by annotating a small but highly informative set of unlabeled data. This ensures high generalizability in space and improves classification performance with test data. The task is more challenging when the query budget is small, the data is imbalanced, multiple classes are present, and no predefined knowledge is available. To address these challenges, we present a novel active learner geometrically based on principal component analysis (PCA) and linear discriminant analysis (LDA). The proposed active learner consists of two phases: The PCA-inspired exploration phase, in which regions with high variances are explored, and the LDA-inspired exploitation phase, in which boundary points between classes are selected. The proposed geometric strategy improves the search capabilities of the active learner, allowing it to explore the space of minority classes even with multiple minority classes and a small query budget. Experiments on synthetic and real binary and multi-class imbalanced data show that the proposed algorithm has significant advantages over multiple known active learners.},
  keywords = {Active learning,Adaptation models,Class imbalance,Data models,Dimensionality reduction,Imbalanced data,Labeling,LDA,Machine learning algorithms,PCA,Principal component analysis,Space exploration,synthetic dataset,Training data},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\5YGH2NS9\\Tharwat et Schenck - 2024 - Using methods from dimensionality reduction for ac.pdf;C\:\\Users\\E097600\\Zotero\\storage\\AG89PYDA\\10433684.html}
}

@phdthesis{theveninContributionMachineLearning2024,
  title = {Contribution of Machine Learning to the Modeling of Turbulent Mixing},
  author = {Thevenin, S{\'e}bastien},
  year = 2024,
  month = nov,
  urldate = {2025-11-18},
  abstract = {Turbulent mixing is involved in many natural flows, in engineering and in everyday life. Its study is particularly essential to better understand and improve the design of inertial confinement fusion (ICF) experiments. In this context, this thesis aims to explore the new possibilities offered by machine learning to address the open questions and challenges associated with turbulent mixing. This manuscript is structured around three classes of contributions -- assistance, identification and representation -- which can help answer these challenges. The first part investigates the influence of the initial conditions characterizing the interfacial perturbation on Rayleigh-Taylor turbulence, assisted by a physics-informed neural network surrogate. The latter enables realistic extrapolations, extending simulations to very late times otherwise inaccessible, and its negligible computational cost has allowed a global sensitivity analysis to be carried out. The study evidences the persistence of the influence of the initial conditions on the mixing zone size at late times, but suggests that the self-similar growth rate is not responsible for this dependence. Rather, it shows that this is imprinted in the virtual origin, defined in time or size, which depends mainly on the steepness number of the perturbation. This study sheds new light on a long-debated issue. The inverse problem, considered in the second part, aims to determine how well the initial conditions can be reconstructed from a single observation of volume-averaged quantities. It is first solved with a Markov chain Monte Carlo sampling method, and then with a generative deep learning model. Since the initial conditions fully characterize the dynamical trajectories, this is a way of quantifying the uncertainties associated with the choice of state variables, and therefore to identify promising state vector candidates for building a model. A stochastic modeling strategy and a deterministic version based on the maximum a posteriori solution (MAP) are then proposed to represent the complex dynamics. The last part employs sparse regression to identify a model describing a turbulent plasma under compression and subject to rapid viscosity variations. Unlike black-box approaches such as neural networks, this approach provides a transparent two-equations model that is both predictive and interpretable. In particular, analysis of the model has confirmed its realizability and enabled the construction of a criterion predicting whether rapid viscous phases are expected or not. The results of this work show that data-driven approaches are powerful tools for addressing challenges associated to the study of turbulent mixing, especially when combined with prior knowledge of the underlying physics. This synergy is the key ingredient for tackling problems that are beyond the reach of traditional methods, drawing on the strengths of both approaches.},
  langid = {english},
  school = {Universit\'e Paris-Saclay},
  file = {C:\Users\E097600\Zotero\storage\2M7JC73V\Thevenin - 2024 - Contribution of machine learning to the modeling of turbulent mixing.pdf}
}

@article{thomineDistillationbasedFabricAnomaly2024,
  title = {Distillation-Based Fabric Anomaly Detection},
  author = {Thomine, Simon and Snoussi, Hichem},
  year = 2024,
  month = mar,
  journal = {Textile Research Journal},
  volume = {94},
  number = {5-6},
  eprint = {2401.02287},
  primaryclass = {cs},
  pages = {552--565},
  issn = {0040-5175, 1746-7748},
  doi = {10.1177/00405175231206820},
  urldate = {2024-03-04},
  abstract = {Unsupervised texture anomaly detection has been a concerning topic in a vast amount of industrial processes. Patterned textures inspection, particularly in the context of fabric defect detection, is indeed a widely encountered use case. This task involves handling a diverse spectrum of colors and textile types, encompassing a wide range of fabrics. Given the extensive variability in colors, textures, and defect types, fabric defect detection poses a complex and challenging problem in the field of patterned textures inspection. In this article, we propose a knowledge distillation-based approach tailored specifically for addressing the challenge of unsupervised anomaly detection in textures resembling fabrics. Our method aims to redefine the recently introduced reverse distillation approach, which advocates for an encoder-decoder design to mitigate classifier bias and to prevent the student from reconstructing anomalies. In this study, we present a new reverse distillation technique for the specific task of fabric defect detection. Our approach involves a meticulous design selection that strategically highlights high-level features. To demonstrate the capabilities of our approach both in terms of performance and inference speed, we conducted a series of experiments on multiple texture datasets, including MVTEC AD, AITEX, and TILDA, alongside conducting experiments on a dataset acquired from a textile manufacturing facility. The main contributions of this paper are the following: a robust texture anomaly detector utilizing a reverse knowledge-distillation technique suitable for both anomaly detection and domain generalization and a novel dataset encompassing a diverse range of fabrics and defects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\2DUMYXC4\\Thomine et Snoussi - 2024 - Distillation-based fabric anomaly detection.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ESGR33VU\\2401.html}
}

@article{tianDCCCenterNetRapidDetection2022,
  title = {{{DCC-CenterNet}}: {{A}} Rapid Detection Method for Steel Surface Defects},
  shorttitle = {{{DCC-CenterNet}}},
  author = {Tian, Rushuai and Jia, Minping},
  year = 2022,
  month = jan,
  journal = {Measurement},
  volume = {187},
  pages = {110211},
  issn = {0263-2241},
  doi = {10.1016/j.measurement.2021.110211},
  urldate = {2024-03-05},
  abstract = {In recent years, surface defect detection methods based on deep learning have been widely used. A conflict between speed and accuracy, however, still exists. In this paper, a steel surface defect detector, DCC-CenterNet, is proposed to achieve the best speed-accuracy trade-off. This detector uses keypoint estimation to locate center points and regresses all other defect properties. Firstly, a dilated feature enhancement model is proposed to enlarge the receptive field of the detector. Secondly, a new centerness function center-weight is proposed to make the keypoint estimation more accurate. Then, the CIoU loss that considers the overlap area and aspect ratio of the defect is adopted in the size regression. Finally, the results of experiments show that the accuracy of DCC-CenterNet can reach 79.41 mAP, and the running speed FPS is 71.37 with input size 224~\texttimes ~224 on the NEU-DET steel defect dataset. And it reaches 61.93 mAP on the GC10-DET steel sheet surface defect dataset at a running speed of 31.47 FPS with input size 512~\texttimes ~512. It demonstrates that the developed detector can detect steel surface defects efficiently and effectively.},
  keywords = {Center-weight,CIoU loss,Dilated convolution,Surface defect detection},
  file = {C:\Users\E097600\Zotero\storage\SSMNP577\S0263224121011210.html}
}

@misc{tibshiraniConformalPredictionCovariate2020,
  title = {Conformal {{Prediction Under Covariate Shift}}},
  author = {Tibshirani, Ryan J. and Barber, Rina Foygel and Candes, Emmanuel J. and Ramdas, Aaditya},
  year = 2020,
  month = jul,
  number = {arXiv:1904.06019},
  eprint = {1904.06019},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.06019},
  urldate = {2025-11-05},
  abstract = {We extend conformal prediction methodology beyond the case of exchangeable data. In particular, we show that a weighted version of conformal prediction can be used to compute distribution-free prediction intervals for problems in which the test and training covariate distributions differ, but the likelihood ratio between these two distributions is known---or, in practice, can be estimated accurately with access to a large set of unlabeled data (test covariate points). Our weighted extension of conformal prediction also applies more generally, to settings in which the data satisfies a certain weighted notion of exchangeability. We discuss other potential applications of our new conformal methodology, including latent variable and missing data problems.},
  archiveprefix = {arXiv},
  keywords = {Conformal prediction,covariate shift,domain adaptation,Domain Shift,Statistics - Methodology},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\96UTIMK7\\Tibshirani et al. - 2020 - Conformal Prediction Under Covariate Shift.pdf;C\:\\Users\\E097600\\Zotero\\storage\\IE8BDPXY\\1904.html}
}

@incollection{tilborghsDiceLossContext2022,
  title = {The {{Dice}} Loss in the Context of Missing or Empty Labels: {{Introducing}} \${{$\Phi$}}\$ and \${$\varepsilon\$$}},
  shorttitle = {The {{Dice}} Loss in the Context of Missing or Empty Labels},
  author = {Tilborghs, Sofie and Bertels, Jeroen and Robben, David and Vandermeulen, Dirk and Maes, Frederik},
  year = 2022,
  volume = {13435},
  eprint = {2207.09521},
  primaryclass = {cs},
  pages = {527--537},
  doi = {10.1007/978-3-031-16443-9_51},
  urldate = {2025-01-20},
  abstract = {Albeit the Dice loss is one of the dominant loss functions in medical image segmentation, most research omits a closer look at its derivative, i.e. the real motor of the optimization when using gradient descent. In this paper, we highlight the peculiar action of the Dice loss in the presence of missing or empty labels. First, we formulate a theoretical basis that gives a general description of the Dice loss and its derivative. It turns out that the choice of the reduction dimensions \$\textbackslash Phi\$ and the smoothing term \$\textbackslash epsilon\$ is non-trivial and greatly influences its behavior. We find and propose heuristic combinations of \$\textbackslash Phi\$ and \$\textbackslash epsilon\$ that work in a segmentation setting with either missing or empty labels. Second, we empirically validate these findings in a binary and multiclass segmentation setting using two publicly available datasets. We confirm that the choice of \$\textbackslash Phi\$ and \$\textbackslash epsilon\$ is indeed pivotal. With \$\textbackslash Phi\$ chosen such that the reductions happen over a single batch (and class) element and with a negligible \$\textbackslash epsilon\$, the Dice loss deals with missing labels naturally and performs similarly compared to recent adaptations specific for missing labels. With \$\textbackslash Phi\$ chosen such that the reductions happen over multiple batch elements or with a heuristic value for \$\textbackslash epsilon\$, the Dice loss handles empty labels correctly. We believe that this work highlights some essential perspectives and hope that it encourages researchers to better describe their exact implementation of the Dice loss in future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\ZNRDPBGY\\Tilborghs et al. - 2022 - The Dice loss in the context of missing or empty labels Introducing $Φ$ and $ε$.pdf;C\:\\Users\\E097600\\Zotero\\storage\\28QJ7R8I\\2207.html}
}

@misc{timansAdaptiveBoundingBox2024,
  title = {Adaptive {{Bounding Box Uncertainties}} via {{Two-Step Conformal Prediction}}},
  author = {Timans, Alexander and Straehle, Christoph-Nikolas and Sakmann, Kaspar and Nalisnick, Eric},
  year = 2024,
  month = jul,
  number = {arXiv:2403.07263},
  eprint = {2403.07263},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.07263},
  urldate = {2025-05-19},
  abstract = {Quantifying a model's predictive uncertainty is essential for safety-critical applications such as autonomous driving. We consider quantifying such uncertainty for multi-object detection. In particular, we leverage conformal prediction to obtain uncertainty intervals with guaranteed coverage for object bounding boxes. One challenge in doing so is that bounding box predictions are conditioned on the object's class label. Thus, we develop a novel two-step conformal approach that propagates uncertainty in predicted class labels into the uncertainty intervals of bounding boxes. This broadens the validity of our conformal coverage guarantees to include incorrectly classified objects, thus offering more actionable safety assurances. Moreover, we investigate novel ensemble and quantile regression formulations to ensure the bounding box intervals are adaptive to object size, leading to a more balanced coverage. Validating our two-step approach on real-world datasets for 2D bounding box localization, we find that desired coverage levels are satisfied with practically tight predictive uncertainty intervals.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\CDDCINLC\\Timans et al. - 2024 - Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction.pdf;C\:\\Users\\E097600\\Zotero\\storage\\VKM8NGJ2\\2403.html}
}

@misc{tommasiDeeperLookDataset2015,
  title = {A {{Deeper Look}} at {{Dataset Bias}}},
  author = {Tommasi, Tatiana and Patricia, Novi and Caputo, Barbara and Tuytelaars, Tinne},
  year = 2015,
  month = may,
  number = {arXiv:1505.01257},
  eprint = {1505.01257},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1505.01257},
  urldate = {2025-10-15},
  abstract = {The presence of a bias in each image data collection has recently attracted a lot of attention in the computer vision community showing the limits in generalization of any learning method trained on a specific dataset. At the same time, with the rapid development of deep learning architectures, the activation values of Convolutional Neural Networks (CNN) are emerging as reliable and robust image descriptors. In this paper we propose to verify the potential of the DeCAF features when facing the dataset bias problem. We conduct a series of analyses looking at how existing datasets differ among each other and verifying the performance of existing debiasing methods under different representations. We learn important lessons on which part of the dataset bias problem can be considered solved and which open questions still need to be tackled.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,domain adaptation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\I4VD3JCI\\Tommasi et al. - 2015 - A Deeper Look at Dataset Bias.pdf;C\:\\Users\\E097600\\Zotero\\storage\\MUCYFAI4\\1505.html}
}

@misc{tsengActiveLearningMethods2025,
  title = {Active {{Learning Methods}} for {{Efficient Data Utilization}} and {{Model Performance Enhancement}}},
  author = {Tseng, Chiung-Yi and Song, Junhao and Bi, Ziqian and Wang, Tianyang and Liang, Chia Xin and Liu, Ming},
  year = 2025,
  month = apr,
  number = {arXiv:2504.16136},
  eprint = {2504.16136},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.16136},
  urldate = {2025-04-25},
  abstract = {In the era of data-driven intelligence, the paradox of data abundance and annotation scarcity has emerged as a critical bottleneck in the advancement of machine learning. This paper gives a detailed overview of Active Learning (AL), which is a strategy in machine learning that helps models achieve better performance using fewer labeled examples. It introduces the basic concepts of AL and discusses how it is used in various fields such as computer vision, natural language processing, transfer learning, and real-world applications. The paper focuses on important research topics such as uncertainty estimation, handling of class imbalance, domain adaptation, fairness, and the creation of strong evaluation metrics and benchmarks. It also shows that learning methods inspired by humans and guided by questions can improve data efficiency and help models learn more effectively. In addition, this paper talks about current challenges in the field, including the need to rebuild trust, ensure reproducibility, and deal with inconsistent methodologies. It points out that AL often gives better results than passive learning, especially when good evaluation measures are used. This work aims to be useful for both researchers and practitioners by providing key insights and proposing directions for future progress in active learning.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,review},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\NPS89EF8\\Tseng et al. - 2025 - Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement.pdf;C\:\\Users\\E097600\\Zotero\\storage\\K4XTMZSS\\2504.html}
}

@article{tulbureReviewModernDefect2022,
  title = {A Review on Modern Defect Detection Models Using {{DCNNs}} -- {{Deep}} Convolutional Neural Networks},
  author = {Tulbure, Andrei-Alexandru and Tulbure, Adrian-Alexandru and Dulf, Eva-Henrietta},
  year = 2022,
  month = jan,
  journal = {Journal of Advanced Research},
  volume = {35},
  pages = {33--48},
  issn = {2090-1232},
  doi = {10.1016/j.jare.2021.03.015},
  urldate = {2024-03-01},
  abstract = {Background Over the last years Deep Learning has shown to yield remarkable results when compared to traditional computer vision algorithms, in a large variety of computer vision applications. The deeplearning models outperformed in both accuracy and processing time. Thus, once a deeplearning models won the Image Net Large Scale Visual Recognition Contest, it proved that this area of research is of great potential. Furthermore, these increases in recognition performance resulted in more applied research and thus, more applications where deeplearning is useful: one of which is defect detection (or visual defect detection). In the last few years, deeplearning models achieved higher and higher accuracy on the complex testing datasets used for benchmarking. This surge in accuracy and usage is also supported (besides swarms of researchers pouring into the race), by incremental breakthroughs in computing hardware: such as more powerful GPUs(Graphical processing units), CPUs(central processing units) and better computing procedures (libraries and frameworks). Aim of the review To offer a structured and analytical overview(stating both advantages and disadvantages) of the existing popular object detection models that can be re-purposed for defect detection: such as Region based CNNs(Convolutional neural networks), YOLO(You only look once), SSD(single shot detectors) and cascaded architectures. A further brief summary on model compression and acceleration techniques that enabled the portability of deeplearning detection models is included. Key Scientific Concepts of Review It is of great use for future developments in the manufacturing industry that many of the popular, above mentioned models are easy to re-purpose for defect detection and, thus could really contribute to the overall increase in productivity of this sector. Moreover, in the experiment performed the YOLOv4 model was trained and re-purposed for industrial cable detection in several hours. The computing needs could be fulfilled by a general purpose computer or by a high-performance desktop setup, depending on the specificity of the application. Hence, the barrier of computing shall be somewhat easier to climb for all types of businesses.},
  keywords = {Deep convolutional neural networks,Deeplearning,Defect detection,Image classification,Object detection},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\LQE58CGK\\Tulbure et al. - 2022 - A review on modern defect detection models using D.pdf;C\:\\Users\\E097600\\Zotero\\storage\\M9K3R5GG\\S2090123221000643.html}
}

@article{valiantTheoryLearnable1984,
  title = {A Theory of the Learnable},
  author = {Valiant, L. G.},
  year = 1984,
  month = nov,
  journal = {Commun. ACM},
  volume = {27},
  number = {11},
  pages = {1134--1142},
  issn = {0001-0782},
  doi = {10.1145/1968.1972},
  urldate = {2024-12-23},
  file = {C:\Users\E097600\Zotero\storage\NAKWSXHN\Valiant - 1984 - A theory of the learnable.pdf}
}

@article{vandengoorberghHarmClassImbalance2022,
  title = {The Harm of Class Imbalance Corrections for Risk Prediction Models: Illustration and Simulation Using Logistic Regression},
  shorttitle = {The Harm of Class Imbalance Corrections for Risk Prediction Models},
  author = {{van den Goorbergh}, Ruben and {van Smeden}, Maarten and Timmerman, Dirk and Van Calster, Ben},
  year = 2022,
  month = aug,
  journal = {Journal of the American Medical Informatics Association: JAMIA},
  volume = {29},
  number = {9},
  pages = {1525--1534},
  issn = {1527-974X},
  doi = {10.1093/jamia/ocac093},
  abstract = {OBJECTIVE: Methods to correct class imbalance (imbalance between the frequency of outcome events and nonevents) are receiving increasing interest for developing prediction models. We examined the effect of imbalance correction on the performance of logistic regression models. MATERIAL AND METHODS: Prediction models were developed using standard and penalized (ridge) logistic regression under 4 methods to address class imbalance: no correction, random undersampling, random oversampling, and SMOTE. Model performance was evaluated in terms of discrimination, calibration, and classification. Using Monte Carlo simulations, we studied the impact of training set size, number of predictors, and the outcome event fraction. A case study on prediction modeling for ovarian cancer diagnosis is presented. RESULTS: The use of random undersampling, random oversampling, or SMOTE yielded poorly calibrated models: the probability to belong to the minority class was strongly overestimated. These methods did not result in higher areas under the ROC curve when compared with models developed without correction for class imbalance. Although imbalance correction improved the balance between sensitivity and specificity, similar results were obtained by shifting the probability threshold instead. DISCUSSION: Imbalance correction led to models with strong miscalibration without better ability to distinguish between patients with and without the outcome event. The inaccurate probability estimates reduce the clinical utility of the model, because decisions about treatment are ill-informed. CONCLUSION: Outcome imbalance is not a problem in itself, imbalance correction may even worsen model performance.},
  langid = {english},
  pmcid = {PMC9382395},
  pmid = {35686364},
  keywords = {calibration,Calibration,class imbalance,Class imbalance,classification,Computer Simulation,Humans,Logistic Models,logistic regression,Probability,ROC Curve,Sensitivity and Specificity,synthetic minority oversampling technique,undersampling},
  file = {C:\Users\E097600\Zotero\storage\KQIV9877\van den Goorbergh et al. - 2022 - The harm of class imbalance corrections for risk prediction models illustration and simulation usin.pdf}
}

@misc{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = 2023,
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2025-04-05},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\LS7T4YMQ\\Vaswani et al. - 2023 - Attention Is All You Need.pdf;C\:\\Users\\E097600\\Zotero\\storage\\KI3LB452\\1706.html}
}

@misc{vettoruzzoAdvancesChallengesMetaLearning2023,
  title = {Advances and {{Challenges}} in {{Meta-Learning}}: {{A Technical Review}}},
  shorttitle = {Advances and {{Challenges}} in {{Meta-Learning}}},
  author = {Vettoruzzo, Anna and Bouguelia, Mohamed-Rafik and Vanschoren, Joaquin and R{\"o}gnvaldsson, Thorsteinn and Santosh, K. C.},
  year = 2023,
  month = jul,
  number = {arXiv:2307.04722},
  eprint = {2307.04722},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.04722},
  urldate = {2024-03-12},
  abstract = {Meta-learning empowers learning systems with the ability to acquire knowledge from multiple tasks, enabling faster adaptation and generalization to new tasks. This review provides a comprehensive technical overview of meta-learning, emphasizing its importance in real-world applications where data may be scarce or expensive to obtain. The paper covers the state-of-the-art meta-learning approaches and explores the relationship between meta-learning and multi-task learning, transfer learning, domain adaptation and generalization, self-supervised learning, personalized federated learning, and continual learning. By highlighting the synergies between these topics and the field of meta-learning, the paper demonstrates how advancements in one area can benefit the field as a whole, while avoiding unnecessary duplication of efforts. Additionally, the paper delves into advanced meta-learning topics such as learning from complex multi-modal task distributions, unsupervised meta-learning, learning to efficiently adapt to data distribution shifts, and continual meta-learning. Lastly, the paper highlights open problems and challenges for future research in the field. By synthesizing the latest research developments, this paper provides a thorough understanding of meta-learning and its potential impact on various machine learning applications. We believe that this technical overview will contribute to the advancement of meta-learning and its practical implications in addressing real-world problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\MU46GSPU\\Vettoruzzo et al. - 2023 - Advances and Challenges in Meta-Learning A Techni.pdf;C\:\\Users\\E097600\\Zotero\\storage\\TFP9EYLY\\2307.html}
}

@misc{VisionTransformersViT,
  title = {Vision {{Transformers}} ({{ViT}}) {{Explained}} \textbar{} {{Pinecone}}},
  urldate = {2024-06-28},
  howpublished = {https://www.pinecone.io/learn/series/image-search/vision-transformers/},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\8E3KUSKV\vision-transformers.html}
}

@misc{voActiveLearningMulticlass2025,
  title = {Active {{Learning}} for {{Multi-class Image Classification}}},
  author = {Vo, Thien Nhan},
  year = 2025,
  month = may,
  number = {arXiv:2505.06825},
  eprint = {2505.06825},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.06825},
  urldate = {2025-05-13},
  abstract = {A principle bottleneck in image classification is the large number of training examples needed to train a classifier. Using active learning, we can reduce the number of training examples to teach a CNN classifier by strategically selecting examples. Assigning values to image examples using different uncertainty metrics allows the model to identify and select high-value examples in a smaller training set size. We demonstrate results for digit recognition and fruit classification on the MNIST and Fruits360 data sets. We formally compare results for four different uncertainty metrics. Finally, we observe active learning is also effective on simpler (binary) classification tasks, but marked improvement from random sampling is more evident on more difficult tasks. We show active learning is a viable algorithm for image classification problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\Y99WTUPZ\\Vo - 2025 - Active Learning for Multi-class Image Classification.pdf;C\:\\Users\\E097600\\Zotero\\storage\\R7U39CXF\\2505.html}
}

@misc{voVisionLanguageModels2025,
  title = {Vision {{Language Models}} Are {{Biased}}},
  author = {Vo, An and Nguyen, Khai-Nguyen and Taesiri, Mohammad Reza and Dang, Vy Tuong and Nguyen, Anh Totti and Kim, Daeyoung},
  year = 2025,
  month = may,
  journal = {arXiv.org},
  urldate = {2025-06-02},
  abstract = {Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that help them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of 17.05\% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, board games, optical illusions, to patterned grids. Insert text (e.g., "Adidas") describing the subject name into the counterfactual image further decreases VLM accuracy. The biases in VLMs are so strong that instructing them to double-check their results or rely exclusively on image details to answer improves counting accuracy by only +2 points, on average. Our work presents an interesting failure mode in VLMs and an automated framework for testing VLM biases. Code and data are available at: vlmsarebiased.github.io.},
  howpublished = {https://arxiv.org/abs/2505.23941v1},
  langid = {english},
  keywords = {Robustness,VLM},
  file = {C:\Users\E097600\Zotero\storage\4YFFQQCY\Vo et al. - 2025 - Vision Language Models are Biased.pdf}
}

@book{vovkAlgorithmicLearningRandom2022,
  title = {Algorithmic {{Learning}} in a {{Random World}}},
  author = {Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn},
  year = 2022,
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-06649-8},
  urldate = {2024-06-28},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-031-06648-1 978-3-031-06649-8},
  langid = {english},
  keywords = {book,Conformal prediction,conformal predictive distributions,conformal testing,nonparametric statistics,online compression modeling,Venn prediction},
  file = {C:\Users\E097600\Zotero\storage\WZ59D4HK\Vovk et al. - 2022 - Algorithmic Learning in a Random World.pdf}
}

@misc{wanFoundationModelInsights2025,
  title = {Foundation {{Model Insights}} and a {{Multi-Model Approach}} for {{Superior Fine-Grained One-shot Subset Selection}}},
  author = {Wan, Zhijing and Wang, Zhixiang and Wang, Zheng and Xu, Xin and Satoh, Shin'ichi},
  year = 2025,
  month = jun,
  journal = {arXiv.org},
  urldate = {2025-06-18},
  abstract = {One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.},
  howpublished = {https://arxiv.org/abs/2506.14473v1},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\MLZ8B972\Wan et al. - 2025 - Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selec.pdf}
}

@article{wang$A2$DPAnnotationawareData2024,
  title = {\${{A2}}\$-{{DP}}: {{Annotation-aware Data Pruning}} for {{Object Detection}}},
  shorttitle = {\${{A2}}\$-{{DP}}},
  author = {Wang, Zengran and Zhang, Yanan and Qi, Yunlong and Chen, Jiaxin and Fu, Zehua and Huang, Di},
  year = 2024,
  month = oct,
  urldate = {2025-05-21},
  abstract = {As the size of datasets for training deep neural networks expands, data pruning has become an intriguing area of research due to its ability to achieve lossless performance with a reduced overall data volume. However, traditional data pruning usually demands complete dataset annotations, incurring high costs. To tackle this, we propose an innovative Annotation-Aware Data Pruning paradigm tailored for object detection, dubbed as \$A\textasciicircum 2\$-DP, which aims to reduce the burdens of both annotation and storage. Our approach, consisting of two phases, integrates a hard sample mining module to extract crucial hidden objects, a class balance module to identify important objects in rare or challenging classes and a global similarity removal module that enhances the elimination of redundant information through object-level similarity assessments. Extensive experiments on 2D and 3D detection tasks validate the effectiveness of the \$A\textasciicircum 2\$-DP, consistently achieving a minimum pruning rate of 20\textbackslash\% across various datasets, showcasing the practical value and efficiency of our methods.},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\RCJ5STF6\Wang et al. - 2024 - $A^2$-DP Annotation-aware Data Pruning for Object Detection.pdf}
}

@article{wangAleatoricUncertaintyEstimation2019,
  title = {Aleatoric Uncertainty Estimation with Test-Time Augmentation for Medical Image Segmentation with Convolutional Neural Networks},
  author = {Wang, Guotai and Li, Wenqi and Aertsen, Michael and Deprest, Jan and Ourselin, S{\'e}bastien and Vercauteren, Tom},
  year = 2019,
  month = apr,
  journal = {Neurocomputing},
  volume = {338},
  pages = {34--45},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.01.103},
  urldate = {2025-10-23},
  abstract = {Despite the state-of-the-art performance for medical image segmentation, deep convolutional neural networks (CNNs) have rarely provided uncertainty estimations regarding their segmentation outputs, e.g., model (epistemic) and image-based (aleatoric) uncertainties. In this work, we analyze these different types of uncertainties for CNN-based 2D and 3D medical image segmentation tasks at both pixel level and structure level. We additionally propose a test-time augmentation-based aleatoric uncertainty to analyze the effect of different transformations of the input image on the segmentation output. Test-time augmentation has been previously used to improve segmentation accuracy, yet not been formulated in a consistent mathematical framework. Hence, we also propose a theoretical formulation of test-time augmentation, where a distribution of the prediction is estimated by Monte Carlo simulation with prior distributions of parameters in an image acquisition model that involves image transformations and noise. We compare and combine our proposed aleatoric uncertainty with model uncertainty. Experiments with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic Resonance Images (MRI) showed that 1) the test-time augmentation-based aleatoric uncertainty provides a better uncertainty estimation than calculating the test-time dropout-based model uncertainty alone and helps to reduce overconfident incorrect predictions, and 2) our test-time augmentation outperforms a single-prediction baseline and dropout-based multiple predictions.},
  keywords = {Convolutional neural networks,Data augmentation,Medical image segmentation,Uncertainty,Uncertainty estimation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\S9KJBH4L\\Wang et al. - 2019 - Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with con.pdf;C\:\\Users\\E097600\\Zotero\\storage\\6GCAPMXA\\S0925231219301961.html}
}

@misc{wangCalibratingSemanticSegmentation2023,
  title = {On {{Calibrating Semantic Segmentation Models}}: {{Analyses}} and {{An Algorithm}}},
  shorttitle = {On {{Calibrating Semantic Segmentation Models}}},
  author = {Wang, Dongdong and Gong, Boqing and Wang, Liqiang},
  year = 2023,
  month = mar,
  number = {arXiv:2212.12053},
  eprint = {2212.12053},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.12053},
  urldate = {2025-01-21},
  abstract = {We study the problem of semantic segmentation calibration. Lots of solutions have been proposed to approach model miscalibration of confidence in image classification. However, to date, confidence calibration research on semantic segmentation is still limited. We provide a systematic study on the calibration of semantic segmentation models and propose a simple yet effective approach. First, we find that model capacity, crop size, multi-scale testing, and prediction correctness have impact on calibration. Among them, prediction correctness, especially misprediction, is more important to miscalibration due to over-confidence. Next, we propose a simple, unifying, and effective approach, namely selective scaling, by separating correct/incorrect prediction for scaling and more focusing on misprediction logit smoothing. Then, we study popular existing calibration methods and compare them with selective scaling on semantic segmentation calibration. We conduct extensive experiments with a variety of benchmarks on both in-domain and domain-shift calibration and show that selective scaling consistently outperforms other methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\7PS8KIWH\\Wang et al. - 2023 - On Calibrating Semantic Segmentation Models Analyses and An Algorithm.pdf;C\:\\Users\\E097600\\Zotero\\storage\\C59Y7JRK\\2212.html}
}

@misc{wangCalibratingSemanticSegmentation2023a,
  title = {On {{Calibrating Semantic Segmentation Models}}: {{Analyses}} and {{An Algorithm}}},
  shorttitle = {On {{Calibrating Semantic Segmentation Models}}},
  author = {Wang, Dongdong and Gong, Boqing and Wang, Liqiang},
  year = 2023,
  month = mar,
  number = {arXiv:2212.12053},
  eprint = {2212.12053},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.12053},
  urldate = {2025-11-28},
  abstract = {We study the problem of semantic segmentation calibration. Lots of solutions have been proposed to approach model miscalibration of confidence in image classification. However, to date, confidence calibration research on semantic segmentation is still limited. We provide a systematic study on the calibration of semantic segmentation models and propose a simple yet effective approach. First, we find that model capacity, crop size, multi-scale testing, and prediction correctness have impact on calibration. Among them, prediction correctness, especially misprediction, is more important to miscalibration due to over-confidence. Next, we propose a simple, unifying, and effective approach, namely selective scaling, by separating correct/incorrect prediction for scaling and more focusing on misprediction logit smoothing. Then, we study popular existing calibration methods and compare them with selective scaling on semantic segmentation calibration. We conduct extensive experiments with a variety of benchmarks on both in-domain and domain-shift calibration and show that selective scaling consistently outperforms other methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\T7JBPM26\\Wang et al. - 2023 - On Calibrating Semantic Segmentation Models Analyses and An Algorithm.pdf;C\:\\Users\\E097600\\Zotero\\storage\\EMTN95SU\\2212.html}
}

@misc{wangComprehensiveSurveyContinual2024,
  title = {A {{Comprehensive Survey}} of {{Continual Learning}}: {{Theory}}, {{Method}} and {{Application}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Continual Learning}}},
  author = {Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  year = 2024,
  month = feb,
  number = {arXiv:2302.00487},
  eprint = {2302.00487},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00487},
  urldate = {2025-03-26},
  abstract = {To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative methods address continual learning, and how they are adapted to particular challenges in realistic applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\39D99WAA\\Wang et al. - 2024 - A Comprehensive Survey of Continual Learning Theory, Method and Application.pdf;C\:\\Users\\E097600\\Zotero\\storage\\LVWZWNNU\\2302.html}
}

@article{wangFeatureHallucinationSelfsupervised2025,
  title = {Feature {{Hallucination}} for {{Self-supervised Action Recognition}}},
  author = {Wang, Lei and Koniusz, Piotr},
  year = 2025,
  month = aug,
  journal = {International Journal of Computer Vision},
  issn = {1573-1405},
  doi = {10.1007/s11263-025-02513-4},
  urldate = {2025-09-22},
  abstract = {Understanding human actions in videos requires more than raw pixel analysis; it relies on high-level semantic reasoning and effective integration of multimodal features. We propose a deep translational action recognition framework that enhances recognition accuracy by jointly predicting action concepts and auxiliary features from RGB video frames. At test time, hallucination streams infer missing cues, enriching feature representations without increasing computational overhead. To focus on action-relevant regions beyond raw pixels, we introduce two novel domain-specific descriptors. Object Detection Features (ODF) aggregate outputs from multiple object detectors to capture contextual cues, while Saliency Detection Features (SDF) highlight spatial and intensity patterns crucial for action recognition. Our framework seamlessly integrates these descriptors with auxiliary modalities such as optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It remains compatible with state-of-the-art architectures, including I3D, AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE V2 and InternVideo2. To handle uncertainty in auxiliary features, we incorporate aleatoric uncertainty modeling in the hallucination step and introduce a robust loss function to mitigate feature noise. Our multimodal self-supervised action recognition framework achieves state-of-the-art performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and Something-Something V2, demonstrating its effectiveness in capturing fine-grained action dynamics.},
  langid = {english},
  keywords = {Action recognition,Audio,Domain-specific descriptor,Feature hallucination,Fine-grained recognition,Multimodal,Object detection,Optical flow,Saliency detection,Self-supervision,Skeleton,Uncertainty modeling},
  file = {C:\Users\E097600\Zotero\storage\TTUCR3Y9\Wang et Koniusz - 2025 - Feature Hallucination for Self-supervised Action Recognition.pdf}
}

@misc{wangJaccardMetricLosses2024,
  title = {Jaccard {{Metric Losses}}: {{Optimizing}} the {{Jaccard Index}} with {{Soft Labels}}},
  shorttitle = {Jaccard {{Metric Losses}}},
  author = {Wang, Zifu and Ning, Xuefei and Blaschko, Matthew B.},
  year = 2024,
  month = mar,
  number = {arXiv:2302.05666},
  eprint = {2302.05666},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.05666},
  urldate = {2025-01-17},
  abstract = {Intersection over Union (IoU) losses are surrogates that directly optimize the Jaccard index. Leveraging IoU losses as part of the loss function have demonstrated superior performance in semantic segmentation tasks compared to optimizing pixel-wise losses such as the cross-entropy loss alone. However, we identify a lack of flexibility in these losses to support vital training techniques like label smoothing, knowledge distillation, and semi-supervised learning, mainly due to their inability to process soft labels. To address this, we introduce Jaccard Metric Losses (JMLs), which are identical to the soft Jaccard loss in standard settings with hard labels but are fully compatible with soft labels. We apply JMLs to three prominent use cases of soft labels: label smoothing, knowledge distillation and semi-supervised learning, and demonstrate their potential to enhance model accuracy and calibration. Our experiments show consistent improvements over the cross-entropy loss across 4 semantic segmentation datasets (Cityscapes, PASCAL VOC, ADE20K, DeepGlobe Land) and 13 architectures, including classic CNNs and recent vision transformers. Remarkably, our straightforward approach significantly outperforms state-of-the-art knowledge distillation and semi-supervised learning methods. The code is available at \textbackslash href\textbraceleft https://github.com/zifuwanggg/JDTLosses\textbraceright\textbraceleft https://github.com/zifuwanggg/JDTLosses\textbraceright.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\JVUVSWBL\\Wang et al. - 2024 - Jaccard Metric Losses Optimizing the Jaccard Index with Soft Labels.pdf;C\:\\Users\\E097600\\Zotero\\storage\\MCQTLYXY\\2302.html}
}

@misc{wangMultiSimilarityLossGeneral2020,
  title = {Multi-{{Similarity Loss}} with {{General Pair Weighting}} for {{Deep Metric Learning}}},
  author = {Wang, Xun and Han, Xintong and Huang, Weilin and Dong, Dengke and Scott, Matthew R.},
  year = 2020,
  month = mar,
  number = {arXiv:1904.06627},
  eprint = {1904.06627},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.06627},
  urldate = {2024-03-12},
  abstract = {A family of loss functions built on pair-based computation have been proposed in the literature which provide a myriad of solutions for deep metric learning. In this paper, we provide a general weighting framework for understanding recent pair-based loss functions. Our contributions are three-fold: (1) we establish a General Pair Weighting (GPW) framework, which casts the sampling problem of deep metric learning into a unified view of pair weighting through gradient analysis, providing a powerful tool for understanding recent pair-based loss functions; (2) we show that with GPW, various existing pair-based methods can be compared and discussed comprehensively, with clear differences and key limitations identified; (3) we propose a new loss called multi-similarity loss (MS loss) under the GPW, which is implemented in two iterative steps (i.e., mining and weighting). This allows it to fully consider three similarities for pair weighting, providing a more principled approach for collecting and weighting informative pairs. Finally, the proposed MS loss obtains new state-of-the-art performance on four image retrieval benchmarks, where it outperforms the most recent approaches, such as ABE\textbackslash cite\textbraceleft Kim\_2018\_ECCV\textbraceright{} and HTL by a large margin: 60.6\% to 65.7\% on CUB200, and 80.9\% to 88.0\% on In-Shop Clothes Retrieval dataset at Recall@1. Code is available at https://github.com/MalongTech/research-ms-loss.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\IWIQNDNA\\Wang et al. - 2020 - Multi-Similarity Loss with General Pair Weighting .pdf;C\:\\Users\\E097600\\Zotero\\storage\\65CYKPQD\\1904.html}
}

@inproceedings{wangNewActiveLabeling2014,
  title = {A New Active Labeling Method for Deep Learning},
  booktitle = {2014 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Wang, Dan and Shang, Yi},
  year = 2014,
  month = jul,
  pages = {112--119},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2014.6889457},
  urldate = {2025-02-06},
  abstract = {Deep learning has been shown to achieve outstanding performance in a number of challenging real-world applications. However, most of the existing works assume a fixed set of labeled data, which is not necessarily true in real-world applications. Getting labeled data is usually expensive and time consuming. Active labelling in deep learning aims at achieving the best learning result with a limited labeled data set, i.e., choosing the most appropriate unlabeled data to get labeled. This paper presents a new active labeling method, AL-DL, for cost-effective selection of data to be labeled. AL-DL uses one of three metrics for data selection: least confidence, margin sampling, and entropy. The method is applied to deep learning networks based on stacked restricted Boltzmann machines, as well as stacked autoencoders. In experiments on the MNIST benchmark dataset, the method outperforms random labeling consistently by a significant margin.},
  keywords = {Classification algorithms,Entropy,Labeling,Measurement,Neural networks,Training,Uncertainty},
  file = {C:\Users\E097600\Zotero\storage\BE59552N\6889457.html}
}

@misc{wangSemisupervisedActiveLearning2020,
  title = {Semi-Supervised {{Active Learning}} for {{Instance Segmentation}} via {{Scoring Predictions}}},
  author = {Wang, Jun and Wen, Shaoguo and Chen, Kaixing and Yu, Jianghua and Zhou, Xin and Gao, Peng and Li, Changsheng and Xie, Guotong},
  year = 2020,
  month = dec,
  number = {arXiv:2012.04829},
  eprint = {2012.04829},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.04829},
  urldate = {2025-02-20},
  abstract = {Active learning generally involves querying the most representative samples for human labeling, which has been widely studied in many fields such as image classification and object detection. However, its potential has not been explored in the more complex instance segmentation task that usually has relatively higher annotation cost. In this paper, we propose a novel and principled semi-supervised active learning framework for instance segmentation. Specifically, we present an uncertainty sampling strategy named Triplet Scoring Predictions (TSP) to explicitly incorporate samples ranking clues from classes, bounding boxes and masks. Moreover, we devise a progressive pseudo labeling regime using the above TSP in semi-supervised manner, it can leverage both the labeled and unlabeled data to minimize labeling effort while maximize performance of instance segmentation. Results on medical images datasets demonstrate that the proposed method results in the embodiment of knowledge from available data in a meaningful way. The extensive quantitatively and qualitatively experiments show that, our method can yield the best-performing model with notable less annotation costs, compared with state-of-the-arts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\VNZYLAA9\\Wang et al. - 2020 - Semi-supervised Active Learning for Instance Segmentation via Scoring Predictions.pdf;C\:\\Users\\E097600\\Zotero\\storage\\XUYZNRBD\\2012.html}
}

@inproceedings{wangTrainingDeepNeural2016,
  title = {Training Deep Neural Networks on Imbalanced Data Sets},
  booktitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Wang, Shoujin and Liu, Wei and Wu, Jia and Cao, Longbing and Meng, Qinxue and Kennedy, Paul J.},
  year = 2016,
  month = jul,
  pages = {4368--4374},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2016.7727770},
  urldate = {2025-02-11},
  abstract = {Deep learning has become increasingly popular in both academic and industrial areas in the past years. Various domains including pattern recognition, computer vision, and natural language processing have witnessed the great power of deep networks. However, current studies on deep learning mainly focus on data sets with balanced class labels, while its performance on imbalanced data is not well examined. Imbalanced data sets exist widely in real world and they have been providing great challenges for classification tasks. In this paper, we focus on the problem of classification using deep network on imbalanced data sets. Specifically, a novel loss function called mean false error together with its improved version mean squared false error are proposed for the training of deep networks on imbalanced data sets. The proposed method can effectively capture classification errors from both majority class and minority class equally. Experiments and comparisons demonstrate the superiority of the proposed approach compared with conventional methods in classifying imbalanced data sets on deep neural networks.},
  keywords = {Australia,Class imbalance,Classification algorithms,data imbalance,loss function,Machine Learning,Neural networks,Sampling methods,Standards,Training},
  file = {C:\Users\E097600\Zotero\storage\BM8YY2HI\7727770.html}
}

@misc{WhatFewshotLearning,
  title = {What Is Few-Shot Learning? \textbar{} {{IBM}}},
  shorttitle = {What Is Few-Shot Learning?},
  urldate = {2024-03-06},
  abstract = {Few-shot learning is a machine learning framework in which an AI model learns to make accurate predictions by training on a very small number of labeled samples.},
  howpublished = {https://www.ibm.com/topics/few-shot-learning},
  langid = {american}
}

@misc{WikiwandCosineSimilarity,
  title = {Wikiwand - {{Cosine}} Similarity},
  journal = {Wikiwand},
  urldate = {2024-03-12},
  abstract = {In data analysis, cosine similarity is a measure of similarity between two non-zero vectors defined in an inner product space. Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths. It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle. The cosine similarity always belongs to the interval  For example, two proportional vectors have a cosine similarity of 1, two orthogonal vectors have a similarity of 0, and two opposite vectors have a similarity of -1. In some contexts, the component values of the vectors cannot be negative, in which case the cosine similarity is bounded in .},
  howpublished = {https://www.wikiwand.com/en/Cosine\_similarity},
  file = {C:\Users\E097600\Zotero\storage\QEFVYYVU\Cosine_similarity.html}
}

@misc{wooActiveLearningBayesian2023,
  title = {Active {{Learning}} in {{Bayesian Neural Networks}} with {{Balanced Entropy Learning Principle}}},
  author = {Woo, Jae Oh},
  year = 2023,
  month = apr,
  number = {arXiv:2105.14559},
  eprint = {2105.14559},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.14559},
  urldate = {2025-10-28},
  abstract = {Acquiring labeled data is challenging in many machine learning applications with limited budgets. Active learning gives a procedure to select the most informative data points and improve data efficiency by reducing the cost of labeling. The info-max learning principle maximizing mutual information such as BALD has been successful and widely adapted in various active learning applications. However, this pool-based specific objective inherently introduces a redundant selection and further requires a high computational cost for batch selection. In this paper, we design and propose a new uncertainty measure, Balanced Entropy Acquisition (BalEntAcq), which captures the information balance between the uncertainty of underlying softmax probability and the label variable. To do this, we approximate each marginal distribution by Beta distribution. Beta approximation enables us to formulate BalEntAcq as a ratio between an augmented entropy and the marginalized joint entropy. The closed-form expression of BalEntAcq facilitates parallelization by estimating two parameters in each marginal Beta distribution. BalEntAcq is a purely standalone measure without requiring any relational computations with other data points. Nevertheless, BalEntAcq captures a well-diversified selection near the decision boundary with a margin, unlike other existing uncertainty measures such as BALD, Entropy, or Mean Standard Deviation (MeanSD). Finally, we demonstrate that our balanced entropy learning principle with BalEntAcq consistently outperforms well-known linearly scalable active learning methods, including a recently proposed PowerBALD, a simple but diversified version of BALD, by showing experimental results obtained from MNIST, CIFAR-100, SVHN, and TinyImageNet datasets.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,Image classification,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\RX9FY6HR\\Woo - 2023 - Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle.pdf;C\:\\Users\\E097600\\Zotero\\storage\\K2UB7CJI\\2105.html}
}

@article{wuAutomatedMetadataAnnotation2023,
  title = {Automated Metadata Annotation: {{What}} Is and Is Not Possible with Machine Learning},
  shorttitle = {Automated Metadata Annotation},
  author = {Wu, Mingfang and Brandhorst, Hans and Marinescu, Maria-Cristina and Lopez, Joaquim More and Hlava, Margorie and Busch, Joseph},
  year = 2023,
  month = mar,
  journal = {Data Intelligence},
  volume = {5},
  number = {1},
  pages = {122--138},
  issn = {2641-435X},
  doi = {10.1162/dint_a_00162},
  urldate = {2025-11-17},
  abstract = {Automated metadata annotation is only as good as training dataset, or rules that are available for the domain. It's important to learn what type of data content a pre-trained machine learning algorithm has been trained on to understand its limitations and potential biases. Consider what type of content is readily available to train an algorithm---what's popular and what's available. However, scholarly and historical content is often not available in consumable, homogenized, and interoperable formats at the large volume that is required for machine learning. There are exceptions such as science and medicine, where large, well documented collections are available. This paper presents the current state of automated metadata annotation in cultural heritage and research data, discusses challenges identified from use cases, and proposes solutions.},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\CES3TAKX\\Wu et al. - 2023 - Automated metadata annotation What is and is not possible with machine learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\9Y66KE7H\\dint_a_00162.html}
}

@misc{wuRethinkingClassImbalance2023,
  title = {Rethinking {{Class Imbalance}} in {{Machine Learning}}},
  author = {Wu, Ou},
  year = 2023,
  month = may,
  number = {arXiv:2305.03900},
  eprint = {2305.03900},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.03900},
  urldate = {2025-09-18},
  abstract = {Imbalance learning is a subfield of machine learning that focuses on learning tasks in the presence of class imbalance. Nearly all existing studies refer to class imbalance as a proportion imbalance, where the proportion of training samples in each class is not balanced. The ignorance of the proportion imbalance will result in unfairness between/among classes and poor generalization capability. Previous literature has presented numerous methods for either theoretical/empirical analysis or new methods for imbalance learning. This study presents a new taxonomy of class imbalance in machine learning with a broader scope. Four other types of imbalance, namely, variance, distance, neighborhood, and quality imbalances between/among classes, which may exist in machine learning tasks, are summarized. Two different levels of imbalance including global and local are also presented. Theoretical analysis is used to illustrate the significant impact of the new imbalance types on learning fairness. Moreover, our taxonomy and theoretical conclusions are used to analyze the shortcomings of several classical methods. As an example, we propose a new logit perturbation-based imbalance learning loss when proportion, variance, and distance imbalances exist simultaneously. Several classical losses become the special case of our proposed method. Meta learning is utilized to infer the hyper-parameters related to the three types of imbalance. Experimental results on several benchmark corpora validate the effectiveness of the proposed method.},
  archiveprefix = {arXiv},
  keywords = {Class imbalance,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\3EAI45TW\\Wu - 2023 - Rethinking Class Imbalance in Machine Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\PBXV8B4R\\2305.html}
}

@article{xiaoTransformersMedicalImage2023,
  title = {Transformers in Medical Image Segmentation: {{A}} Review},
  shorttitle = {Transformers in Medical Image Segmentation},
  author = {Xiao, Hanguang and Li, Li and Liu, Qiyuan and Zhu, Xiuhong and Zhang, Qihang},
  year = 2023,
  month = jul,
  journal = {Biomedical Signal Processing and Control},
  volume = {84},
  pages = {104791},
  issn = {1746-8094},
  doi = {10.1016/j.bspc.2023.104791},
  urldate = {2025-01-24},
  abstract = {Background and Objectives: Transformer is a model relying entirely on self-attention which has a wide range of applications in the field of natural language processing. Researchers are beginning to focus on the transformer in medical images due to the past few years having seen the rapid development of transformer in many vision fields such as vision transformer (ViT) and Swin transformer. In the last year, moreover, many scholars have applied transformer to medical image segmentation and have achieved good segmentation results. Transformer-based medical image segmentation has become one of the hot spots in this field. The purpose of this work is to categorize and review the segmentation methods of Unet-based transformer and other model based transformer in medical images. Methods: This paper summarizes the transformer-based segmentation models in the abdominal organs, heart, brain, and lung based on the relevant studies in the last two years. We described and analyzed the model structure including the position of the transformer in the model, the changes made by scholars to transformer and the combination with the model. In this work, the segmentation performance results based on Dice evaluation metrics are compared. Results: Through the help of 93 references, we find that researchers prefer to use Unet-based transformer models than others and place the transformer structure in the encoder. These new models improve the segmentation performance compared with U-Net and other segmentation models. However, there are not many related studies on lungs, which points to a new way for future research. Conclusions: We found that the combination of U-Net and transformer is more suitable for segmentation. In future research on medical image segmentation, researchers can use a suitable transformer-based segmentation method or modify the transformer structure according to the segmentation requirements. We hope that this work will be helpful for improvements of the transformer to solve clinical problems in medicine.},
  keywords = {3D segmentation,Medical image,Segmentation analysis,Transformer},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\XXFSD2KP\\Xiao et al. - 2023 - Transformers in medical image segmentation A review.pdf;C\:\\Users\\E097600\\Zotero\\storage\\EAT5LWMD\\S1746809423002240.html}
}

@misc{xieActiveLearningDomain2022,
  title = {Active {{Learning}} for {{Domain Adaptation}}: {{An Energy-Based Approach}}},
  shorttitle = {Active {{Learning}} for {{Domain Adaptation}}},
  author = {Xie, Binhui and Yuan, Longhui and Li, Shuang and Liu, Chi Harold and Cheng, Xinjing and Wang, Guoren},
  year = 2022,
  month = mar,
  number = {arXiv:2112.01406},
  eprint = {2112.01406},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.01406},
  urldate = {2025-05-27},
  abstract = {Unsupervised domain adaptation has recently emerged as an effective paradigm for generalizing deep neural networks to new target domains. However, there is still enormous potential to be tapped to reach the fully supervised performance. In this paper, we present a novel active learning strategy to assist knowledge transfer in the target domain, dubbed active domain adaptation. We start from an observation that energy-based models exhibit \textbackslash textit\textbraceleft free energy biases\textbraceright{} when training (source) and test (target) data come from different distributions. Inspired by this inherent mechanism, we empirically reveal that a simple yet efficient energy-based sampling strategy sheds light on selecting the most valuable target samples than existing approaches requiring particular architectures or computation of the distances. Our algorithm, Energy-based Active Domain Adaptation (EADA), queries groups of target data that incorporate both domain characteristic and instance uncertainty into every selection round. Meanwhile, by aligning the free energy of target data compact around the source domain via a regularization term, domain gap can be implicitly diminished. Through extensive experiments, we show that EADA surpasses state-of-the-art methods on well-known challenging benchmarks with substantial improvements, making it a useful option in the open world. Code is available at https://github.com/BIT-DA/EADA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\6ZJREY4H\\Xie et al. - 2022 - Active Learning for Domain Adaptation An Energy-Based Approach.pdf;C\:\\Users\\E097600\\Zotero\\storage\\LSR6UA2G\\2112.html}
}

@misc{xieDataAttributionDiffusion2024,
  title = {Data {{Attribution}} for {{Diffusion Models}}: {{Timestep-induced Bias}} in {{Influence Estimation}}},
  shorttitle = {Data {{Attribution}} for {{Diffusion Models}}},
  author = {Xie, Tong and Li, Haoyu and Bai, Andrew and Hsieh, Cho-Jui},
  year = 2024,
  month = jan,
  journal = {arXiv.org},
  urldate = {2025-05-07},
  abstract = {Data attribution methods trace model behavior back to its training dataset, offering an effective approach to better understand ''black-box'' neural networks. While prior research has established quantifiable links between model output and training data in diverse settings, interpreting diffusion model outputs in relation to training samples remains underexplored. In particular, diffusion models operate over a sequence of timesteps instead of instantaneous input-output relationships in previous contexts, posing a significant challenge to extend existing frameworks to diffusion models directly. Notably, we present Diffusion-TracIn that incorporates this temporal dynamics and observe that samples' loss gradient norms are highly dependent on timestep. This trend leads to a prominent bias in influence estimation, and is particularly noticeable for samples trained on large-norm-inducing timesteps, causing them to be generally influential. To mitigate this effect, we introduce Diffusion-ReTrac as a re-normalized adaptation that enables the retrieval of training samples more targeted to the test sample of interest, facilitating a localized measurement of influence and considerably more intuitive visualization. We demonstrate the efficacy of our approach through various evaluation metrics and auxiliary tasks, reducing the amount of generally influential samples to \$\textbackslash frac\textbraceleft 1\textbraceright\textbraceleft 3\textbraceright\$ of its original quantity.},
  howpublished = {https://arxiv.org/abs/2401.09031v3},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\V3D8P9Y2\Xie et al. - 2024 - Data Attribution for Diffusion Models Timestep-induced Bias in Influence Estimation.pdf}
}

@inproceedings{xieDEALDifficultyawareActive2020,
  title = {{{DEAL}}: {{Difficulty-aware Active Learning}} for {{Semantic Segmentation}}},
  shorttitle = {{{DEAL}}},
  booktitle = {Proceedings of the {{Asian Conference}} on {{Computer Vision}}},
  author = {Xie, Shuai and Feng, Zunlei and Chen, Ying and Sun, Songtao and Ma, Chao and Song, Mingli},
  year = 2020,
  urldate = {2025-10-22},
  langid = {english},
  keywords = {Active learning,semantic segmentation},
  file = {C:\Users\E097600\Zotero\storage\VTIKV7KH\Xie et al. - 2020 - DEAL Difficulty-aware Active Learning for Semantic Segmentation.pdf}
}

@misc{xieStructuralEntropyBasedSampleSelection2025,
  title = {Structural-{{Entropy-Based Sample Selection}} for {{Efficient}} and {{Effective Learning}}},
  author = {Xie, Tianchi and Zhu, Jiangning and Ma, Guozu and Lin, Minzhi and Chen, Wei and Yang, Weikai and Liu, Shixia},
  year = 2025,
  month = mar,
  number = {arXiv:2410.02268},
  eprint = {2410.02268},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.02268},
  urldate = {2025-03-06},
  abstract = {Sample selection improves the efficiency and effectiveness of machine learning models by providing informative and representative samples. Typically, samples can be modeled as a sample graph, where nodes are samples and edges represent their similarities. Most existing methods are based on local information, such as the training difficulty of samples, thereby overlooking global information, such as connectivity patterns. This oversight can result in suboptimal selection because global information is crucial for ensuring that the selected samples well represent the structural properties of the graph. To address this issue, we employ structural entropy to quantify global information and losslessly decompose it from the whole graph to individual nodes using the Shapley value. Based on the decomposition, we present \$\textbackslash textbf\textbraceleft S\textbraceright\$tructural-\$\textbackslash textbf\textbraceleft E\textbraceright\$ntropy-based sample \$\textbackslash textbf\textbraceleft S\textbraceright\$election (\$\textbackslash textbf\textbraceleft SES\textbraceright\$), a method that integrates both global and local information to select informative and representative samples. SES begins by constructing a \$k\$NN-graph among samples based on their similarities. It then measures sample importance by combining structural entropy (global metric) with training difficulty (local metric). Finally, SES applies importance-biased blue noise sampling to select a set of diverse and representative samples. Comprehensive experiments on three learning scenarios -- supervised learning, active learning, and continual learning -- clearly demonstrate the effectiveness of our method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\ZTD3AK59\\Xie et al. - 2025 - Structural-Entropy-Based Sample Selection for Efficient and Effective Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\68APIS5T\\2410.html}
}

@article{xuAutomaticDefectDetection2021,
  title = {Automatic Defect Detection and Segmentation of Tunnel Surface Using Modified {{Mask R-CNN}}},
  author = {Xu, Yingying and Li, Dawei and Xie, Qian and Wu, Qiaoyun and Wang, Jun},
  year = 2021,
  month = jun,
  journal = {Measurement},
  volume = {178},
  pages = {109316},
  issn = {0263-2241},
  doi = {10.1016/j.measurement.2021.109316},
  urldate = {2024-03-05},
  abstract = {The detection of tunnel surface defects is the very important part to ensure tunnel safety. Traditional tunnel detection mainly relies on naked-eye inspection, which is time-consuming and error-prone. In the past few years, many defect detection methods based on computer vision have been introduced. However, these methods with manual feature extraction do not perform well in detecting tunnel defects due to the complicated background of tunnel surfaces. To address these problems, this paper proposes a novel tunnel defect inspection method based on the Mask R-CNN. To improve the accuracy of the network, we endow it with a path augmentation feature pyramid network (PAFPN) and an edge detection branch. These improvements are easy to implement, with subtle extra memory and computational overhead. In this paper, we perform a detailed study of the PAFPN and the edge detection branch, and the experiment results show their robustness and accuracy in tunnel defect detection and segmentation.},
  keywords = {Deep learning,Defect detection,Instance segmentation,Leakage,Mask R-CNN,Spalling},
  file = {C:\Users\E097600\Zotero\storage\BDYTKST7\S0263224121003158.html}
}

@misc{xuDPPMaskMaskedImage2023,
  title = {{{DPPMask}}: {{Masked Image Modeling}} with {{Determinantal Point Processes}}},
  shorttitle = {{{DPPMask}}},
  author = {Xu, Junde and Lin, Zikai and Zhou, Donghao and Yang, Yaodong and Liao, Xiangyun and Wu, Bian and Chen, Guangyong and Heng, Pheng-Ann},
  year = 2023,
  month = mar,
  number = {arXiv:2303.12736},
  eprint = {2303.12736},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.12736},
  urldate = {2024-03-04},
  abstract = {Masked Image Modeling (MIM) has achieved impressive representative performance with the aim of reconstructing randomly masked images. Despite the empirical success, most previous works have neglected the important fact that it is unreasonable to force the model to reconstruct something beyond recovery, such as those masked objects. In this work, we show that uniformly random masking widely used in previous works unavoidably loses some key objects and changes original semantic information, resulting in a misalignment problem and hurting the representative learning eventually. To address this issue, we augment MIM with a new masking strategy namely the DPPMask by substituting the random process with Determinantal Point Process (DPPs) to reduce the semantic change of the image after masking. Our method is simple yet effective and requires no extra learnable parameters when implemented within various frameworks. In particular, we evaluate our method on two representative MIM frameworks, MAE and iBOT. We show that DPPMask surpassed random sampling under both lower and higher masking ratios, indicating that DPPMask makes the reconstruction task more reasonable. We further test our method on the background challenge and multi-class classification tasks, showing that our method is more robust at various tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\TXBBZSFD\\Xu et al. - 2023 - DPPMask Masked Image Modeling with Determinantal .pdf;C\:\\Users\\E097600\\Zotero\\storage\\F8JAGCV7\\2303.html}
}

@article{yan2DWassersteinLoss2021,
  title = {{{2D Wasserstein}} Loss for Robust Facial Landmark Detection},
  author = {Yan, Yongzhe and Duffner, Stefan and Phutane, Priyanka and Berthelier, Anthony and Blanc, Christophe and Garcia, Christophe and Chateau, Thierry},
  year = 2021,
  month = aug,
  journal = {Pattern Recognition},
  volume = {116},
  pages = {107945},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2021.107945},
  urldate = {2025-03-24},
  abstract = {The recent performance of facial landmark detection has been significantly improved by using deep Convolutional Neural Networks (CNNs), especially the Heatmap Regression Models (HRMs). Although their performance on common benchmark datasets has reached a high level, the robustness of these models still remains a challenging problem in the practical use under noisy conditions of realistic environments. Contrary to most existing work focusing on the design of new models, we argue that improving the robustness requires rethinking many other aspects, including the use of datasets, the format of landmark annotation, the evaluation metric as well as the training and detection algorithm itself. In this paper, we propose a novel method for robust facial landmark detection, using a loss function based on the 2D Wasserstein distance combined with a new landmark coordinate sampling relying on the barycenter of the individual probability distributions. Our method can be plugged-and-play on most state-of-the-art HRMs with neither additional complexity nor structural modifications of the models. Further, with the large performance increase, we found that current evaluation metrics can no longer fully reflect the robustness of these models. Therefore, we propose several improvements to the standard evaluation protocol. Extensive experimental results on both traditional evaluation metrics and our evaluation metrics demonstrate that our approach significantly improves the robustness of state-of-the-art facial landmark detection models.},
  keywords = {Face alignment,Facial landmark detection,Heatmap regression,Wasserstein distance},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\IIJDZZAP\\Yan et al. - 2021 - 2D Wasserstein loss for robust facial landmark detection.pdf;C\:\\Users\\E097600\\Zotero\\storage\\5DUDK7XJ\\S0031320321001321.html}
}

@misc{yanActiveLearningLogged2018,
  title = {Active {{Learning}} with {{Logged Data}}},
  author = {Yan, Songbai and Chaudhuri, Kamalika and Javidi, Tara},
  year = 2018,
  month = jun,
  number = {arXiv:1802.09069},
  eprint = {1802.09069},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.09069},
  urldate = {2025-09-19},
  abstract = {We consider active learning with logged data, where labeled examples are drawn conditioned on a predetermined logging policy, and the goal is to learn a classifier on the entire population, not just conditioned on the logging policy. Prior work addresses this problem either when only logged data is available, or purely in a controlled random experimentation setting where the logged data is ignored. In this work, we combine both approaches to provide an algorithm that uses logged data to bootstrap and inform experimentation, thus achieving the best of both worlds. Our work is inspired by a connection between controlled random experimentation and active learning, and modifies existing disagreement-based active learning algorithms to exploit logged data.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Machine Learning,Statistics - Machine Learning,tabular data,theoric},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\2CGHMWD8\\Yan et al. - 2018 - Active Learning with Logged Data.pdf;C\:\\Users\\E097600\\Zotero\\storage\\ZTTFXIMK\\1802.html}
}

@misc{yangDatasetPruningReducing2023,
  title = {Dataset {{Pruning}}: {{Reducing Training Data}} by {{Examining Generalization Influence}}},
  shorttitle = {Dataset {{Pruning}}},
  author = {Yang, Shuo and Xie, Zeke and Peng, Hanyu and Xu, Min and Sun, Mingming and Li, Ping},
  year = 2023,
  month = feb,
  number = {arXiv:2205.09329},
  eprint = {2205.09329},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.09329},
  urldate = {2025-05-26},
  abstract = {The great success of deep learning heavily relies on increasingly larger training data, which comes at a price of huge computational and infrastructural costs. This poses crucial questions that, do all training data contribute to model's performance? How much does each individual training sample or a sub-training-set affect the model's generalization, and how to construct the smallest subset from the entire training data as a proxy training set without significantly sacrificing the model's performance? To answer these, we propose dataset pruning, an optimization-based sample selection method that can (1) examine the influence of removing a particular set of training samples on model's generalization ability with theoretical guarantee, and (2) construct the smallest subset of training data that yields strictly constrained generalization gap. The empirically observed generalization gap of dataset pruning is substantially consistent with our theoretical expectations. Furthermore, the proposed method prunes 40\% training examples on the CIFAR-10 dataset, halves the convergence time with only 1.3\% test accuracy decrease, which is superior to previous score-based sample selection methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,data pruning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\6PS2RNPD\\Yang et al. - 2023 - Dataset Pruning Reducing Training Data by Examining Generalization Influence.pdf;C\:\\Users\\E097600\\Zotero\\storage\\G25ZAKNR\\2205.html}
}

@misc{yangNotAllData2023,
  title = {Not {{All Data Matters}}: {{An End-to-End Adaptive Dataset Pruning Framework}} for {{Enhancing Model Performance}} and {{Efficiency}}},
  shorttitle = {Not {{All Data Matters}}},
  author = {Yang, Suorong and Yang, Hongchao and Guo, Suhan and Shen, Furao and Zhao, Jian},
  year = 2023,
  month = dec,
  number = {arXiv:2312.05599},
  eprint = {2312.05599},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.05599},
  urldate = {2025-05-20},
  abstract = {While deep neural networks have demonstrated remarkable performance across various tasks, they typically require massive training data. Due to the presence of redundancies and biases in real-world datasets, not all data in the training dataset contributes to the model performance. To address this issue, dataset pruning techniques have been introduced to enhance model performance and efficiency by eliminating redundant training samples and reducing computational and memory overhead. However, previous works most rely on manually crafted scalar scores, limiting their practical performance and scalability across diverse deep networks and datasets. In this paper, we propose AdaPruner, an end-to-end Adaptive DAtaset PRUNing framEwoRk. AdaPruner can perform effective dataset pruning without the need for explicitly defined metrics. Our framework jointly prunes training data and fine-tunes models with task-specific optimization objectives. AdaPruner leverages (1) An adaptive dataset pruning (ADP) module, which iteratively prunes redundant samples to an expected pruning ratio; and (2) A pruning performance controller (PPC) module, which optimizes the model performance for accurate pruning. Therefore, AdaPruner exhibits high scalability and compatibility across various datasets and deep networks, yielding improved dataset distribution and enhanced model performance. AdaPruner can still significantly enhance model performance even after pruning up to 10-30\textbackslash\% of the training data. Notably, these improvements are accompanied by substantial savings in memory and computation costs. Qualitative and quantitative experiments suggest that AdaPruner outperforms other state-of-the-art dataset pruning methods by a large margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\L26ZUS5Q\\Yang et al. - 2023 - Not All Data Matters An End-to-End Adaptive Dataset Pruning Framework for Enhancing Model Performan.pdf;C\:\\Users\\E097600\\Zotero\\storage\\C6C2DRZQ\\2312.html}
}

@misc{yangPlugPlayActive2024,
  title = {Plug and {{Play Active Learning}} for {{Object Detection}}},
  author = {Yang, Chenhongyi and Huang, Lichao and Crowley, Elliot J.},
  year = 2024,
  month = mar,
  number = {arXiv:2211.11612},
  eprint = {2211.11612},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.11612},
  urldate = {2025-05-14},
  abstract = {Annotating datasets for object detection is an expensive and time-consuming endeavor. To minimize this burden, active learning (AL) techniques are employed to select the most informative samples for annotation within a constrained "annotation budget". Traditional AL strategies typically rely on model uncertainty or sample diversity for query sampling, while more advanced methods have focused on developing AL-specific object detector architectures to enhance performance. However, these specialized approaches are not readily adaptable to different object detectors due to the significant engineering effort required for integration. To overcome this challenge, we introduce Plug and Play Active Learning (PPAL), a simple and effective AL strategy for object detection. PPAL is a two-stage method comprising uncertainty-based and diversity-based sampling phases. In the first stage, our Difficulty Calibrated Uncertainty Sampling leverage a category-wise difficulty coefficient that combines both classification and localisation difficulties to re-weight instance uncertainties, from which we sample a candidate pool for the subsequent diversity-based sampling. In the second stage, we propose Category Conditioned Matching Similarity to better compute the similarities of multi-instance images as ensembles of their instance similarities, which is used by the k-Means++ algorithm to sample the final AL queries. PPAL makes no change to model architectures or detector training pipelines; hence it can be easily generalized to different object detectors. We benchmark PPAL on the MS-COCO and Pascal VOC datasets using different detector architectures and show that our method outperforms prior work by a large margin. Code is available at https://github.com/ChenhongyiYang/PPAL},
  archiveprefix = {arXiv},
  keywords = {Active learning,coco,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,fasterrcnn,Object detection,pascalvoc,retinanet},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\LBAB74RU\\Yang et al. - 2024 - Plug and Play Active Learning for Object Detection.pdf;C\:\\Users\\E097600\\Zotero\\storage\\CFQSNJE8\\2211.html}
}

@misc{yehudaActiveLearningCovering2022,
  title = {Active {{Learning Through}} a {{Covering Lens}}},
  author = {Yehuda, Ofer and Dekel, Avihu and Hacohen, Guy and Weinshall, Daphna},
  year = 2022,
  month = dec,
  number = {arXiv:2205.11320},
  eprint = {2205.11320},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.11320},
  urldate = {2024-03-08},
  abstract = {Deep active learning aims to reduce the annotation cost for the training of deep models, which is notoriously data-hungry. Until recently, deep active learning methods were ineffectual in the low-budget regime, where only a small number of examples are annotated. The situation has been alleviated by recent advances in representation and self-supervised learning, which impart the geometry of the data representation with rich information about the points. Taking advantage of this progress, we study the problem of subset selection for annotation through a "covering" lens, proposing ProbCover - a new active learning algorithm for the low budget regime, which seeks to maximize Probability Coverage. We then describe a dual way to view the proposed formulation, from which one can derive strategies suitable for the high budget regime of active learning, related to existing methods like Coreset. We conclude with extensive experiments, evaluating ProbCover in the low-budget regime. We show that our principled active learning strategy improves the state-of-the-art in the low-budget regime in several image recognition benchmarks. This method is especially beneficial in the semi-supervised setting, allowing state-of-the-art semi-supervised methods to match the performance of fully supervised methods, while using much fewer labels nonetheless. Code is available at https://github.com/avihu111/TypiClust.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\QBDIGVZD\\Yehuda et al. - 2022 - Active Learning Through a Covering Lens.pdf;C\:\\Users\\E097600\\Zotero\\storage\\UZMFHCUI\\2205.html}
}

@misc{YOLOS,
  title = {{{YOLOS}}},
  urldate = {2024-03-04},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/model\_doc/yolos},
  file = {C:\Users\E097600\Zotero\storage\HNGXJDG3\yolos.html}
}

@misc{yooLearningLossActive2019,
  title = {Learning {{Loss}} for {{Active Learning}}},
  author = {Yoo, Donggeun and Kweon, In So},
  year = 2019,
  month = may,
  number = {arXiv:1905.03677},
  eprint = {1905.03677},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.03677},
  urldate = {2024-03-06},
  abstract = {The performance of deep neural networks improves with more annotated data. The problem is that the budget for annotation is limited. One solution to this is active learning, where a model asks human to annotate data that it perceived as uncertain. A variety of recent methods have been proposed to apply active learning to deep networks but most of them are either designed specific for their target tasks or computationally inefficient for large networks. In this paper, we propose a novel active learning method that is simple but task-agnostic, and works efficiently with the deep networks. We attach a small parametric module, named "loss prediction module," to a target network, and learn it to predict target losses of unlabeled inputs. Then, this module can suggest data that the target model is likely to produce a wrong prediction. This method is task-agnostic as networks are learned from a single loss regardless of target tasks. We rigorously validate our method through image classification, object detection, and human pose estimation, with the recent network architectures. The results demonstrate that our method consistently outperforms the previous methods over the tasks.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Image classification,Object detection},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\2CYS5ZL3\\Yoo et Kweon - 2019 - Learning Loss for Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\7IQP7D56\\1905.html}
}

@article{yuanLearningHardtolearnActive2024,
  title = {Learning the Hard-to-Learn: {{Active}} Learning for Imbalanced Datasets in Data-Centric Tunnel Engineering},
  shorttitle = {Learning the Hard-to-Learn},
  author = {Yuan, Xiao and Wang, Shuying and Qu, Tongming and Feng, Huanhuan and Liu, Pengfei and Zeng, Junhao and Chen, Xiangsheng},
  year = 2024,
  month = oct,
  journal = {Computers and Geotechnics},
  volume = {174},
  pages = {106629},
  issn = {0266-352X},
  doi = {10.1016/j.compgeo.2024.106629},
  urldate = {2025-03-05},
  abstract = {Data-driven methodologies hold the potential to revolutionize mechanized tunneling by informing decision-making. However, imbalanced data categories are prevalent within the realm of mechanized tunneling. Data-driven models often struggle to accurately predict data from minor sample categories, which, despite their scarcity, are crucial in practice. To address this bottleneck, this study introduces a committee-based active learning strategy to tackle the imbalanced sample identification problem. This strategy involves constructing multiple committee models to quantify the uncertainty in surrogate predictions. These specimens exhibiting high forecast uncertainty will be resampled with replacement from the validation data to iteratively augment the training data, thereby enhancing the model's capability to handle imbalanced datasets. This study validates the strategy by applying it to two real tunneling engineering scenarios. The first case is to predict the grade of surrounding rock and the other one involves forecasting muck clogging in mechanised tunneling. The results demonstrate that the proposed strategy significantly improves the prediction accuracy of minority categories. Further, the study reveals that both the ``entropy method'' and ``weight method'' can effectively quantify the learning difficulty of samples. The same strategy can be equally applied to other fields of engineering and science.},
  keywords = {Active learning,Class imbalance,Imbalanced data,Machine Learning,Rock Mass Classification,semantic segmentation,Soil clogging recognition,Tunnel boring machine},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\DM4NCVQ6\\Yuan et al. - 2024 - Learning the hard-to-learn Active learning for imbalanced datasets in data-centric tunnel engineeri.pdf;C\:\\Users\\E097600\\Zotero\\storage\\3PNDLR48\\S0266352X24005688.html}
}

@misc{yuanMultipleInstanceActive2021,
  title = {Multiple Instance Active Learning for Object Detection},
  author = {Yuan, Tianning and Wan, Fang and Fu, Mengying and Liu, Jianzhuang and Xu, Songcen and Ji, Xiangyang and Ye, Qixiang},
  year = 2021,
  month = apr,
  number = {arXiv:2104.02324},
  eprint = {2104.02324},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.02324},
  urldate = {2025-05-14},
  abstract = {Despite the substantial progress of active learning for image recognition, there still lacks an instance-level active learning method specified for object detection. In this paper, we propose Multiple Instance Active Object Detection (MI-AOD), to select the most informative images for detector training by observing instance-level uncertainty. MI-AOD defines an instance uncertainty learning module, which leverages the discrepancy of two adversarial instance classifiers trained on the labeled set to predict instance uncertainty of the unlabeled set. MI-AOD treats unlabeled images as instance bags and feature anchors in images as instances, and estimates the image uncertainty by re-weighting instances in a multiple instance learning (MIL) fashion. Iterative instance uncertainty learning and re-weighting facilitate suppressing noisy instances, toward bridging the gap between instance uncertainty and image-level uncertainty. Experiments validate that MI-AOD sets a solid baseline for instance-level active learning. On commonly used object detection datasets, MI-AOD outperforms state-of-the-art methods with significant margins, particularly when the labeled sets are small. Code is available at https://github.com/yuantn/MI-AOD.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,multiple instance learning,Object detection},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\CJVTAGYK\\Yuan et al. - 2021 - Multiple instance active learning for object detection.pdf;C\:\\Users\\E097600\\Zotero\\storage\\G256E69W\\2104.html}
}

@misc{yuMulticlassRoadDefect2024,
  title = {Multi-Class {{Road Defect Detection}} and {{Segmentation}} Using {{Spatial}} and {{Channel-wise Attention}} for {{Autonomous Road Repairing}}},
  author = {Yu, Jongmin and Chi, Chen Bene and Fichera, Sebastiano and Paoletti, Paolo and Mehta, Devansh and Luo, Shan},
  year = 2024,
  month = feb,
  number = {arXiv:2402.04064},
  eprint = {2402.04064},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.04064},
  urldate = {2024-03-04},
  abstract = {Road pavement detection and segmentation are critical for developing autonomous road repair systems. However, developing an instance segmentation method that simultaneously performs multi-class defect detection and segmentation is challenging due to the textural simplicity of road pavement image, the diversity of defect geometries, and the morphological ambiguity between classes. We propose a novel end-to-end method for multi-class road defect detection and segmentation. The proposed method comprises multiple spatial and channel-wise attention blocks available to learn global representations across spatial and channel-wise dimensions. Through these attention blocks, more globally generalised representations of morphological information (spatial characteristics) of road defects and colour and depth information of images can be learned. To demonstrate the effectiveness of our framework, we conducted various ablation studies and comparisons with prior methods on a newly collected dataset annotated with nine road defect classes. The experiments show that our proposed method outperforms existing state-of-the-art methods for multi-class road defect detection and segmentation methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\WWPHCH8T\\Yu et al. - 2024 - Multi-class Road Defect Detection and Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\2GIDP8D3\\2402.html}
}

@article{yuTwoStepActiveLearning2023,
  title = {Two-{{Step Active Learning}} for {{Instance Segmentation}} with {{Uncertainty}} and {{Diversity Sampling}}},
  author = {Yu, Ke and Albro, Stephen and DeSalvo, Giulia and Kothawade, Suraj and Rashwan, Abdullah and Tavakkol, Sasan and Batmanghelich, Kayhan and Yin, Xiaoqi},
  year = 2023,
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2309.16139},
  urldate = {2025-08-25},
  abstract = {Training high-quality instance segmentation models requires an abundance of labeled images with instance masks and classifications, which is often expensive to procure. Active learning addresses this challenge by striving for optimum performance with minimal labeling cost by selecting the most informative and representative images for labeling. Despite its potential, active learning has been less explored in instance segmentation compared to other tasks like image classification, which require less labeling. In this study, we propose a post-hoc active learning algorithm that integrates uncertainty-based sampling with diversity-based sampling. Our proposed algorithm is not only simple and easy to implement, but it also delivers superior performance on various datasets. Its practical application is demonstrated on a real-world overhead imagery dataset, where it increases the labeling efficiency fivefold.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Active learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Instance segmentation,Machine Learning (cs.LG)},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\FEDLEEFI\\Yu et al. - 2023 - Two-Step Active Learning for Instance Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\EBE5PWKC\\2309.html;C\:\\Users\\E097600\\Zotero\\storage\\M32HTIUY\\2309.html}
}

@misc{zhanALdatasetBenchmarkPoolbased2020,
  title = {{{ALdataset}}: A Benchmark for Pool-Based Active Learning},
  shorttitle = {{{ALdataset}}},
  author = {Zhan, Xueying and Chan, Antoni Bert},
  year = 2020,
  month = oct,
  number = {arXiv:2010.08161},
  eprint = {2010.08161},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.08161},
  urldate = {2025-09-18},
  abstract = {Active learning (AL) is a subfield of machine learning (ML) in which a learning algorithm could achieve good accuracy with less training samples by interactively querying a user/oracle to label new data points. Pool-based AL is well-motivated in many ML tasks, where unlabeled data is abundant, but their labels are hard to obtain. Although many pool-based AL methods have been developed, the lack of a comparative benchmarking and integration of techniques makes it difficult to: 1) determine the current state-of-the-art technique; 2) evaluate the relative benefit of new methods for various properties of the dataset; 3) understand what specific problems merit greater attention; and 4) measure the progress of the field over time. To conduct easier comparative evaluation among AL methods, we present a benchmark task for pool-based active learning, which consists of benchmarking datasets and quantitative metrics that summarize overall performance. We present experiment results for various active learning strategies, both recently proposed and classic highly-cited methods, and draw insights from the results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\D3T7DSMS\\Zhan et Chan - 2020 - ALdataset a benchmark for pool-based active learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\4V75F8WF\\2010.html}
}

@misc{zhanComparativeSurveyDeep2022,
  title = {A {{Comparative Survey}} of {{Deep Active Learning}}},
  author = {Zhan, Xueying and Wang, Qingzhong and Huang, Kuan-hao and Xiong, Haoyi and Dou, Dejing and Chan, Antoni B.},
  year = 2022,
  month = jul,
  number = {arXiv:2203.13450},
  eprint = {2203.13450},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.13450},
  urldate = {2025-04-30},
  abstract = {While deep learning (DL) is data-hungry and usually relies on extensive labeled data to deliver good performance, Active Learning (AL) reduces labeling costs by selecting a small proportion of samples from unlabeled data for labeling and training. Therefore, Deep Active Learning (DAL) has risen as a feasible solution for maximizing model performance under a limited labeling cost/budget in recent years. Although abundant methods of DAL have been developed and various literature reviews conducted, the performance evaluation of DAL methods under fair comparison settings is not yet available. Our work intends to fill this gap. In this work, We construct a DAL toolkit, DeepAL+, by re-implementing 19 highly-cited DAL methods. We survey and categorize DAL-related works and construct comparative experiments across frequently used datasets and DAL algorithms. Additionally, we explore some factors (e.g., batch size, number of epochs in the training process) that influence the efficacy of DAL, which provides better references for researchers to design their DAL experiments or carry out DAL-related applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\NPFAX8GN\\Zhan et al. - 2022 - A Comparative Survey of Deep Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\RTSCGFEW\\2203.html}
}

@misc{zhangFlexMatchBoostingSemiSupervised2022,
  title = {{{FlexMatch}}: {{Boosting Semi-Supervised Learning}} with {{Curriculum Pseudo Labeling}}},
  shorttitle = {{{FlexMatch}}},
  author = {Zhang, Bowen and Wang, Yidong and Hou, Wenxin and Wu, Hao and Wang, Jindong and Okumura, Manabu and Shinozaki, Takahiro},
  year = 2022,
  month = jan,
  number = {arXiv:2110.08263},
  eprint = {2110.08263},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.08263},
  urldate = {2024-06-24},
  abstract = {The recently proposed FixMatch achieved state-of-the-art results on most semi-supervised learning (SSL) benchmarks. However, like other modern SSL algorithms, FixMatch uses a pre-defined constant threshold for all classes to select unlabeled data that contribute to the training, thus failing to consider different learning status and learning difficulties of different classes. To address this issue, we propose Curriculum Pseudo Labeling (CPL), a curriculum learning approach to leverage unlabeled data according to the model's learning status. The core of CPL is to flexibly adjust thresholds for different classes at each time step to let pass informative unlabeled data and their pseudo labels. CPL does not introduce additional parameters or computations (forward or backward propagation). We apply CPL to FixMatch and call our improved algorithm FlexMatch. FlexMatch achieves state-of-the-art performance on a variety of SSL benchmarks, with especially strong performances when the labeled data are extremely limited or when the task is challenging. For example, FlexMatch achieves 13.96\% and 18.96\% error rate reduction over FixMatch on CIFAR-100 and STL-10 datasets respectively, when there are only 4 labels per class. CPL also significantly boosts the convergence speed, e.g., FlexMatch can use only 1/5 training time of FixMatch to achieve even better performance. Furthermore, we show that CPL can be easily adapted to other SSL algorithms and remarkably improve their performances. We open-source our code at https://github.com/TorchSSL/TorchSSL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\U9TPMXYX\\Zhang et al. - 2022 - FlexMatch Boosting Semi-Supervised Learning with .pdf;C\:\\Users\\E097600\\Zotero\\storage\\BL5T7LWV\\2110.html}
}

@misc{zhangGALAXYGraphbasedActive2022,
  title = {{{GALAXY}}: {{Graph-based Active Learning}} at the {{Extreme}}},
  shorttitle = {{{GALAXY}}},
  author = {Zhang, Jifan and {Katz-Samuels}, Julian and Nowak, Robert},
  year = 2022,
  month = may,
  number = {arXiv:2202.01402},
  eprint = {2202.01402},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.01402},
  urldate = {2025-09-18},
  abstract = {Active learning is a label-efficient approach to train highly effective models while interactively selecting only small subsets of unlabelled data for labelling and training. In "open world" settings, the classes of interest can make up a small fraction of the overall dataset -- most of the data may be viewed as an out-of-distribution or irrelevant class. This leads to extreme class-imbalance, and our theory and methods focus on this core issue. We propose a new strategy for active learning called GALAXY (Graph-based Active Learning At the eXtrEme), which blends ideas from graph-based active learning and deep learning. GALAXY automatically and adaptively selects more class-balanced examples for labeling than most other methods for active learning. Our theory shows that GALAXY performs a refined form of uncertainty sampling that gathers a much more class-balanced dataset than vanilla uncertainty sampling. Experimentally, we demonstrate GALAXY's superiority over existing state-of-art deep active learning algorithms in unbalanced vision classification settings generated from popular datasets.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Class imbalance,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,GNN,Image classification},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\9U5ACW4Y\\Zhang et al. - 2022 - GALAXY Graph-based Active Learning at the Extreme.pdf;C\:\\Users\\E097600\\Zotero\\storage\\PGRBHVYX\\2202.html}
}

@misc{zhangGeneralizedCrossEntropy2018,
  title = {Generalized {{Cross Entropy Loss}} for {{Training Deep Neural Networks}} with {{Noisy Labels}}},
  author = {Zhang, Zhilu and Sabuncu, Mert R.},
  year = 2018,
  month = nov,
  number = {arXiv:1805.07836},
  eprint = {1805.07836},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.07836},
  urldate = {2025-02-07},
  abstract = {Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\RSN6SEUN\\Zhang et Sabuncu - 2018 - Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels.pdf;C\:\\Users\\E097600\\Zotero\\storage\\2F98PFY2\\1805.html}
}

@misc{zhangLabelefficientSinglePhoton2025,
  title = {Label-Efficient {{Single Photon Images Classification}} via {{Active Learning}}},
  author = {Zhang, Zili and Wen, Ziting and Qiang, Yiheng and Dong, Hongzhou and Dong, Wenle and Li, Xinyang and Wang, Xiaofan and Ren, Xiaoqiang},
  year = 2025,
  month = may,
  number = {arXiv:2505.04376},
  eprint = {2505.04376},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.04376},
  urldate = {2025-05-13},
  abstract = {Single-photon LiDAR achieves high-precision 3D imaging in extreme environments through quantum-level photon detection technology. Current research primarily focuses on reconstructing 3D scenes from sparse photon events, whereas the semantic interpretation of single-photon images remains underexplored, due to high annotation costs and inefficient labeling strategies. This paper presents the first active learning framework for single-photon image classification. The core contribution is an imaging condition-aware sampling strategy that integrates synthetic augmentation to model variability across imaging conditions. By identifying samples where the model is both uncertain and sensitive to these conditions, the proposed method selectively annotates only the most informative examples. Experiments on both synthetic and real-world datasets show that our approach outperforms all baselines and achieves high classification accuracy with significantly fewer labeled samples. Specifically, our approach achieves 97\% accuracy on synthetic single-photon data using only 1.5\% labeled samples. On real-world data, we maintain 90.63\% accuracy with just 8\% labeled samples, which is 4.51\% higher than the best-performing baseline. This illustrates that active learning enables the same level of classification performance on single-photon images as on classical images, opening doors to large-scale integration of single-photon data in real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\SKWI4VIP\\Zhang et al. - 2025 - Label-efficient Single Photon Images Classification via Active Learning.pdf;C\:\\Users\\E097600\\Zotero\\storage\\LVH94ACJ\\2505.html}
}

@misc{zhangQuantifyingLimitsSegmentation2025,
  title = {Quantifying the {{Limits}} of {{Segmentation Foundation Models}}: {{Modeling Challenges}} in {{Segmenting Tree-Like}} and {{Low-Contrast Objects}}},
  shorttitle = {Quantifying the {{Limits}} of {{Segmentation Foundation Models}}},
  author = {Zhang, Yixin and Konz, Nicholas and Kramer, Kevin and Mazurowski, Maciej A.},
  year = 2025,
  month = nov,
  number = {arXiv:2412.04243},
  eprint = {2412.04243},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.04243},
  urldate = {2025-11-18},
  abstract = {Image segmentation foundation models (SFMs) like Segment Anything Model (SAM) have achieved impressive zero-shot and interactive segmentation across diverse domains. However, they struggle to segment objects with certain structures, particularly those with dense, tree-like morphology and low textural contrast from their surroundings. These failure modes are crucial for understanding the limitations of SFMs in real-world applications. To systematically study this issue, we introduce interpretable metrics quantifying object tree-likeness and textural separability. On carefully controlled synthetic experiments and real-world datasets, we show that SFM performance (\textbackslash eg, SAM, SAM 2, HQ-SAM) noticeably correlates with these factors. We attribute these failures to SFMs misinterpreting local structure as global texture, resulting in over-segmentation or difficulty distinguishing objects from similar backgrounds. Notably, targeted fine-tuning fails to resolve this issue, indicating a fundamental limitation. Our study provides the first quantitative framework for modeling the behavior of SFMs on challenging structures, offering interpretable insights into their segmentation capabilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,fundation model,semantic segmentation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\9ICIR9XE\\Zhang et al. - 2025 - Quantifying the Limits of Segmentation Foundation Models Modeling Challenges in Segmenting Tree-Lik.pdf;C\:\\Users\\E097600\\Zotero\\storage\\9ARKBLDQ\\2412.html}
}

@misc{zhangRevisitingDomainShift2023,
  title = {Revisiting the {{Domain Shift}} and {{Sample Uncertainty}} in {{Multi-source Active Domain Transfer}}},
  author = {Zhang, Wenqiao and Lv, Zheqi and Zhou, Hao and Liu, Jia-Wei and Li, Juncheng and Li, Mengze and Tang, Siliang and Zhuang, Yueting},
  year = 2023,
  month = nov,
  number = {arXiv:2311.12905},
  eprint = {2311.12905},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.12905},
  urldate = {2025-09-26},
  abstract = {Active Domain Adaptation (ADA) aims to maximally boost model adaptation in a new target domain by actively selecting a limited number of target data to annotate.This setting neglects the more practical scenario where training data are collected from multiple sources. This motivates us to target a new and challenging setting of knowledge transfer that extends ADA from a single source domain to multiple source domains, termed Multi-source Active Domain Adaptation (MADA). Not surprisingly, we find that most traditional ADA methods cannot work directly in such a setting, mainly due to the excessive domain gap introduced by all the source domains and thus their uncertainty-aware sample selection can easily become miscalibrated under the multi-domain shifts. Considering this, we propose a Dynamic integrated uncertainty valuation framework(Detective) that comprehensively consider the domain shift between multi-source domains and target domain to detect the informative target samples. Specifically, the leverages a dynamic Domain Adaptation(DA) model that learns how to adapt the model's parameters to fit the union of multi-source domains. This enables an approximate single-source domain modeling by the dynamic model. We then comprehensively measure both domain uncertainty and predictive uncertainty in the target domain to detect informative target samples using evidential deep learning, thereby mitigating uncertainty miscalibration. Furthermore, we introduce a contextual diversity-aware calculator to enhance the diversity of the selected samples. Experiments demonstrate that our solution outperforms existing methods by a considerable margin on three domain adaptation benchmarks.},
  archiveprefix = {arXiv},
  keywords = {active domain adaptation,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,domain adaptation},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\85QSGL7M\\Zhang et al. - 2023 - Revisiting the Domain Shift and Sample Uncertainty in Multi-source Active Domain Transfer.pdf;C\:\\Users\\E097600\\Zotero\\storage\\S4CZJDQC\\2311.html}
}

@misc{zhaoActiveLearningAbrupt2023,
  title = {Active {{Learning}} for {{Abrupt Shifts Change-point Detection}} via {{Derivative-Aware Gaussian Processes}}},
  author = {Zhao, Hao and Pan, Rong},
  year = 2023,
  month = dec,
  number = {arXiv:2312.03176},
  eprint = {2312.03176},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.03176},
  urldate = {2024-03-08},
  abstract = {Change-point detection (CPD) is crucial for identifying abrupt shifts in data, which influence decision-making and efficient resource allocation across various domains. To address the challenges posed by the costly and time-intensive data acquisition in CPD, we introduce the Derivative-Aware Change Detection (DACD) method. It leverages the derivative process of a Gaussian process (GP) for Active Learning (AL), aiming to pinpoint change-point locations effectively. DACD balances the exploitation and exploration of derivative processes through multiple data acquisition functions (AFs). By utilizing GP derivative mean and variance as criteria, DACD sequentially selects the next sampling data point, thus enhancing algorithmic efficiency and ensuring reliable and accurate results. We investigate the effectiveness of DACD method in diverse scenarios and show it outperforms other active learning change-point detection approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\8MY37VM7\\Zhao et Pan - 2023 - Active Learning for Abrupt Shifts Change-point Det.pdf;C\:\\Users\\E097600\\Zotero\\storage\\MYBY35DV\\2312.html;C\:\\Users\\E097600\\Zotero\\storage\\NG5FVUF6\\2312.html}
}

@inproceedings{zhaoActiveLearningLabel2021,
  title = {Active {{Learning}} under {{Label Shift}}},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Zhao, Eric and Liu, Anqi and Anandkumar, Animashree and Yue, Yisong},
  year = 2021,
  month = mar,
  pages = {3412--3420},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-09-18},
  abstract = {We address the problem of active learning under label shift: when the class proportions of source and target domains differ. We introduce a "medial distribution" to incorporate a tradeoff between importance weighting and class-balanced sampling and propose their combined usage in active learning. Our method is known as Mediated Active Learning under Label Shift (MALLS). It balances the bias from class-balanced sampling and the variance from importance weighting. We prove sample complexity and generalization guarantees for MALLS which show active learning reduces asymptotic sample complexity even under arbitrary label shift. We empirically demonstrate MALLS scales to high-dimensional datasets and can reduce the sample complexity of active learning by 60\% in deep active learning tasks.},
  langid = {english},
  keywords = {active domain adaptation,Active learning,Class imbalance,Computer Science - Machine Learning,Distribution shift,Domain Generalisation,Image classification,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\64EF9G3R\\Zhao et al. - 2021 - Active Learning under Label Shift.pdf;C\:\\Users\\E097600\\Zotero\\storage\\E53VJ3LY\\Zhao et al. - 2021 - Active Learning under Label Shift.pdf;C\:\\Users\\E097600\\Zotero\\storage\\T2YZ37SV\\Zhao et al. - 2021 - Active Learning under Label Shift.pdf;C\:\\Users\\E097600\\Zotero\\storage\\S44CEHQB\\2007.html}
}

@inproceedings{zhaoDeeplySupervisedActive2020,
  title = {Deeply {{Supervised Active Learning}} for {{Finger Bones Segmentation}}},
  booktitle = {2020 42nd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} \& {{Biology Society}} ({{EMBC}})},
  author = {Zhao, Ziyuan and Yang, Xiaoyan and Veeravalli, Bharadwaj and Zeng, Zeng},
  year = 2020,
  month = jul,
  eprint = {2005.03225},
  primaryclass = {cs, eess},
  pages = {1620--1623},
  doi = {10.1109/EMBC44109.2020.9176662},
  urldate = {2024-03-26},
  abstract = {Segmentation is a prerequisite yet challenging task for medical image analysis. In this paper, we introduce a novel deeply supervised active learning approach for finger bones segmentation. The proposed architecture is fine-tuned in an iterative and incremental learning manner. In each step, the deep supervision mechanism guides the learning process of hidden layers and selects samples to be labeled. Extensive experiments demonstrated that our method achieves competitive segmentation results using less labeled samples as compared with full annotation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\SN255BHE\\Zhao et al. - 2020 - Deeply Supervised Active Learning for Finger Bones.pdf;C\:\\Users\\E097600\\Zotero\\storage\\KS9V65F5\\2005.html}
}

@article{zhaoEventBasedControlOnline2020,
  title = {Event-{{Based Control}} for {{Online Training}} of {{Neural Networks}}},
  author = {Zhao, Zilong and Cerf, Sophie and Robu, Bogdan and Marchand, Nicolas},
  year = 2020,
  month = jul,
  journal = {IEEE Control Systems Letters},
  volume = {4},
  number = {3},
  pages = {773--778},
  issn = {2475-1456},
  doi = {10.1109/LCSYS.2020.2981984},
  urldate = {2025-11-27},
  abstract = {Convolutional Neural Network (CNN) has become the most used method for image classification tasks. During its training the learning rate and the gradient are two key factors to tune for influencing the convergence speed of the model. Usual learning rate strategies are time-based, i.e., monotonous decay over time. Recent state-of-the-art techniques focus on adaptive gradient algorithms, i.e., Adam and its versions. In this letter we consider an online learning scenario and we propose two Event-Based control loops to adjust the learning rate of a classical algorithm E (Exponential)/PD (Proportional Derivative)-Control. The first Event-Based control loop will be implemented to prevent sudden drop of the learning rate when the model is approaching the optimum. The second Event-Based control loop will decide, based on the learning speed, when to switch to the next data batch. Experimental evaluation is provided using two state-of-the-art machine learning image datasets (CIFAR-10 and CIFAR-100). Results show the Event-Based E/PD is better than the original algorithm (higher final accuracy, lower final loss value), and the Double-Event-Based E/PD can accelerate the training process, save up to 67\% training time compared to state-of-the-art algorithms and even result in better performance.},
  keywords = {Computer Science - Machine Learning,Control systems,Convergence,Convolutional neural networks,Event based control,gradient methods,Handheld computers,Machine learning algorithms,neural networks,Stability analysis,Statistics - Machine Learning,Training},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\T9EPHBQJ\\Zhao et al. - 2020 - Event-Based Control for Online Training of Neural Networks.pdf;C\:\\Users\\E097600\\Zotero\\storage\\KGD29UNT\\9042857.html}
}

@misc{zhaoEvidentialDeepActive2025,
  title = {Evidential {{Deep Active Learning}} for {{Semi-Supervised Classification}}},
  author = {Zhao, Shenkai and Zhang, Xinao and Pan, Lipeng and Xu, Xiaobin and Pelusi, Danilo},
  year = 2025,
  month = may,
  number = {arXiv:2505.20691},
  eprint = {2505.20691},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.20691},
  urldate = {2025-06-02},
  abstract = {Semi-supervised classification based on active learning has made significant progress, but the existing methods often ignore the uncertainty estimation (or reliability) of the prediction results during the learning process, which makes it questionable whether the selected samples can effectively update the model. Hence, this paper proposes an evidential deep active learning approach for semi-supervised classification (EDALSSC). EDALSSC builds a semi-supervised learning framework to simultaneously quantify the uncertainty estimation of labeled and unlabeled data during the learning process. The uncertainty estimation of the former is associated with evidential deep learning, while that of the latter is modeled by combining ignorance information and conflict information of the evidence from the perspective of the T-conorm operator. Furthermore, this article constructs a heuristic method to dynamically balance the influence of evidence and the number of classes on uncertainty estimation to ensure that it does not produce counter-intuitive results in EDALSSC. For the sample selection strategy, EDALSSC selects the sample with the greatest uncertainty estimation that is calculated in the form of a sum when the training loss increases in the latter half of the learning process. Experimental results demonstrate that EDALSSC outperforms existing semi-supervised and supervised active learning approaches on image classification datasets.},
  archiveprefix = {arXiv},
  keywords = {Active learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Image classification,Uncertainty},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\KRVL4TSX\\Zhao et al. - 2025 - Evidential Deep Active Learning for Semi-Supervised Classification.pdf;C\:\\Users\\E097600\\Zotero\\storage\\Z6W8PELW\\2505.html}
}

@article{zhaoLossFunctionsImage2017,
  title = {Loss {{Functions}} for {{Image Restoration With Neural Networks}}},
  author = {Zhao, Hang and Gallo, Orazio and Frosio, Iuri and Kautz, Jan},
  year = 2017,
  month = mar,
  journal = {IEEE Transactions on Computational Imaging},
  volume = {3},
  number = {1},
  pages = {47--57},
  issn = {2333-9403},
  doi = {10.1109/TCI.2016.2644865},
  urldate = {2025-01-16},
  abstract = {Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is \textbackslash ell \_2. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.},
  keywords = {Image processing,Image quality,image restoration,Image restoration,loss function,Measurement,neural networks,Neural networks},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\3LVPFBDE\\Zhao et al. - 2017 - Loss Functions for Image Restoration With Neural Networks.pdf;C\:\\Users\\E097600\\Zotero\\storage\\CJ7ZRPTM\\7797130.html}
}

@misc{zhaoPyramidSceneParsing2017,
  title = {Pyramid {{Scene Parsing Network}}},
  author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  year = 2017,
  month = apr,
  number = {arXiv:1612.01105},
  eprint = {1612.01105},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1612.01105},
  urldate = {2024-07-10},
  abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\6C7ZK2SA\\Zhao et al. - 2017 - Pyramid Scene Parsing Network.pdf;C\:\\Users\\E097600\\Zotero\\storage\\JCRCTQFD\\1612.html}
}

@inproceedings{zhengConditionalRandomFields2015,
  title = {Conditional {{Random Fields}} as {{Recurrent Neural Networks}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zheng, Shuai and Jayasumana, Sadeep and {Romera-Paredes}, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip H. S.},
  year = 2015,
  month = dec,
  eprint = {1502.03240},
  primaryclass = {cs},
  pages = {1529--1537},
  doi = {10.1109/ICCV.2015.179},
  urldate = {2025-09-30},
  abstract = {Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\WP5LNEVS\\Zheng et al. - 2015 - Conditional Random Fields as Recurrent Neural Networks.pdf;C\:\\Users\\E097600\\Zotero\\storage\\Z55MSJ6W\\1502.html}
}

@article{zhouCompetitiveAlgorithmAgnostic2023,
  title = {A {{Competitive Algorithm}} for {{Agnostic Active Learning}}},
  author = {Zhou, Yihan and Price, Eric},
  year = 2023,
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {58670--58691},
  urldate = {2024-03-06},
  langid = {english},
  file = {C:\Users\E097600\Zotero\storage\RRVZDTE7\Zhou et Price - 2023 - A Competitive Algorithm for Agnostic Active Learni.pdf}
}

@article{zhouTrainingCostsensitiveNeural2006,
  title = {Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem},
  author = {Zhou, Zhi-Hua and Liu, Xu-Ying},
  year = 2006,
  month = jan,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {18},
  number = {1},
  pages = {63--77},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2006.17},
  urldate = {2025-02-11},
  abstract = {This paper studies empirically the effect of sampling and threshold-moving in training cost-sensitive neural networks. Both oversampling and undersampling are considered. These techniques modify the distribution of the training data such that the costs of the examples are conveyed explicitly by the appearances of the examples. Threshold-moving tries to move the output threshold toward inexpensive classes such that examples with higher costs become harder to be misclassified. Moreover, hard-ensemble and soft-ensemble, i.e., the combination of above techniques via hard or soft voting schemes, are also tested. Twenty-one UCl data sets with three types of cost matrices and a real-world cost-sensitive data set are used in the empirical study. The results suggest that cost-sensitive learning with multiclass tasks is more difficult than with two-class tasks, and a higher degree of class imbalance may increase the difficulty. It also reveals that almost all the techniques are effective on two-class tasks, while most are ineffective and even may cause negative effect on multiclass tasks. Overall, threshold-moving and soft-ensemble are relatively good choices in training cost-sensitive neural networks. The empirical study also suggests that some methods that have been believed to be effective in addressing the class imbalance problem may, in fact, only be effective on learning with imbalanced two-class data sets.},
  keywords = {class imbalance learning,cost-sensitive learning,Costs,Data mining,Decision trees,ensemble learning.,Index Terms- Machine learning,Learning systems,Machine Learning,neural networks,Neural networks,sampling,Sampling methods,Testing,threshold-moving,Training data,Voting},
  file = {C:\Users\E097600\Zotero\storage\3RD6LHNX\1549828.html}
}

@misc{zhouUNetNestedUNet2018,
  title = {{{UNet}}++: {{A Nested U-Net Architecture}} for {{Medical Image Segmentation}}},
  shorttitle = {{{UNet}}++},
  author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
  year = 2018,
  month = jul,
  number = {arXiv:1807.10165},
  eprint = {1807.10165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.10165},
  urldate = {2025-01-24},
  abstract = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\CFYXCT7Y\\Zhou et al. - 2018 - UNet++ A Nested U-Net Architecture for Medical Image Segmentation.pdf;C\:\\Users\\E097600\\Zotero\\storage\\LW6GR8VS\\1807.html}
}

@article{zhouVisionTransformerDiscover2024,
  title = {Vision Transformer: {{To}} Discover the ``Four Secrets'' of Image Patches},
  shorttitle = {Vision Transformer},
  author = {Zhou, Tao and Niu, Yuxia and Lu, Huiling and Peng, Caiyue and Guo, Yujie and Zhou, Huiyu},
  year = 2024,
  month = may,
  journal = {Information Fusion},
  volume = {105},
  pages = {102248},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2024.102248},
  urldate = {2024-05-22},
  abstract = {Vision Transformer (ViT) is widely used in the field of computer vision, in ViT, there are four main steps, which are ``four secrets'', such as patch division, token selection, position encoding addition, attention calculation, the existing research on transformer in computer vision mainly focuses on the above four steps. Therefore, ``how to divide patch?'', ``how to select token?'', ``how to add position encoding?'', and ``how to calculate attention?'' are crucial to improve ViT performance. But so far, most of the review literatures are summarized from the perspective of application, and there is no corresponding literature to comprehensively summarize these four steps from the technology perspective, which restricts the further development of ViT in some degree. To address the above questions, the 4 major mechanisms and 5 applications of ViT are summarized in this paper, the main innovative works are as follows: Firstly, the basic principle and model structure of ViT are elaborated; Secondly, aiming to ``how to divide patch?'', the 5 key techniques of patch division mechanism are summarized: from single-size division to multi-size division, from fixed number division to adaptive number division, from non-overlapping division to overlapping division, from semantic segmentation division to semantic aggregation division, and from original image division to feature map division; Thirdly, aiming to ``how to select token?'', the 3 key techniques of token selection mechanism are summarized: token selection based on score, token selection based on merge, token selection based on convolution and pooling; Fourthly, aiming to ``how to add position encoding?'', the 5 key techniques of position encoding mechanism are summarized: absolute position encoding, relative position encoding, conditional position encoding, locally-enhanced position encoding, and zero-padding position encoding; Fifthly, aiming to ``how to calculate attention?'', 18 attention mechanisms are summarized based on the timeline; Sixthly, these models that Transformer is combined with U-Net, GAN, YOLO, ResNet, and DenseNet are discussed in the medical image processing field; Finally, around these four questions proposed in this paper, we look forward to the future development direction of frontier technologies such as patch division mechanism, token selection mechanism, position encoding mechanism, and attention mechanism et~al, which play an important role in the further development of ViT.},
  keywords = {Attention mechanism,Patch division mechanism,Position encoding mechanism,Token selection mechanism,Transformer},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\VNSL94UU\\Zhou et al. - 2024 - Vision transformer To discover the “four secrets”.pdf;C\:\\Users\\E097600\\Zotero\\storage\\DQLGD83D\\S1566253524000265.html}
}

@inproceedings{zhouWaveletDomainMultiview2014,
  title = {Wavelet Domain Multi-View Active Learning for Hyperspectral Image Analysis},
  author = {Zhou, Xiong and Prasad, Saurabh and Crawford, Melba},
  year = 2014,
  month = jun,
  pages = {1--4},
  doi = {10.1109/WHISPERS.2014.8077528},
  file = {C:\Users\E097600\Zotero\storage\HWIR3YGN\Zhou et al. - 2014 - Wavelet domain multi-view active learning for hyperspectral image analysis.pdf}
}

@misc{zouzouRobustVisionBasedRunway2025,
  title = {Robust {{Vision-Based Runway Detection}} through {{Conformal Prediction}} and {{Conformal mAP}}},
  author = {Zouzou, Alya and {and{\'e}ol}, L{\'e}o and Ducoffe, M{\'e}lanie and Boumazouza, Ryma},
  year = 2025,
  month = may,
  number = {arXiv:2505.16740},
  eprint = {2505.16740},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.16740},
  urldate = {2025-12-02},
  abstract = {We explore the use of conformal prediction to provide statistical uncertainty guarantees for runway detection in vision-based landing systems (VLS). Using fine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal prediction to quantify localization reliability under user-defined risk levels. We also introduce Conformal mean Average Precision (C-mAP), a novel metric aligning object detection performance with conformal guarantees. Our results show that conformal prediction can improve the reliability of runway detection by quantifying uncertainty in a statistically sound way, increasing safety on-board and paving the way for certification of ML system in the aerospace domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\E097600\\Zotero\\storage\\M82TWDAZ\\Zouzou et al. - 2025 - Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP.pdf;C\:\\Users\\E097600\\Zotero\\storage\\67LQ7THJ\\2505.html}
}
